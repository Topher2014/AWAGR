{
  "title": "RAID (日本語)",
  "url": "https://wiki.archlinux.org/title/RAID_(%E6%97%A5%E6%9C%AC%E8%AA%9E)",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "関連記事\n\n- ソフトウェア RAID 上で LVM\n- LVM#RAID\n- Fake RAID でインストール\n- シングルドライブ環境を RAID に変換\n- ZFS\n- ZFS/仮想ディスク\n- スワップ#ストライピング\n- Btrfs#RAID\n\nRedundant Array of Independent Disks (個別のディスクによる冗長化集合、RAID) とは、複数のディスクドライブ (典型的にはディスクドライブかパーティシヨン) を組み合わせて1つの論理ユニットとして扱うストレージ技術です。RAID の実装に応じて、この論理ユニットは1つのファイルシステムや、いくつかのパーティションを保持する更なる透過型レイヤとすることができます。(必要な冗長性とパフォーマンスのレベルに応じて) #RAID レベル と呼ばれるいくつかの方法の内いずれかを使用して、データをドライブの集合にまたがって分散させます。選択する RAID レベルによって、ハードディスク障害時にデータ損失を防いだり、パフォーマンスを向上させたり、あるいは両方を実現できます。\n\nこの記事では mdadm によるソフトウェア RAID アレイの作成および管理方法を説明します。\n\n"
    },
    {
      "title": "目次",
      "level": 2,
      "content": "- 1 RAID レベル 1.1 スタンダード RAID レベル 1.2 ネスト RAID レベル 1.3 RAID 比較 1.4 非推奨の RAID レベル\n- 2 実装 2.1 使用している RAID はどのタイプか？\n- 3 インストール 3.1 デバイスの準備 3.2 デバイスをパーティショニングする 3.2.1 GUID Partition Table 3.2.2 Master Boot Record 3.3 アレイの構築 3.4 設定ファイルの更新 3.5 アレイのアセンブル 3.6 RAID ファイルシステムのフォーマット 3.6.1 ストライドとストライプ幅の計算 3.6.1.1 例 1. RAID0 3.6.1.2 例 2. RAID5 3.6.1.3 例 3. RAID10,far2\n- 4 ライブ CD からマウント\n- 5 RAID に Arch Linux をインストール 5.1 設定ファイルの更新 5.2 mkinitcpio の設定 5.3 ブートローダーの設定 5.3.1 root デバイス 5.3.2 RAID0 レイアウト\n- 6 RAID のメンテナンス 6.1 スクラビング 6.1.1 スクラビングの一般的な注意事項 6.1.2 RAID1 と RAID10 のスクラビングの注意事項 6.2 アレイからデバイスを削除する 6.3 アレイに新しいデバイスを追加する 6.4 RAID ボリュームのサイズを増やす 6.5 同期速度の制限を変更 6.6 RAID5 のパフォーマンス 6.7 RAID スーパーブロックを更新する\n- 7 監視 7.1 mdstat を watch する 7.2 iotop で IO を追跡する 7.3 iostat で IO を追跡する 7.4 systemd から mdadm を使う 7.4.1 メールで通知する 7.4.2 通知をプログラムする\n- 8 トラブルシューティング 8.1 エラー: \"kernel: ataX.00: revalidation failed\" 8.2 読み取り専用でアレイを起動 8.3 raid のドライブが故障しているまたは存在しない状態から回復する\n- 9 ベンチマーク\n- 10 参照\n\n- 1.1 スタンダード RAID レベル\n- 1.2 ネスト RAID レベル\n- 1.3 RAID 比較\n- 1.4 非推奨の RAID レベル\n\n- 2.1 使用している RAID はどのタイプか？\n\n- 3.1 デバイスの準備\n- 3.2 デバイスをパーティショニングする 3.2.1 GUID Partition Table 3.2.2 Master Boot Record\n- 3.3 アレイの構築\n- 3.4 設定ファイルの更新\n- 3.5 アレイのアセンブル\n- 3.6 RAID ファイルシステムのフォーマット 3.6.1 ストライドとストライプ幅の計算 3.6.1.1 例 1. RAID0 3.6.1.2 例 2. RAID5 3.6.1.3 例 3. RAID10,far2\n\n- 3.2.1 GUID Partition Table\n- 3.2.2 Master Boot Record\n\n- 3.6.1 ストライドとストライプ幅の計算 3.6.1.1 例 1. RAID0 3.6.1.2 例 2. RAID5 3.6.1.3 例 3. RAID10,far2\n\n- 3.6.1.1 例 1. RAID0\n- 3.6.1.2 例 2. RAID5\n- 3.6.1.3 例 3. RAID10,far2\n\n- 5.1 設定ファイルの更新\n- 5.2 mkinitcpio の設定\n- 5.3 ブートローダーの設定 5.3.1 root デバイス 5.3.2 RAID0 レイアウト\n\n- 5.3.1 root デバイス\n- 5.3.2 RAID0 レイアウト\n\n- 6.1 スクラビング 6.1.1 スクラビングの一般的な注意事項 6.1.2 RAID1 と RAID10 のスクラビングの注意事項\n- 6.2 アレイからデバイスを削除する\n- 6.3 アレイに新しいデバイスを追加する\n- 6.4 RAID ボリュームのサイズを増やす\n- 6.5 同期速度の制限を変更\n- 6.6 RAID5 のパフォーマンス\n- 6.7 RAID スーパーブロックを更新する\n\n- 6.1.1 スクラビングの一般的な注意事項\n- 6.1.2 RAID1 と RAID10 のスクラビングの注意事項\n\n- 7.1 mdstat を watch する\n- 7.2 iotop で IO を追跡する\n- 7.3 iostat で IO を追跡する\n- 7.4 systemd から mdadm を使う 7.4.1 メールで通知する 7.4.2 通知をプログラムする\n\n- 7.4.1 メールで通知する\n- 7.4.2 通知をプログラムする\n\n- 8.1 エラー: \"kernel: ataX.00: revalidation failed\"\n- 8.2 読み取り専用でアレイを起動\n- 8.3 raid のドライブが故障しているまたは存在しない状態から回復する\n\n"
    },
    {
      "title": "RAID レベル",
      "level": 2,
      "content": "ほとんどの RAID レベルで冗長性が確保されていますが、RAID はデータが安全であることを保証するものではありません。火事があったときや、コンピュータが盗まれたとき、または複数のドライブが同時に壊れた場合など RAID はデータを保護しません。さらに、RAID でシステムをインストールするのは複雑な作業であり、そのときにデータを破壊してしまう可能性もあります。\n\n"
    },
    {
      "title": "スタンダード RAID レベル",
      "level": 3,
      "content": "RAID のレベルはたくさん存在します、以下は最も一般的なレベルです。\n\n"
    },
    {
      "title": "RAID 比較",
      "level": 3,
      "content": "Table content:\nRAID レベル | データの冗長性 | 物理ドライブの利用効率 | 読込パフォーマンス | 書込パフォーマンス | 最小ドライブ数\n0 | No | 100% | nX 最速 | nX 最速 | 2\n1 | Yes | 50% | 複数プロセスが読み込む場合 nX まで、それ以外では 1X | 1X | 2\n5 | Yes | 67% - 94% | (n−1)X 高速 | (n−1)X 高速 | 3\n6 | Yes | 50% - 88% | (n−2)X | (n−2)X | 4\n10,far2 | Yes | 50% | nX 最速; RAID0 と同等だが冗長 | (n/2)X | 2\n10,near2 | Yes | 50% | 複数プロセスが読み込む場合 nX まで、それ以外では 1X | (n/2)X | 2\n\n最速\n\n最速\n\n高速\n\n高速\n\n最速; RAID0 と同等だが冗長\n\n* n は利用するディスクの数。\n\n"
    },
    {
      "title": "実装",
      "level": 2,
      "content": "RAID デバイスの制御方法は様々です:\n\n- 抽象レイヤー (例: mdadm)。 ノート: このガイドで使用する方法です。\n- 論理ボリュームマネージャ (例: LVM)。\n- ファイルシステムのコンポーネント (例: ZFS、Btrfs)。\n\n"
    },
    {
      "title": "使用している RAID はどのタイプか？",
      "level": 3,
      "content": "ソフトウェア RAID の実装はユーザーが行うため、ソフトウェア RAID を使っていることは簡単にわかります。\n\n反対に、FakeRAID と真のハードウェア RAID を見分けるのは難しいかもしれません。上記の通り、しばしばメーカーはこれら二つの RAID タイプを誤って区別していることがあり、不当表示も考えられます。この場合、最も良い方法は lspci コマンドを実行して、出力から RAID コントローラーを探すことです。そして、その RAID コントローラーに関する情報を検索することです。Hardware RAID コントローラーは先のコマンドのリストに現れますが、FakeRAID の実装は現れません。また、真のハードウェア RAID コントローラーは高価であることが多いので、システムをカスタマイズする際、ハードウェア RAID の構成を選択するとコンピューターの価格にそれとわかるくらいの差があるはずです。\n\n"
    },
    {
      "title": "インストール",
      "level": 2,
      "content": "mdadm をインストールして下さい。mdadm はプレーンなブロックデバイスを使って純粋なソフトウェア RAID を管理するために使用されます:基底のハードウェアは RAID ロジックを提供せず、ディスクだけ供給します。mdadm はどんなブロックデバイスの組み合わせでも使うことができます。あまり一般的でない組み合わせであってもです。例えば、USB ドライブを集めて RAID アレイを作成することも可能です。\n\n"
    },
    {
      "title": "デバイスの準備",
      "level": 3,
      "content": "デバイスを再利用する場合や、既存のアレイを作り直す場合、古い RAID 構成情報を全て消去してください:\n\n```\n# mdadm --misc --zero-superblock /dev/drive\n```\n\nもしくは、ドライブ上の特定のパーティションを削除する場合は:\n\n```\n# mdadm --misc --zero-superblock /dev/partition\n```\n\n- ある一つのパーティションのスーパーブロックを消去しても、そのディスク上の他のパーティションに影響を与えることは無いはずです。\n- RAID 機能の性質上、使用中のアレイ上でディスクをセキュアに完全消去することは非常に困難です。アレイを作成する前に、ディスクの消去が有用であるかどうかを検討してください。\n- blivet-guiAUR を使えばディスクの準備を全て GUI から行うことができます。\n\n"
    },
    {
      "title": "デバイスをパーティショニングする",
      "level": 3,
      "content": "アレイに使用するディスクをパーティショニングすることが強く推奨されます。ほとんどの RAID ユーザーは 2 TiB 以上のディスクドライブを選択するため、GPT が必須ですし推奨されます。パーティショニングやパーティショニングツールに関する詳細は パーティショニング の記事を見てください。\n\n"
    },
    {
      "title": "GUID Partition Table",
      "level": 4,
      "content": "- (GPT の) パーティションを作成したあと、パーティションのパーティションタイプの GUID は A19D880F-05FC-4D3B-A006-743F0F84911E になっている必要があります (このタイプは、fdisk ではパーティションタイプ Linux RAID を、gdisk では FD00 を選択することで割り当てることができます)。\n- 大きなディスクアレイを使用する場合は、後で個々のディスクを簡単に判別できるようにするためにファイルシステムラベルかパーティションラベルを割り当てることを検討してください。\n- 各デバイスのサイズと同じサイズのパーティションを作成することが推奨されます。\n\n"
    },
    {
      "title": "Master Boot Record",
      "level": 4,
      "content": "HDD 上に MBR パーティションテーブルでパーティションを作成する場合、利用できるパーティションタイプの ID は以下の通りです:\n\n- ファイルシステムデータ以外には 0xDA (fdisk では Non-FS data)。これは Arch Linux で RAID アレイを作成する際の推奨される mdadm パーティションタイプです。\n- RAID 自動検出アレイには 0xFD (fdisk では Linux RAID autodetect)。このパーティションタイプは、RAID 自動検出が望ましい場合に限り (initramfs を用いないシステムや、古い mdadm メタデータフォーマットの場合)、使用するべきです。\n\n詳細は Linux Raid Wiki:Partition Types を見てください。\n\n"
    },
    {
      "title": "アレイの構築",
      "level": 3,
      "content": "mdadm を使ってアレイを構築します。サポートされているオプションは mdadm(8) を見てください。例をいくつか以下に挙げます。\n\n- RAID1 アレイを Syslinux から起動する場合、syslinux v4.07 の制限として metadata の値をデフォルトの 1.2 ではなく 1.0 にする必要があります。\n- Arch インストールメディアからアレイを作成する際、--homehost=yourhostname オプション (あるいは、--homehost=any でホストに依らず常に同じ名前を使う) を使ってホスト名を設定してください。さもないと、archiso というホスト名がアレイのメタデータに書き込まれてしまいます。\n\n以下の例では、2つのデバイスを使用する RAID1 アレイを構築しています:\n\n```\n# mdadm --create --verbose --level=1 --metadata=1.2 --raid-devices=2 /dev/md/MyRAID1Array /dev/sdb1 /dev/sdc1\n```\n\n以下の例では、4つのアクティブデバイスと1つのスペアデバイスを使用して RAID5 アレイを構築しています:\n\n```\n# mdadm --create --verbose --level=5 --metadata=1.2 --chunk=256 --raid-devices=4 /dev/md/MyRAID5Array /dev/sdb1 /dev/sdc1 /dev/sdd1 /dev/sde1 --spare-devices=1 /dev/sdf1\n```\n\n以下の例では、2つのデバイスを使って RAID10,far2 アレイを構築しています:\n\n```\n# mdadm --create --verbose --level=10 --metadata=1.2 --chunk=512 --raid-devices=2 --layout=f2 /dev/md/MyRAID10Array /dev/sdb1 /dev/sdc1\n```\n\nアレイは仮想デバイス /dev/mdX に作成され、アセンブルされて (縮退モードで) 使用できます。このデバイスファイルは直接使うことができ、mdadm がバックグラウンドでアレイを同期してくれます。パリティのレストアには長い時間がかかる可能性があります。進捗は次のコマンドで確認できます:\n\n```\n$ cat /proc/mdstat\n```\n\n"
    },
    {
      "title": "設定ファイルの更新",
      "level": 3,
      "content": "デフォルトでは、mdadm.conf 内の殆どの行がコメントアウトされており、以下だけが含まれています:\n\n```\n/etc/mdadm.conf\n```\n\n```\n...\nDEVICE partitions\n...\n```\n\nこのディレクティブは、/proc/partitions から参照されているデバイスを検査し、可能な限り多くのアレイをアセンブルします。あなたが利用可能なアレイを全て起動するつもりで、想定外のスーパーブロックが検出されないことが確実である場合には、これで良いでしょう。より正確な方法は、アレイを明示的に /etc/mdadm.conf に追加することです:\n\n```\n# mdadm --detail --scan >> /etc/mdadm.conf\n```\n\n結果は以下のようになります:\n\n```\n/etc/mdadm.conf\n```\n\n```\n...\nDEVICE partitions\n...\nARRAY /dev/md/MyRAID1Array metadata=1.2 name=pine:MyRAID1Array UUID=27664f0d:111e493d:4d810213:9f291abe\n```\n\nこの場合でも mdadm は /proc/partitions から参照されているデバイスを検査します。しかし、27664… の UUID を持つスーパーブロックのみがアクティブなアレイにアセンブルされます。\n\n詳細は mdadm.conf(5) を参照してください。\n\n"
    },
    {
      "title": "アレイのアセンブル",
      "level": 3,
      "content": "設定ファイルを更新できたら、mdadm を使ってアレイをアセンブルできます:\n\n```\n# mdadm --assemble --scan\n```\n\n"
    },
    {
      "title": "RAID ファイルシステムのフォーマット",
      "level": 3,
      "content": "これまでの手順を終えた今、他のファイルシステムと同じようにアレイをファイルシステムでフォーマットすることができます。覚えておくべきことは:\n\n- ボリュームサイズが巨大であるため、一部のファイルシステムは適しません (Wikipedia:Comparison of file systems#Limits を参照)。\n- 使用するファイルシステムはオンラインでの拡張と縮小に対応している必要があります (Wikipedia:Comparison of file systems#Features を参照)。\n- 最適なパフォーマンスを得るには適切なストライドとストライプ幅を計算する必要があります。\n\n"
    },
    {
      "title": "ストライドとストライプ幅の計算",
      "level": 4,
      "content": "ファイルシステムの構造を基底の RAID 構造に合うように最適化するにはストライドとストライプ幅の2つを調整する必要があります。これらは RAID のチャンクサイズ、ファイルシステムのブロックサイズ、\"データディスク\"の数から導かれます。\n\nチャンクサイズは RAID アレイのプロパティであり、RAID の作成時に決まります。mdadm の現在のデフォルト値は 512 KiB です。mdadm を使えば、チャンクサイズを調べられます:\n\n```\n# mdadm --detail /dev/mdX | grep 'Chunk Size'\n```\n\nブロックサイズはファイルシステムのプロパティであり、ファイルシステムの作成時に決まります。多くのファイルシステム (ext4 を含む) でデフォルトのブロックサイズは 4 KiB となっています。ext4 に関する詳細は /etc/mke2fs.conf を参照してください。\n\n\"データディスク\"の数とは、データの損失を引き起こさずに完全にアレイを再構築するのに必要なデバイスの最小数です。例えば、N 個のデバイスからなるアレイでは RAID0 では N 個となり、RAID5 では N - 1 個となります。\n\nこれら3つの値がわかったら、以下の公式でストライドとストライプ幅を計算できます:\n\n```\nストライド = チャンクサイズ / ブロックサイズ\nストライプ幅 = データディスクの数 * ストライド\n```\n\n適切なストライドとストライプ幅で ext4 でフォーマットする例:\n\n- 仮に RAID0 アレイは2つの物理ディスクからなるとします。\n- チャンクサイズは 512 KiB とします。\n- ブロックサイズは 4 KiB です。\n\nストライド = チャンクサイズ / ブロックサイズ。この例では、512 / 4 となり、ストライド = 128 です。\n\nストライプ幅 = 物理データディスクの数 * ストライド。この例では、2 * 128 となり、ストライプ幅 = 256 です。\n\n```\n# mkfs.ext4 -v -L myarray -b 4096 -E stride=128,stripe-width=256 /dev/md0\n```\n\n適切なストライドとストライプ幅で ext4 でフォーマットする例:\n\n- 仮に RAID5 アレイは4つの物理ディスクからなり、3つはデータディスク、1つはパリティディスクとします。\n- チャンクサイズは 512 KiB とします。\n- ブロックサイズは 4 KiB です。\n\nストライド = チャンクサイズ / ブロックサイズ。この例では、512 / 4 となり、ストライド = 128 です。\n\nストライプ幅 = 物理データディスクの数 * ストライド。この例では、3 * 128 となり、ストライプ幅 = 384 です。\n\n```\n# mkfs.ext4 -v -L myarray -b 4096 -E stride=128,stripe-width=384 /dev/md0\n```\n\nストライドとストライプ幅に関する詳細は RAID Math を参照してください。\n\n適切なストライドとストライプ幅で ext4 でフォーマットする例:\n\n- 仮に RAID10 アレイは2つの物理ディスクからなるとします。far2 レイアウトでの RAID10 の性質上、これら2つともデータディスクとしてカウントします。\n- チャンクサイズは 512 KiB とします。\n- ブロックサイズは 4 KiB です。\n\nストライド = チャンクサイズ / ブロックサイズ。この例では、512 / 4 となり、ストライド = 128 です。\n\nストライプ幅 = 物理データディスクの数 * ストライド。この例では、2 * 128 となり、ストライプ幅 = 256 です。\n\n```\n# mkfs.ext4 -v -L myarray -b 4096 -E stride=128,stripe-width=256 /dev/md0\n```\n\n"
    },
    {
      "title": "ライブ CD からマウント",
      "level": 2,
      "content": "ライブ CD から RAID パーティションをマウントしたい場合、次のコマンドを使います:\n\n```\n# mdadm --assemble /dev/mdnumber /dev/disk1 /dev/disk2 /dev/disk3 /dev/disk4\n```\n\nディスクアレイの無い RAID 1 が RAID 1 と自動で誤検出され (mdadm --detail /dev/mdnumber の出力でわかります)、非アクティブであると報告される場合 (cat /proc/mdstat でわかります)、先にアレイを停止してください:\n\n```\n# mdadm --stop /dev/mdnumber\n```\n\n"
    },
    {
      "title": "RAID に Arch Linux をインストール",
      "level": 2,
      "content": "インストール手順のパーティショニングステップとフォーマットステップの間で RAID アレイを作成する必要があります。Root ファイルシステムにするパーティションを直接フォーマットせずに、RAID アレイ上に作成します。#インストール セクションの指示に従って RAID アレイを作成してください。その後は、pacstrap の手順が終わる所までインストール手順に従ってください。UEFI で起動する場合、EFI システムパーティション#ソフトウェア RAID1 上に ESP を配置する も読んでください。\n\n"
    },
    {
      "title": "設定ファイルの更新",
      "level": 3,
      "content": "ベースシステムをインストールしたら、デフォルトの設定ファイル mdadm.conf を次のようにして更新する必要があります:\n\n```\n# mdadm --detail --scan >> /mnt/etc/mdadm.conf\n```\n\n上のコマンドを実行した後は、必ずテキストエディタ等を使って mdadm.conf 設定ファイルをチェックして、中身が問題ないか確認してください。\n\n再びインストール手順を続けて、インストールガイド#Initramfs の前まで進めてください。そこまで行ったら、この次のセクションを見てください。\n\n"
    },
    {
      "title": "mkinitcpio の設定",
      "level": 3,
      "content": "mdadm をインストールし、initramfs イメージに mdadm のサポートを追加する mdadm_udev を mkinitcpio.conf の HOOKS 配列に追加してください:\n\n```\n/etc/mkinitcpio.conf\n```\n\n```\n...\nHOOKS=(base udev autodetect microcode modconf kms keyboard keymap consolefont block mdadm_udev filesystems fsck)\n...\n```\n\nそして、initramfs を再生成してください。\n\n"
    },
    {
      "title": "root デバイス",
      "level": 4,
      "content": "マッピングされた RAID のデバイスを root カーネルパラメータで指定してください。例えば:\n\n```\nroot=/dev/md/MyRAIDArray\n```\n\nこのカーネルデバイスノードを使ってソフトウェア RAID パーティションからのブートに失敗する場合、もう一つの方法は永続的なブロックデバイスの命名にある方法を使うことです。例えば:\n\n```\nroot=LABEL=Root_Label\n```\n\nGRUB#RAID も参照してください。\n\n"
    },
    {
      "title": "RAID0 レイアウト",
      "level": 4,
      "content": "Linux カーネル 5.3.4 から、カーネルにどの RAID0 レイアウト (RAID0_ORIG_LAYOUT (1) か RAID0_ALT_MULTIZONE_LAYOUT (2)) を使うべきかをカーネルに明示的に伝える必要があります。[1] 以下のようにカーネルパラメータを指定することで、カーネルに伝えることができます:\n\n```\nraid0.default_layout=2\n```\n\n指定するべき正しい値は、RAID アレイを作成した時に使っていたカーネルのバージョンに依ります。カーネル 3.14 及びそれ以前を使って作成した場合は 1 を、それより新しいバージョンを使って作成した場合は 2 を指定してください。どちらを使うべきか確認する一つの方法は、RAID アレイの作成日時を見ることです:\n\n```\nmdadm --detail /dev/md1\n```\n\n```\n/dev/md1:\n           Version : 1.2\n     Creation Time : Thu Sep 24 10:17:41 2015\n        Raid Level : raid0\n        Array Size : 975859712 (930.65 GiB 999.28 GB)\n      Raid Devices : 3\n     Total Devices : 3\n       Persistence : Superblock is persistent\n\n       Update Time : Thu Sep 24 10:17:41 2015\n             State : clean\n    Active Devices : 3\n   Working Devices : 3\n    Failed Devices : 0\n     Spare Devices : 0\n\n        Chunk Size : 512K\n\nConsistency Policy : none\n\n              Name : archiso:root\n              UUID : 028de718:20a81234:4db79a2c:e94fd560\n            Events : 0\n\n    Number   Major   Minor   RaidDevice State\n       0     259        2        0      active sync   /dev/nvme0n1p1\n       1     259        6        1      active sync   /dev/nvme2n1p1\n       2     259        5        2      active sync   /dev/nvme1n1p2\n```\n\nここでは、RAID アレイが2015年9月24日に作成されました。Linux カーネル 3.14 のリリース日は2014年3月30日ですので、この RAID アレイは multizone layout (2) を使って作成された可能性が最も高いです。\n\n"
    },
    {
      "title": "RAID のメンテナンス",
      "level": 2,
      "content": "Table content:\nこの記事あるいはセクションは翻訳の途中です。 ノート: 翻訳が古くなっています。 (議論: トーク:RAID#)\n\n"
    },
    {
      "title": "スクラビング",
      "level": 3,
      "content": "誤りをチェック・修正するために定期的にデータスクラビングを実行するのは良い習慣です。アレイのサイズや設定にもよりますが、スクラビングは完了するまでに数時間かかる場合があります。\n\nデータスクラビングを開始するには:\n\nTo initiate a data scrub:\n\n```\n# echo check > /sys/block/md0/md/sync_action\n```\n\nCheck オペレーションは不良セクタがないかドライブをスキャンして自動的に不良セクタを修復します。不良データ (他のディスクが示すデータと一致しないセクタのデータ、例えば、パリティブロックと他のデータブロックによって該当するデータブロックが不正だと判断される場合など) を含んでいる良好セクタを見つけた場合、対処は何もされませんが、イベントが記録されます (下を参照)。\"何もされない\"ことで、管理者はセクタのデータと、重複するデータからセクタを再生成することで得られるデータを検査して正しいデータを選んで保持することができます。\n\nmdadm に関連する様々なタスクやアイテムと同様に、スクラビングの状況は /proc/mdstat を読み出すことで調べることができます。\n\n例:\n\n```\n$ cat /proc/mdstat\n```\n\n```\nPersonalities : [raid6] [raid5] [raid4] [raid1]\nmd0 : active raid1 sdb1[0] sdc1[1]\n      3906778112 blocks super 1.2 [2/2] [UU]\n      [>....................]  check =  4.0% (158288320/3906778112) finish=386.5min speed=161604K/sec\n      bitmap: 0/30 pages [0KB], 65536KB chunk\n```\n\n実行中のデータスクラビングを安全に停止するには:\n\n```\n# echo idle > /sys/block/md0/md/sync_action\n```\n\nスクラビングが完了したら、(不良セクタがあった場合) いくつのブロックが不良として判断されたか確認することができます:\n\n```\n# cat /sys/block/md0/md/mismatch_cnt\n```\n\n"
    },
    {
      "title": "スクラビングの一般的な注意事項",
      "level": 4,
      "content": "定期的にスクラビングを root で実行する cron ジョブを設定するのは良い考えです。ジョブの設定に役立つ raid-checkAUR を見て下さい。cron の代わりに systemd タイマーを使ってスクラビングを実行したい場合は raid-check-systemdAUR をインストールしてください。このパッケージには systemd タイマーのユニットファイルとスクリプトが含まれています。\n\n"
    },
    {
      "title": "RAID1 と RAID10 のスクラビングの注意事項",
      "level": 4,
      "content": "カーネルにおける RAID1 と RAID10 の書き込みはバッファリングされないため、アレイが問題ないときでもアレイのミスマッチ数がゼロ以外になる可能性があります。このようなケースは一時的なデータ領域にしか発生しないため、問題は起こりません。しかしながら、一時的なデータ領域に発生しているミスマッチと、実際に問題が起こっていることによるミスマッチを見分けることはできません。このために RAID1 や RAID10 アレイでは誤検知が発生することがあります。それでもデバイスにあるかもしれない不良セクタを見つけて直すために定期的にスクラビングすることを推奨します。\n\n"
    },
    {
      "title": "アレイからデバイスを削除する",
      "level": 3,
      "content": "アレイからデバイスを削除する際は削除する前にそのブロックデバイスが壊れているとマークを付けます:\n\n```\n# mdadm --fail /dev/md0 /dev/failing_array_member\n```\n\nそしてアレイからデバイスを削除します:\n\n```\n# mdadm --remove /dev/md0 /dev/failing_array_member\n```\n\nデバイスが完全には壊れていないが、交換したい場合 (壊れつつあるように思える場合など)、まず新しいドライブを追加しておいて、その後で交換したいドライブを置き換えることで、より簡単にドライブを交換することができます。\n\n例えば、/dev/sdc1 を新しいドライブ、/dev/sdb1 を壊れているドライブとすると:\n\n```\n# mdadm /dev/md0 --add /dev/sdc1\n# mdadm /dev/md0 --replace /dev/sdb1 --with /dev/sdc1\n```\n\n--with /dev/sdc1 の部分は必須ではありませんが、こうすることで交換後のデバイスを明示的に指定できます。詳細は [2] を参照してください。\n\nデバイスを永続的に除去するには (例えば、そのデバイスを個別のデバイスとして使いたい場合など)、先の手順 (--fail/--remove または --add/--replace) の後に以下を実行してください:\n\n```\n# mdadm --zero-superblock /dev/failing_array_member\n```\n\n- 上記のコマンドを linear や RAID0 のアレイで実行してはいけません。データが消失してしまいます。\n- 除去したディスクを再利用する際は、そのディスクのスーパーブロックをゼロ消去しないと、次回起動時に全てのデータが消失します。(mdadm がそのディスクを RAID アレイの一部として使おうとするからです。)\n\nStop using an array: アレイの使用を停止するには:\n\n1. 対象のアレイをアンマウント\n1. 次のコマンドでアレイを停止: mdadm --stop /dev/md0\n1. この章の最初に書かれている3つのコマンドを各デバイスに対して実行する (訳注: --fail、--remove、そして --zero-superblock です)。\n1. /etc/mdadm.conf からこのアレイに対応する行を削除。\n\n"
    },
    {
      "title": "アレイに新しいデバイスを追加する",
      "level": 3,
      "content": "デバイスがマウントされている動作中のシステム上でも mdadm で新しいデバイスを追加することができます。上述のようにして、すでにアレイに存在しているものと同じレイアウトで新しいデバイスをパーティショニングしてください。\n\nまだされていない場合は RAID アレイをアセンブルしてください:\n\n```\n# mdadm --assemble /dev/md0 /dev/sda1 /dev/sdb1\n```\n\n新しいデバイスをアレイに追加してください:\n\n```\n# mdadm --add /dev/md0 /dev/sdc1\n```\n\nmdadm がデバイスをアレイに追加するのに対して時間はかからないはずです。\n\nRAID のタイプによっては (例えば RAID1)、デバイスとデータを同期せずに、デバイスをスペアとして追加することもできます。RAID が使用するディスクの数は --grow と --raid-devices オプションで増やすことができます。例えば、アレイを4つのディスクに増やすには:\n\n```\n# mdadm --grow /dev/md0 --raid-devices=4\n```\n\n以下のコマンドで進捗を確認できます:\n\n```\n# cat /proc/mdstat\n```\n\n以下のコマンドでデバイスが追加されたことを確認できます:\n\n```\n# mdadm --misc --detail /dev/md0\n```\n\n```\nmdadm: add new device failed for /dev/sdc1 as 2: Invalid argument\n```\n\nこれは上記のコマンドが新しいディスクを「スペア」として追加しますが、RAID0 にはスペアは存在しないからです。RAID0 アレイにデバイスを追加したい場合は、以下のようにして同じコマンドで \"grow\" し、かつ \"add\" する必要があります:\n\n```\n# mdadm --grow /dev/md0 --raid-devices=3 --add /dev/sdc1\n```\n\n"
    },
    {
      "title": "RAID ボリュームのサイズを増やす",
      "level": 3,
      "content": "RAID アレイに巨大なディスクを追加した場合やパーティションサイズを増やした場合、RAID ボリュームのサイズを増加させて空き領域を埋めると良いでしょう。まずは、ディスクの交換に関する上記のセクションに従ってください。RAID ボリュームをより大きなディスク上に再構築したら、その領域を埋めるようにボリュームを拡張する必要があります。\n\n```\n# mdadm --grow /dev/md0 --size=max\n```\n\nそして RAID ボリューム /dev/md0 に存在するパーティションのサイズを変更してください。詳しくはパーティショニングを参照。最後に、パーティション内のファイルシステムをリサイズしてください。gparted でパーティショニングした場合、自動的にリサイズされます。他のツールを使った場合、ファイルシステムをアンマウントして手動でファイルシステムをリサイズしてください:\n\n```\n# umount /storage\n# fsck.ext4 -f /dev/md0p1\n# resize2fs /dev/md0p1\n```\n\n"
    },
    {
      "title": "同期速度の制限を変更",
      "level": 3,
      "content": "同期には時間がかかります。マシンで他の作業をしていない場合、速度制限を上げることが可能です。\n\n```\n# cat /proc/mdstat\n```\n\n```\nPersonalities : [raid10]\n md127 : active raid10 sdd1[3] sdc1[2] sdb1[1] sda1[0]\n     31251490816 blocks super 1.2 512K chunks 2 far-copies [4/4] [UUUU]\n     [=>...................]  resync =  5.2% (1629533760/31251490816) finish=2071.7min speed=238293K/sec\n     bitmap: 221/233 pages [884KB], 65536KB chunk\n```\n\n上記の例では、最大速度が約 238 M 毎秒に制限されているように見えます。\n\n現在の速度制限を確認すると:\n\n```\n# sysctl dev.raid.speed_limit_min\n```\n\n```\ndev.raid.speed_limit_min = 1000\n```\n\n```\n# sysctl dev.raid.speed_limit_max\n```\n\n```\ndev.raid.speed_limit_max = 200000\n```\n\nsysctl を使って RAID 再同期操作の最大速度を変更します:\n\n```\n# sysctl -w dev.raid.speed_limit_min=600000\n# sysctl -w dev.raid.speed_limit_max=600000\n```\n\n上記の設定後、同期速度と予想完了時間を確認してみてください:\n\n```\n# cat /proc/mdstat\n```\n\n```\nPersonalities : [raid10] \n md127 : active raid10 sdd1[3] sdc1[2] sdb1[1] sda1[0]\n     31251490816 blocks super 1.2 512K chunks 2 far-copies [4/4] [UUUU]\n     [=>...................]  resync =  5.3% (1657016448/31251490816) finish=1234.9min speed=399407K/sec\n     bitmap: 221/233 pages [884KB], 65536KB chunk\n```\n\n"
    },
    {
      "title": "RAID5 のパフォーマンス",
      "level": 3,
      "content": "高速なストレージ (NVMe など) において RAID5 のパフォーマンスを向上させるには、スレッド数 /sys/block/mdx/md/group_thread_cnt を増やしてください。例えば、RAID5 デバイス上で 8 スレッド使うには:\n\n```\n# echo 8 > /sys/block/md0/md/group_thread_cnt\n```\n\nカーネル の git コミット 851c30c9badf を参照してください。\n\n"
    },
    {
      "title": "RAID スーパーブロックを更新する",
      "level": 3,
      "content": "RAID スーパーブロックを更新するには、まずアレイをアンマウントして、その後に以下のコマンドでアレイを停止する必要があります:\n\n```\n# mdadm --stop /dev/md0\n```\n\nそして、アレイを再アセンブルすることで特定のパラメータを更新します。例えば、homehost を更新するには:\n\n```\n# mdadm --assemble --update=homehost --homehost=NAS /dev/md0 /dev/sda1 /dev/sdb1\n```\n\n詳細は --update の引数を見てください。\n\n"
    },
    {
      "title": "監視",
      "level": 2,
      "content": "以下は RAID デバイスの状態を出力するシンプルなワンライナーです:\n\n```\n# awk '/^md/ {printf \"%s: \", $1}; /blocks/ {print $NF}' </proc/mdstat\n```\n\n```\nmd1: [UU]\nmd0: [UU]\n```\n\n"
    },
    {
      "title": "mdstat を watch する",
      "level": 3,
      "content": "```\n# watch -t 'cat /proc/mdstat'\n```\n\nまたは tmux を使うことがより望ましいです:\n\n```\n# tmux split-window -l 12 \"watch -t 'cat /proc/mdstat'\"\n```\n\n"
    },
    {
      "title": "iotop で IO を追跡する",
      "level": 3,
      "content": "iotop パッケージはプロセスの入出力の統計を表示します。次のコマンドを使って raid スレッドの IO を表示することができます:\n\n```\n# iotop -a $(sed 's/^/-p /g' <<<`pgrep \"_raid|_resync|jbd2\"`)\n```\n\n"
    },
    {
      "title": "iostat で IO を追跡する",
      "level": 3,
      "content": "sysstat パッケージに入っている iostat ユーティリティはデバイスやパーティションの入出力の統計を表示します:\n\n```\n# iostat -dmy 1 /dev/md0\n# iostat -dmy 1 # all\n```\n\n"
    },
    {
      "title": "systemd から mdadm を使う",
      "level": 3,
      "content": "mdadm は、mdmonitor.service という systemd サービスを提供しています。これは、raid アレイの健康状態を監視したり、何かが起こったときに通知したりすることに使えます。\n\nこのサービスは通常のサービスのように手動でアクティブ化できないという点で特殊です。システムのスタートアップ時にアレイをアセンブルする時に udev 経由で mdadm によってアクティブ化されます。しかし、このサービスが有効化されるのは、通知を送信するためのメールアドレスやプログラムが設定されている場合のみです (以下を参照)。\n\n"
    },
    {
      "title": "メールで通知する",
      "level": 4,
      "content": "この機能を利用するには、/etc/mdadm.conf でメールアドレスを定義してください:\n\n```\nMAILADDR user@domain\n```\n\n次に、以下のコマンドを使って、正しく動作することを確認してください:\n\n```\n# mdadm --monitor --scan --oneshot --test\n```\n\nテストが成功してメールが届いたら、作業は完了です。次回アレイがアセンブルされる時、mdmonitor.service はアレイを監視して、エラーが発生した時に通知します。\n\n"
    },
    {
      "title": "通知をプログラムする",
      "level": 4,
      "content": "上記のメール通知のように、/etc/mdadm.conf の以下の行を編集してください:\n\n```\nPROGRAM /usr/sbin/handle-mdadm-events\n```\n\nPROGRAM の引数は、イベント発生時に実行したいスクリプトです。使用例としては、適切なネットワーク監視エージェントと対話したり、ホームユーザーの場合は IM クライアントや ntfy.sh のようなプッシュ通知サービスを使用したりするといったものがあるでしょう。\n\n上記のメール通知と同じようにテストしてください。\n\n"
    },
    {
      "title": "トラブルシューティング",
      "level": 2,
      "content": "起動時に \"invalid raid superblock magic\" というエラーが発生する場合、RAID を構成しているハードドライブ以外にもドライブが存在しているのであれば、ハードドライブの順番が正しいか確認してください。インストール時に RAID デバイスのレターが (例えば) それぞれ hdd、hde、hdf だったとしても、再起動後には hda、hdb、hdc になっていることもあります。カーネルパラメータを適宜設定してください。\n\n"
    },
    {
      "title": "エラー: \"kernel: ataX.00: revalidation failed\"",
      "level": 3,
      "content": "突然、(再起動後や BIOS の設定を変更した後) 以下のようなエラーメッセージが出力される場合:\n\n```\nFeb  9 08:15:46 hostserver kernel: ata8.00: revalidation failed (errno=-5)\n```\n\nこれは必ずしもドライブが壊れていることを意味しているわけではありません。ウェブ上には不安を煽るような記事が多くありますが、焦ってはいけません。おろらく原因は、BIOS かカーネルパラメータで APIC や ACPI の設定をなにかの拍子で変更してしまっただけです。設定を元に戻せば大丈夫なはずです。通常、APIC か ACPI、またはこれら両方をオフにすると直るはずです。\n\n"
    },
    {
      "title": "読み取り専用でアレイを起動",
      "level": 3,
      "content": "md アレイが起動する際にスーパーブロックに書き込みが起こり、この時 resync が始まることがあります。読み取り専用でアレイを起動するには、md_mod カーネルモジュールに start_ro パラメータを設定してください。このパラメータが設定されていると、新しく起動するアレイは 'auto-ro' モードになり、全ての内部 IO (スーパーブロックの更新、resync、リカバリ) が無効化されます。これは、書き込みリクエストが発行されると自動的に 'rw' に切り替わります。\n\n起動時にモジュールのパラメータを設定するには、カーネルパラメータに md_mod.start_ro=1 を追加してください。\n\nもしくは、/etc/modprobe.d/ 内のファイルを使用するか /sys/ 内のファイルに直接書き込むことでモジュールのロード時に設定することもできます:\n\n```\n# echo 1 > /sys/module/md_mod/parameters/start_ro\n```\n\n"
    },
    {
      "title": "raid のドライブが故障しているまたは存在しない状態から回復する",
      "level": 3,
      "content": "何らかの理由でドライブの一つが壊れた時も上述のエラーが表示されることがあります。その場合、ディスクが一つ足りない状態でもオンになるように raid を強制する必要があります。以下のコマンドを実行してください (必要に応じてパラメータは変えてください):\n\n```\n# mdadm --manage /dev/md0 --run\n```\n\nこれを実行すると、以下のようにしてマウントできるようになっているはずです (以下は fstab に raid のファイルシステムを記述している場合です。そうでない場合は、マウント元とマウントポイントを明示的に指定する必要があります。):\n\n```\n# mount /dev/md0\n```\n\nraid がまた動作するようになって使えるようにはなりましたが、ディスクが一つ不足している状態です。なので、#デバイスの準備 章で説明されているようにして、新しいディスクをパーティショニングしてください。そうしたら、その新しいディスクを以下のようにして raid に追加できます:\n\n```\n# mdadm --manage --add /dev/md0 /dev/sdd1\n```\n\n以下のコマンドを実行すると、raid がアクティブになっていて、再構築中であることが確認できるでしょう:\n\n```\n# cat /proc/mdstat\n```\n\nまた、これに合わせて設定ファイルを更新したほうが良いかもしれません (#設定ファイルの更新 を参照)。\n\n"
    },
    {
      "title": "ベンチマーク",
      "level": 2,
      "content": "RAID をベンチマークするツールは複数存在します。複数のスレッドが同じ RAID ボリュームから読み込む際の速度の向上が最もわかりやすい改善点です。\n\nbonnie++ はひとつまたは複数のファイルへのデータベースタイプのアクセスをテストしたり、小さなファイルを作成・読込・削除することで Squid や INN、または Maildir フォーマットのメールなどといったプログラムの使用をシミュレートします。同梱されている ZCAV プログラムはディスクに書き込みを行わずにハードドライブの異なる領域のパフォーマンスをテストします。\n\nhdparm を RAID のベンチマークに使ってはいけません。出てくる結果には全く一貫性が無いからです。\n\n"
    },
    {
      "title": "参照",
      "level": 2,
      "content": "- Linux Software RAID (thomas-krenn.com)\n- Linux カーネルアーカイブの Linux RAID wiki エントリ\n- How Bitmaps Work\n- Red Hat Enterprise Linux 6 ドキュメントの 第15章: Redundant Array of Independent Disks (RAID)[リンク切れ 2024-07-30]\n- Linux Documentation Project の Linux-RAID FAQ\n- Art S. Kagel による BAARF (Archive.org) と Why should I not use RAID 5? (Archive.org)\n- Linux Magazine の Introduction to RAID、Nested-RAID: RAID-5 and RAID-6 Based Configurations、Intro to Nested-RAID: RAID-01 and RAID-10、Nested-RAID: The Triple Lindy\n- HowTo: Speed Up Linux Software Raid Building And Re-syncing\n- Wikipedia:Non-RAID drive architectures\n\nメーリングリスト\n\n- Kernel Linux-Raid mailing list\n\nmdadm\n\n- mdadm のソースコード\n- Linux Magazine の Software RAID on Linux with mdadm\n- Wikipedia - mdadm\n\nフォーラムのスレッド\n\n- Raid Performance Improvements with bitmaps\n- GRUB and GRUB2\n- Can't install grub2 on software RAID\n- Use RAID metadata 1.2 in boot and root partition\n\n"
    }
  ]
}