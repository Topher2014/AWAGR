{
  "title": "KVM (Русский)",
  "url": "https://wiki.archlinux.org/title/KVM_(%D0%A0%D1%83%D1%81%D1%81%D0%BA%D0%B8%D0%B9)",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Ссылки по теме\n\n- Category:Hypervisors (Русский)\n- Libvirt\n\nKVM (Kernel-based Virtual Machine) — это гипервизор, встроенный в ядро Linux. Он похож по функциональности на Xen, но гораздо проще в настройке. В отличие от обычного QEMU, который использует эмуляцию, KVM — это специальный режим QEMU, который с помощью модуля ядра использует дополнительные инструкции процессора для виртуализации (HVM).\n\nИспользуя KVM, вы можете запускать несколько виртуальных машин под управлением обыкновенных (не требующих патчей) GNU/Linux, Windows или любой другой операционной системой (смотрите Guest Support Status для дополнительной информации о поддерживаемых ОС). Каждая виртуальная машина имеет свой отдельный набор виртуального оборудования: сетевую карту, диск, видеокарту и т.д.\n\nОтличия KVM от Xen, VMware или QEMU описаны в KVM FAQ.\n\nВ этой статье не рассказывается о возможностях эмуляторов, использующих KVM в качестве бэкенда. Обратитесь к связанным статьям, если вам это интересно.\n\n"
    },
    {
      "title": "Аппаратная поддержка",
      "level": 3,
      "content": "Для работы KVM необходимо, чтобы процессор хост-машины поддерживал технологию виртуализации (VT-x для процессоров Intel и AMD-V для процессоров AMD). Вы можете проверить, поддерживает ли ваш процессор аппаратную виртуализацию, выполнив следующую команду:\n\n```\n$ LC_ALL=C.UTF-8 lscpu | grep Virtualization\n```\n\nИли:\n\n```\n$ grep -E --color=auto 'vmx|svm|0xc0f' /proc/cpuinfo\n```\n\nЕсли после выполнения команды на экране ничего не выводится, тогда процессор вашего компьютера не поддерживает аппаратную виртуализацию, и вы не сможете использовать KVM.\n\n"
    },
    {
      "title": "Поддержка в ядре",
      "level": 3,
      "content": "Ядра Arch Linux имеют необходимые модули ядра для работы KVM.\n\n- Вы можете проверить, что необходимые модули — kvm, а также либо kvm_amd, либо kvm_intel — доступны в ядре с помощью команды:\n\n```\n$ zgrep CONFIG_KVM= /proc/config.gz\n```\n\nМодуль доступен, если установлено значение y или m.\n\n- Далее убедитесь, что модули ядра автоматически загружены, с помощью команды:\n\n```\n$ lsmod | grep kvm\n```\n\n```\nkvm_intel             245760  0\nkvmgt                  28672  0\nmdev                   20480  2 kvmgt,vfio_mdev\nvfio                   32768  3 kvmgt,vfio_mdev,vfio_iommu_type1\nkvm                   737280  2 kvmgt,kvm_intel\nirqbypass              16384  1 kvm\n```\n\nЕсли команда ничего не возвращает, то модуль необходимо загрузить вручную; смотрите Модуль ядра#Управление модулями вручную.\n\n"
    },
    {
      "title": "Паравиртуализация с Virtio",
      "level": 2,
      "content": "Паравиртуализация — это быстрый и эффективный способ для гостевой системы использовать устройства хост-машины. KVM предоставляет виртуальным машинам паравиртуализированные устройства посредством Virtio API. Это API служит прослойкой между гипервизором и гостевой системой.\n\nВсе устройства Virtio состоят из двух частей: хостового устройства и гостевого драйвера.\n\n"
    },
    {
      "title": "Поддержка в ядре",
      "level": 3,
      "content": "Выполните следующую команду в виртуальной машине, чтобы проверить, что модули VIRTIO доступны в ядре:\n\n```\n$ zgrep VIRTIO /proc/config.gz\n```\n\nДалее проверьте, что модули ядра автоматически загружены, выполнив команду:\n\n```\n$ lsmod | grep virtio\n```\n\nЕсли команды выше ничего не вернули, то загрузите модули ядра вручную.\n\n"
    },
    {
      "title": "Список паравиртуализированных устройств",
      "level": 3,
      "content": "- сетевая карта (virtio-net)\n- блочное устройство (virtio-blk)\n- контроллер SCSI (virtio-scsi)\n- последовательное устройство (virtio-serial)\n- устройство balloon (память переменного объёма) (virtio-balloon)\n\n"
    },
    {
      "title": "Как пользоваться KVM",
      "level": 2,
      "content": "Смотрите основную статью: QEMU (Русский).\n\n"
    },
    {
      "title": "Вложенная виртуализация",
      "level": 3,
      "content": "Вложенная виртуализация позволяет запускать существующие виртуальные машины на других гипервизорах и облаках без внесения изменений в них или их сетевую конфигурацию.\n\nНа хост-машине включите вложенную виртуализацию для kvm_intel:\n\n```\n# modprobe -r kvm_intel\n# modprobe kvm_intel nested=1\n```\n\nЧтобы изменение сохранилось после перезагрузки, создайте файл (подробнее: Модуль ядра#Настройка параметров модуля):\n\n```\n/etc/modprobe.d/kvm_intel.conf\n```\n\n```\noptions kvm_intel nested=1\n```\n\nУбедитесь, что функция активирована:\n\n```\n$ cat /sys/module/kvm_intel/parameters/nested\n```\n\n```\nY\n```\n\nВключите режим «host passthrough», чтобы передать все возможности процессора в гостевую систему:\n\n1. Если вы используете QEMU, запустите гостевую машину командой: qemu-system-x86_64 -enable-kvm -cpu host.\n1. Если вы используете virt-manager, измените модель процессора на host-passthrough.\n1. Если вы используете virsh, введите virsh edit vm-name и замените строчку CPU на <cpu mode='host-passthrough' check='partial'/>\n\nЗагрузите виртуальную машину и проверьте, установлен ли флаг vmx:\n\n```\n$ grep -E --color=auto 'vmx|svm' /proc/cpuinfo\n```\n\n"
    },
    {
      "title": "Включение поддержки больших страниц (hugepages)",
      "level": 3,
      "content": "Вы, возможно, захотите включить поддержку больших страниц для повышения производительности виртуальной машины. Если используется актуальный Arch Linux и модуль KVM включен, вероятно, всё необходимое уже есть. Проверьте, есть ли каталог /dev/hugepages. Если нет, то создайте его. Также необходимы правильные права для его использования. По умолчанию он принадлежит пользователю и группе root, битовая маска режима доступа установлена в значение 0755, но нам нужно, чтобы все пользователи группы kvm имели доступ к hugepages.\n\nДобавьте в свой /etc/fstab:\n\n```\n/etc/fstab\n```\n\n```\nhugetlbfs       /dev/hugepages  hugetlbfs       mode=01770,gid=kvm        0 0\n```\n\nВместо того, чтобы прямо указывать имя группы gid=kvm, вы, конечно, можете указать gid с помощью числа, но оно должно соответствовать группе kvm. Режим 1770 позволяет любому члену группы создавать файлы, но не разрешает удалять ссылки или переименовывать файлы других членов группы. Убедитесь, что /dev/hugepages смонтирована верно:\n\n```\n# umount /dev/hugepages\n# mount /dev/hugepages\n$ mount | grep huge\n```\n\n```\nhugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime,mode=1770,gid=78)\n```\n\nТеперь можно посчитать как много hugepages вам нужно. Проверьте размер hugepages командой:\n\n```\n$ grep Hugepagesize /proc/meminfo\n```\n\nОбычный размер - 2048 kB ≙ 2 MB. Допустим, вы хотите виртуальную машину с размером страниц 1024 MB. 1024 / 2 = 512. Добавим ещё немного, чтобы округлить до 550. Теперь укажите как много hugepages вам надо:\n\n```\n# sysctl -w vm.nr_hugepages=550\n```\n\nЕсли вы располагаете достаточным объёмом свободной памяти, то увидите:\n\n```\n$ grep HugePages_Total /proc/meminfo\n```\n\n```\nHugesPages_Total:  550\n```\n\nЕсли число меньше, закройте ненужные программы или запустите вашу виртуальную машину, уменьшив объём её памяти (количество_страниц x 2):\n\n```\n$ qemu-system-x86_64 -enable-kvm -m 1024 -mem-path /dev/hugepages -hda <disk_image> [...]\n```\n\nОбратите внимание на параметр -mem-path. Он определяет использование hugepages.\n\nТеперь, пока виртуальная машина работает, вы можете проверить, как много страниц использовано:\n\n```\n$ grep HugePages /proc/meminfo\n```\n\n```\nHugePages_Total:     550\nHugePages_Free:       48\nHugePages_Rsvd:        6\nHugePages_Surp:        0\n```\n\nТеперь, когда всё работает, вы можете, если хотите, включить hugepages по умолчанию. Создайте /etc/sysctl.d/40-hugepage.conf:\n\n```\n/etc/sysctl.d/40-hugepage.conf\n```\n\n```\nvm.nr_hugepages = 550\n```\n\nСмотрите также:\n\n- Summary of hugetlbpage support in the Linux kernel\n- Debian Wiki - Hugepages\n\n"
    },
    {
      "title": "Secure Boot",
      "level": 3,
      "content": "KVM Secure boot требует выполнения нескольких условий для включения:\n\n1. Вы должны использовать UEFI, скомпилированный с поддержкой secure boot.\n1. UEFI должен иметь записанные ключи.\n\nЧтобы включить UEFI с поддержкой secure boot, установите edk2-ovmf и укажите вашей виртуальной машине использовать UEFI с secure boot. Если вы используете libvirt, то можете сделать это, добавив следующие строки в XML файл конфигурации вашей виртуальной машины.\n\n```\n<os firmware=\"efi\">\n  <loader readonly=\"yes\" secure=\"yes\" type=\"pflash\">/usr/share/edk2/x64/OVMF_CODE.secboot.4m.fd</loader>\n</os>\n```\n\nДалее следует записать ключи. В данном примере мы запишем ключи secure boot от Microsoft и Redhat. Установите virt-firmware и выполните следующую команду, заменив vm_name именем вашей виртуальной машины.\n\n```\n$ virt-fw-vars --input /var/lib/libvirt/qemu/nvram/vm_name_VARS.fd --output /var/lib/libvirt/qemu/nvram/vm_name_SECURE_VARS.fd --secure-boot --enroll-redhat\n```\n\nОтредактируйте XML-файл конфигурации вашей виртуальной машины, указав на новый файл VARS.\n\n```\n<os firmware=\"efi\">\n  <loader readonly=\"yes\" secure=\"yes\" type=\"pflash\">/usr/share/edk2/x64/OVMF_CODE.secboot.4m.fd</loader>\n  <nvram template=\"/usr/share/edk2/x64/OVMF_VARS.4m.fd\">/var/lib/libvirt/qemu/nvram/{vm-name}_SECURE_VARS.fd</nvram>\n</os>\n```\n\nПосле этого secure boot должна быть автоматически включена. Можете перепроверить в BIOS виртуальной машины. Чтобы открыть интерфейс BIOS нажмите F2, когда увидите логотип загрузки UEFI.\n\n"
    },
    {
      "title": "Смотрите также",
      "level": 2,
      "content": "- KVM Howto\n- KVM FAQ\n\n"
    }
  ]
}