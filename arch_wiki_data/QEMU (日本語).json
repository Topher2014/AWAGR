{
  "title": "QEMU (日本語)",
  "url": "https://wiki.archlinux.org/title/QEMU_(%E6%97%A5%E6%9C%AC%E8%AA%9E)",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "関連記事\n\n- カテゴリ:ハイパーバイザ\n- Libvirt\n- QEMU/Guest graphics acceleration\n- OVMF による PCI パススルー\n\nQEMU about page によると:\n\nQEMU は Xen や KVM などの他のハイパーバイザを使用して、仮想化のための CPU 拡張命令 (HVM) を利用することができます。バーチャライザとして使う場合、QEMU はゲストコードをホスト CPU で直接実行することで、ネイティブに近いパフォーマンスを実現します。\n\n"
    },
    {
      "title": "目次",
      "level": 2,
      "content": "- 1 インストール 1.1 QEMU バリアンツ 1.2 Arch Linux で利用可能なパッケージの詳細\n- 2 QEMU のグラフィカルフロントエンド\n- 3 新しい仮想化システムの作成 3.1 ハードディスクイメージの作成 3.1.1 オーバーレイストレージイメージ 3.1.2 イメージのリサイズ 3.1.2.1 イメージの縮小 3.1.3 イメージの変換 3.2 インストールメディアを準備する 3.3 オペレーティングシステムのインストール 3.4 既製の仮想マシンイメージ\n- 4 仮想化システムを実行する 4.1 KVM を有効にする 4.2 IOMMU (Intel VT-d/AMD-Vi) サポートを有効にする 4.3 UEFI モードでの起動 4.3.1 Enabling Secure Boot 4.4 Trusted Platform Module のエミュレーション\n- 5 ホスト・ゲスト OS 間通信 5.1 ネットワーク 5.2 QEMU のポートフォワーディング 5.3 vsock 経由で SSH にアクセスする 5.4 QEMU の内蔵 SMB サーバ 5.5 9pfs VirtFS によるホストファイル共有 5.6 virtiofsd によるホストファイル共有 5.6.1 通常のユーザーとして virtiofsd を実行する 5.6.2 root として virtiofsd を実行する 5.6.3 QEMU を起動する 5.6.4 Linux ゲストで共有を使う 5.6.5 Windows ゲストで共有を使う 5.7 ゲストのパーティションをホストにマウントする 5.7.1 raw イメージからパーティションをマウントする 5.7.1.1 手動でバイトオフセットを指定する 5.7.1.2 loop モジュールでパーティションを自動検出する 5.7.1.3 kpartx を使う 5.7.2 qcow2 イメージからパーティションをマウントする\n- 6 ネットワーク 6.1 リンク層アドレス 6.2 ユーザーモードネットワーク 6.2.1 SLIRP 6.2.2 passt 6.3 QEMU の Tap ネットワーク 6.3.1 ホストオンリーネットワーク 6.3.2 内部ネットワーク 6.3.3 qemu-bridge-helper を使用したブリッジネットワーク 6.3.4 ブリッジを手動で作成する 6.3.5 iptables による物理デバイスと Tap デバイスのネットワーク共有 6.4 VDE2 によるネットワーク 6.4.1 VDE とは? 6.4.2 基本 6.4.3 起動スクリプト 6.4.4 他の方法 6.5 VDE2 Bridge 6.5.1 基本 6.5.2 起動スクリプト 6.6 省略記法の設定\n- 7 グラフィックスカード 7.1 std 7.2 qxl 7.3 vmware 7.4 virtio 7.5 cirrus 7.6 none\n- 8 SPICE 8.1 ホストで SPICE サポートを有効にする 8.2 SPICE クライアントでゲストに接続する 8.2.1 SPICE クライアントを手動で実行する 8.2.2 QEMU で SPICE クライアントを実行する 8.3 ゲストで SPICE サポートを有効にする 8.4 SPICE によるパスワード認証 8.5 SPICE による TLS 暗号化通信\n- 9 VNC 9.1 基本的なパスワード認証\n- 10 オーディオ 10.1 オーディオバックエンドを作成する 10.2 オーディオバックエンドを使用する 10.2.1 Intel HD Audio 10.2.2 Intel 82801AA AC97 10.2.3 VirtIO sound\n- 11 virtio ドライバーを使う 11.1 Arch Linux ゲストを用意する 11.1.1 メモリバルーニング 11.2 Windows ゲストを用意する 11.2.1 Windows 用の virtio ドライバ 11.2.2 ブロックデバイスドライバ 11.2.2.1 Windows の新規インストール 11.2.2.2 virtio を使用するように既存の Windows 仮想マシンを変更する 11.2.3 ネットワークドライバ 11.2.4 バルーンドライバ 11.2.5 virtiofsd の共有を使う 11.3 FreeBSD ゲストを用意する\n- 12 QEMU モニタ 12.1 モニタコンソールにアクセスする 12.1.1 グラフィカルビュー 12.1.2 Telnet 12.1.3 UNIX ソケット 12.1.4 TCP 12.1.5 標準 I/O 12.2 モニタコンソールを使って仮想マシンにキーボードの押下を送信する 12.3 モニタコンソールを使ってスナップショットを作成・管理する 12.4 immutable モードで仮想マシンを実行する 12.5 モニタコンソールによる一時停止と電源オプション 12.6 仮想マシンのスクリーンショットを取得する\n- 13 QEMU マシンプロトコル 13.1 QMP を開始する 13.2 親イメージへの子イメージのライブマージ 13.3 新しいスナップショットのライブ作成\n- 14 ヒントとテクニック 14.1 仮想マシンのパフォーマンスを向上させる 14.2 実際のパーティションをハードディスクイメージのシングルプライマリパーティションとして使う 14.2.1 カーネルと initrd を手動で指定する 14.2.2 MBR で仮想ディスクをシミュレートする 14.2.2.1 device-mapper を使う 14.2.2.2 リニア RAID を使う 14.2.2.3 ネットワークブロックデバイスを使う 14.3 ブート時に QEMU 仮想マシンを開始する 14.3.1 libvirt を使う 14.3.2 systemd サービスを使う 14.4 マウスの統合 14.5 ホスト USB デバイスのパススルー 14.6 SPICE による USB リダイレクト 14.6.1 udev による自動 USB 転送 14.7 KSM の有効化 14.8 マルチモニターのサポート 14.9 Custom display resolution 14.10 コピーアンドペースト 14.10.1 SPICE 14.10.2 qemu-vdagent 14.11 Windows 特有のノート 14.11.1 高速スタートアップ 14.11.2 リモートデスクトッププロトコル 14.12 物理機器にインストールされた Linux システムのクローン 14.13 x86_64 から arm/arm64 環境への chrooting 14.13.1 sudo in chroot 14.14 マウス入力をつかまない\n- 15 トラブルシューティング 15.1 マウスカーソルが敏感すぎたり迷走する 15.2 カーソルが表示されない 15.3 2つの異なるマウスカーソルが表示される 15.4 VNC 使用時のキーボードの問題 15.5 キーボードが壊れているまたは矢印キーが動作しない 15.6 キーマップファイルを読み込めない 15.7 ウィンドウのリサイズでゲストのディスプレイが引き伸ばされる 15.8 ioctl(KVM_CREATE_VM) failed: 16 Device or resource busy 15.9 libgfapi エラーメッセージ 15.10 ライブ環境でカーネルパニックが発生する 15.11 Windows 7 ゲストの音質が酷い 15.12 Could not access KVM kernel module: Permission denied 15.13 Windows 仮想マシンを起動したときに \"System Thread Exception Not Handled\" 15.14 特定の Windows のゲームやアプリケーションでクラッシュやブルスクリーンが発生する 15.15 高い割り込みレイテンシとマイクロスタッタリング 15.16 QXL ビデオの低解像度化 15.17 セキュアブート対応 OVMF を使用すると仮想マシンが起動しない 15.18 仮想マシンが Arch ISO で起動しない 15.19 ゲスト CPU の割り込みが発生しない 15.20 sddm を使用した KDE でログイン時に spice-vdagent が自動的に起動しない\n- 16 参照\n\n- 1.1 QEMU バリアンツ\n- 1.2 Arch Linux で利用可能なパッケージの詳細\n\n- 3.1 ハードディスクイメージの作成 3.1.1 オーバーレイストレージイメージ 3.1.2 イメージのリサイズ 3.1.2.1 イメージの縮小 3.1.3 イメージの変換\n- 3.2 インストールメディアを準備する\n- 3.3 オペレーティングシステムのインストール\n- 3.4 既製の仮想マシンイメージ\n\n- 3.1.1 オーバーレイストレージイメージ\n- 3.1.2 イメージのリサイズ 3.1.2.1 イメージの縮小\n- 3.1.3 イメージの変換\n\n- 3.1.2.1 イメージの縮小\n\n- 4.1 KVM を有効にする\n- 4.2 IOMMU (Intel VT-d/AMD-Vi) サポートを有効にする\n- 4.3 UEFI モードでの起動 4.3.1 Enabling Secure Boot\n- 4.4 Trusted Platform Module のエミュレーション\n\n- 4.3.1 Enabling Secure Boot\n\n- 5.1 ネットワーク\n- 5.2 QEMU のポートフォワーディング\n- 5.3 vsock 経由で SSH にアクセスする\n- 5.4 QEMU の内蔵 SMB サーバ\n- 5.5 9pfs VirtFS によるホストファイル共有\n- 5.6 virtiofsd によるホストファイル共有 5.6.1 通常のユーザーとして virtiofsd を実行する 5.6.2 root として virtiofsd を実行する 5.6.3 QEMU を起動する 5.6.4 Linux ゲストで共有を使う 5.6.5 Windows ゲストで共有を使う\n- 5.7 ゲストのパーティションをホストにマウントする 5.7.1 raw イメージからパーティションをマウントする 5.7.1.1 手動でバイトオフセットを指定する 5.7.1.2 loop モジュールでパーティションを自動検出する 5.7.1.3 kpartx を使う 5.7.2 qcow2 イメージからパーティションをマウントする\n\n- 5.6.1 通常のユーザーとして virtiofsd を実行する\n- 5.6.2 root として virtiofsd を実行する\n- 5.6.3 QEMU を起動する\n- 5.6.4 Linux ゲストで共有を使う\n- 5.6.5 Windows ゲストで共有を使う\n\n- 5.7.1 raw イメージからパーティションをマウントする 5.7.1.1 手動でバイトオフセットを指定する 5.7.1.2 loop モジュールでパーティションを自動検出する 5.7.1.3 kpartx を使う\n- 5.7.2 qcow2 イメージからパーティションをマウントする\n\n- 5.7.1.1 手動でバイトオフセットを指定する\n- 5.7.1.2 loop モジュールでパーティションを自動検出する\n- 5.7.1.3 kpartx を使う\n\n- 6.1 リンク層アドレス\n- 6.2 ユーザーモードネットワーク 6.2.1 SLIRP 6.2.2 passt\n- 6.3 QEMU の Tap ネットワーク 6.3.1 ホストオンリーネットワーク 6.3.2 内部ネットワーク 6.3.3 qemu-bridge-helper を使用したブリッジネットワーク 6.3.4 ブリッジを手動で作成する 6.3.5 iptables による物理デバイスと Tap デバイスのネットワーク共有\n- 6.4 VDE2 によるネットワーク 6.4.1 VDE とは? 6.4.2 基本 6.4.3 起動スクリプト 6.4.4 他の方法\n- 6.5 VDE2 Bridge 6.5.1 基本 6.5.2 起動スクリプト\n- 6.6 省略記法の設定\n\n- 6.2.1 SLIRP\n- 6.2.2 passt\n\n- 6.3.1 ホストオンリーネットワーク\n- 6.3.2 内部ネットワーク\n- 6.3.3 qemu-bridge-helper を使用したブリッジネットワーク\n- 6.3.4 ブリッジを手動で作成する\n- 6.3.5 iptables による物理デバイスと Tap デバイスのネットワーク共有\n\n- 6.4.1 VDE とは?\n- 6.4.2 基本\n- 6.4.3 起動スクリプト\n- 6.4.4 他の方法\n\n- 6.5.1 基本\n- 6.5.2 起動スクリプト\n\n- 7.1 std\n- 7.2 qxl\n- 7.3 vmware\n- 7.4 virtio\n- 7.5 cirrus\n- 7.6 none\n\n- 8.1 ホストで SPICE サポートを有効にする\n- 8.2 SPICE クライアントでゲストに接続する 8.2.1 SPICE クライアントを手動で実行する 8.2.2 QEMU で SPICE クライアントを実行する\n- 8.3 ゲストで SPICE サポートを有効にする\n- 8.4 SPICE によるパスワード認証\n- 8.5 SPICE による TLS 暗号化通信\n\n- 8.2.1 SPICE クライアントを手動で実行する\n- 8.2.2 QEMU で SPICE クライアントを実行する\n\n- 9.1 基本的なパスワード認証\n\n- 10.1 オーディオバックエンドを作成する\n- 10.2 オーディオバックエンドを使用する 10.2.1 Intel HD Audio 10.2.2 Intel 82801AA AC97 10.2.3 VirtIO sound\n\n- 10.2.1 Intel HD Audio\n- 10.2.2 Intel 82801AA AC97\n- 10.2.3 VirtIO sound\n\n- 11.1 Arch Linux ゲストを用意する 11.1.1 メモリバルーニング\n- 11.2 Windows ゲストを用意する 11.2.1 Windows 用の virtio ドライバ 11.2.2 ブロックデバイスドライバ 11.2.2.1 Windows の新規インストール 11.2.2.2 virtio を使用するように既存の Windows 仮想マシンを変更する 11.2.3 ネットワークドライバ 11.2.4 バルーンドライバ 11.2.5 virtiofsd の共有を使う\n- 11.3 FreeBSD ゲストを用意する\n\n- 11.1.1 メモリバルーニング\n\n- 11.2.1 Windows 用の virtio ドライバ\n- 11.2.2 ブロックデバイスドライバ 11.2.2.1 Windows の新規インストール 11.2.2.2 virtio を使用するように既存の Windows 仮想マシンを変更する\n- 11.2.3 ネットワークドライバ\n- 11.2.4 バルーンドライバ\n- 11.2.5 virtiofsd の共有を使う\n\n- 11.2.2.1 Windows の新規インストール\n- 11.2.2.2 virtio を使用するように既存の Windows 仮想マシンを変更する\n\n- 12.1 モニタコンソールにアクセスする 12.1.1 グラフィカルビュー 12.1.2 Telnet 12.1.3 UNIX ソケット 12.1.4 TCP 12.1.5 標準 I/O\n- 12.2 モニタコンソールを使って仮想マシンにキーボードの押下を送信する\n- 12.3 モニタコンソールを使ってスナップショットを作成・管理する\n- 12.4 immutable モードで仮想マシンを実行する\n- 12.5 モニタコンソールによる一時停止と電源オプション\n- 12.6 仮想マシンのスクリーンショットを取得する\n\n- 12.1.1 グラフィカルビュー\n- 12.1.2 Telnet\n- 12.1.3 UNIX ソケット\n- 12.1.4 TCP\n- 12.1.5 標準 I/O\n\n- 13.1 QMP を開始する\n- 13.2 親イメージへの子イメージのライブマージ\n- 13.3 新しいスナップショットのライブ作成\n\n- 14.1 仮想マシンのパフォーマンスを向上させる\n- 14.2 実際のパーティションをハードディスクイメージのシングルプライマリパーティションとして使う 14.2.1 カーネルと initrd を手動で指定する 14.2.2 MBR で仮想ディスクをシミュレートする 14.2.2.1 device-mapper を使う 14.2.2.2 リニア RAID を使う 14.2.2.3 ネットワークブロックデバイスを使う\n- 14.3 ブート時に QEMU 仮想マシンを開始する 14.3.1 libvirt を使う 14.3.2 systemd サービスを使う\n- 14.4 マウスの統合\n- 14.5 ホスト USB デバイスのパススルー\n- 14.6 SPICE による USB リダイレクト 14.6.1 udev による自動 USB 転送\n- 14.7 KSM の有効化\n- 14.8 マルチモニターのサポート\n- 14.9 Custom display resolution\n- 14.10 コピーアンドペースト 14.10.1 SPICE 14.10.2 qemu-vdagent\n- 14.11 Windows 特有のノート 14.11.1 高速スタートアップ 14.11.2 リモートデスクトッププロトコル\n- 14.12 物理機器にインストールされた Linux システムのクローン\n- 14.13 x86_64 から arm/arm64 環境への chrooting 14.13.1 sudo in chroot\n- 14.14 マウス入力をつかまない\n\n- 14.2.1 カーネルと initrd を手動で指定する\n- 14.2.2 MBR で仮想ディスクをシミュレートする 14.2.2.1 device-mapper を使う 14.2.2.2 リニア RAID を使う 14.2.2.3 ネットワークブロックデバイスを使う\n\n- 14.2.2.1 device-mapper を使う\n- 14.2.2.2 リニア RAID を使う\n- 14.2.2.3 ネットワークブロックデバイスを使う\n\n- 14.3.1 libvirt を使う\n- 14.3.2 systemd サービスを使う\n\n- 14.6.1 udev による自動 USB 転送\n\n- 14.10.1 SPICE\n- 14.10.2 qemu-vdagent\n\n- 14.11.1 高速スタートアップ\n- 14.11.2 リモートデスクトッププロトコル\n\n- 14.13.1 sudo in chroot\n\n- 15.1 マウスカーソルが敏感すぎたり迷走する\n- 15.2 カーソルが表示されない\n- 15.3 2つの異なるマウスカーソルが表示される\n- 15.4 VNC 使用時のキーボードの問題\n- 15.5 キーボードが壊れているまたは矢印キーが動作しない\n- 15.6 キーマップファイルを読み込めない\n- 15.7 ウィンドウのリサイズでゲストのディスプレイが引き伸ばされる\n- 15.8 ioctl(KVM_CREATE_VM) failed: 16 Device or resource busy\n- 15.9 libgfapi エラーメッセージ\n- 15.10 ライブ環境でカーネルパニックが発生する\n- 15.11 Windows 7 ゲストの音質が酷い\n- 15.12 Could not access KVM kernel module: Permission denied\n- 15.13 Windows 仮想マシンを起動したときに \"System Thread Exception Not Handled\"\n- 15.14 特定の Windows のゲームやアプリケーションでクラッシュやブルスクリーンが発生する\n- 15.15 高い割り込みレイテンシとマイクロスタッタリング\n- 15.16 QXL ビデオの低解像度化\n- 15.17 セキュアブート対応 OVMF を使用すると仮想マシンが起動しない\n- 15.18 仮想マシンが Arch ISO で起動しない\n- 15.19 ゲスト CPU の割り込みが発生しない\n- 15.20 sddm を使用した KDE でログイン時に spice-vdagent が自動的に起動しない\n\n"
    },
    {
      "title": "インストール",
      "level": 2,
      "content": "qemu-full パッケージ (または GUI が必要ない場合は qemu-base とデフォルトで x86_64 エミュレーションのみの qemu-desktop) をインストールしてください。また、任意で以下のパッケージもインストールしてください:\n\n- qemu-block-gluster - GlusterFS ブロックのサポート\n- qemu-block-iscsi - iSCSI ブロックのサポート\n- samba - SMB/CIFS サーバーのサポート\n\nあるいは、ユーザーモードと静的のバリアントとして qemu-user-static が存在します。\n\n"
    },
    {
      "title": "QEMU バリアンツ",
      "level": 3,
      "content": "QEMUには、さまざまなユースケースに適したいくつかのバリアンツが用意されています。\n\n最初の分類として、QEMU はフルシステムエミュレーションモードとユーザーモードエミュレーションモードの 2 種類を提供しています:\n\nQEMU には動的リンク型と静的リンク型のバリアンツが提供されています:\n\nArch Linux の場合、フルシステムエミュレーションは以下のように提供されます:\n\nヘッドレス版と非ヘッドレス版は同じ名前のコマンド(例: qemu-system-x86_64)をインストールするため、両方を同時にインストールすることはできないことに注意してください。\n\n"
    },
    {
      "title": "Arch Linux で利用可能なパッケージの詳細",
      "level": 3,
      "content": "- qemu-desktop パッケージはフルシステムエミュレーション用の x86_64 アーキテクチャエミュレータを提供します(qemu-system-x86_64)。 qemu-emulators-full パッケージは、x86_64 ユーザーモード版 (qemu-x86_64) を提供し、サポートされている他のアーキテクチャについても、フルシステム版と ユーザーモード版の両方(例: qemu-system-arm および qemu-arm )が含まれています。\n- これらのパッケージのヘッドレスバージョン (フルシステムエミュレーションにのみ適用可能) は、 qemu-base (x86_64-のみ) および qemu-emulators-full (その他のアーキテクチャ) です。\n- フルシステムエミュレーションは、別のパッケージに含まれるいくつかの QEMU モジュールを使用して拡張することができます: qemu-block-gluster, qemu-block-iscsi, qemu-guest-agent.\n- qemu-user-static は QEMU がサポートする全てのターゲットアーキテクチャにユーザーモードと静的バリアントを提供します。インストールされる QEMU コマンドは qemu-target_architecture-static という名前で、例えば intel 64-bit CPU 用は qemu-x86_64-static です。\n\n"
    },
    {
      "title": "QEMU のグラフィカルフロントエンド",
      "level": 2,
      "content": "VirtualBox や VMware などの他の仮想化プログラムと違って、QEMU は仮想マシンを管理するための GUI(仮想マシン実行時に表示されるウィンドウを除く)を提供せず、保存された設定を使って永続的な仮想マシンを作成する方法も提供しません。仮想マシンを起動するためのカスタムスクリプトを作成していない限り、仮想マシンを実行するためのすべてのパラメータは、起動のたびにコマンドラインで指定する必要があります。\n\nLibvirt は、QEMU 仮想マシンを管理するための便利な方法を提供します。利用可能なフロントエンドについては、libvirtクライアントの一覧 を参照してください。\n\n"
    },
    {
      "title": "ハードディスクイメージの作成",
      "level": 3,
      "content": "CD-ROM やネットワークからライブシステムを起動するのでない (そしてオペレーティングシステムをハードディスクイメージにインストールしない) 限り、QEMU を実行するにはハードディスクイメージが必要になります。ハードディスクイメージはエミュレートするハードディスクの内容を保存するファイルです。\n\nハードディスクイメージを raw にすると、ゲストからは文字通りバイト単位で等しいようになり、ホスト上のゲストハードドライブをフルに使用することになります。この方法は I/O のオーバーヘッドを最小限に抑えますが、ゲスト上の未使用領域はホスト上で使用できないため、多くの領域が無駄になる可能性があります。\n\nまた、ハードディスクイメージを qcow2 などのフォーマットにすることもできます。ゲストオペレーティングシステムが実際に仮想ハードディスク上のセクタに書き込んだ時にイメージファイルに容量を割り当てます。ホストシステムで占める容量はかなり少なくて済み、ゲストオペレーションにはフルサイズのイメージとして見えます。QEMU のスナップショット機能にも対応しています (詳しくは#モニタコンソールを使ってスナップショットを作成・管理を参照)。こちらの形式では raw と違ってパフォーマンスに多少影響を与えます。\n\nQEMU にはハードディスクイメージを作成するための qemu-img コマンドがあります。例えば raw フォーマットで 4GiB イメージを作成するには:\n\n```\n$ qemu-img create -f raw image_file 4G\n```\n\n代わりに -f qcow2 を使って qcow2 ディスクを作成することもできます。\n\n```\n$ qemu-img create -f qcow2 image_file -o nocow=on 4G\n```\n\n"
    },
    {
      "title": "オーバーレイストレージイメージ",
      "level": 4,
      "content": "一度ストレージメディアを作成してから ('backing' イメージ)、QEMU にイメージへの変更を overlay イメージとして維持させることができます。これによってストレージメディアを前の状態に戻すことが可能になります。戻りたい時点で、オリジナルの backing イメージを元に新しい overlay イメージを作成することで戻すことができます。\n\noverlay イメージを作成するには、次のようにコマンドを実行してください:\n\n```\n$ qemu-img create -o backing_file=img1.raw,backing_fmt=raw -f qcow2 img1.cow\n```\n\nその後通常通り QEMU 仮想マシンを起動することができます (仮想化システムを実行するを参照):\n\n```\n$ qemu-system-x86_64 img1.cow\n```\n\nbacking イメージには変更が加えられず、ストレージへの追記は overlay イメージファイルに保存されるようになります。\n\nbacking イメージのパスが変更された場合、修正が必要になります。\n\nオリジナルの backing イメージのパスからこのイメージに繋がるようにしてください。必要ならば、オリジナルのパスに新しいパスへのシンボリックリンクを作成します。次のようなコマンドを実行してください:\n\n```\n$ qemu-img rebase -b /new/img1.raw /new/img1.cow\n```\n\nあなたの判断で、backing イメージの古いパスがチェックされない '安全でない' rebase を実行することもできます:\n\n```\n$ qemu-img rebase -u -b /new/img1.raw /new/img1.cow\n```\n\n"
    },
    {
      "title": "イメージのリサイズ",
      "level": 4,
      "content": "qemu-img 実行可能ファイルには resize オプションがあり、ハードドライブイメージの簡単なリサイズができます。このコマンドは raw と qcow2 の両方で使えます。例えば、イメージ容量を 10GiB 増やすには、次を実行してください:\n\n```\n$ qemu-img resize disk_image +10G\n```\n\nディスクイメージを拡大した後、仮想マシン内でファイルシステムおよびパーティションツールを使用して、新しいスペースを実際に使い始める必要があります。\n\nディスクイメージを縮小する場合は、仮想マシン内のファイルシステムおよびパーティションツールを使用してまず割り当てられたファイル・システムとパーティション・サイズを縮小し、それに応じてディスクイメージを縮小する必要があります。Windows ゲストの場合、\"ハードディスクパーティションの作成とフォーマット\" コントロールパネルを開きます。\n\nイメージ領域を 10 GiB 減らすために、次のコマンドを実行します:\n\n```\n$ qemu-img resize --shrink disk_image -10G\n```\n\n"
    },
    {
      "title": "イメージの変換",
      "level": 4,
      "content": "qemu-img convert を使用して、イメージを他のフォーマットに変換できます。次の例では、 raw イメージを qcow2 に変換する方法を示します:\n\n```\n$ qemu-img convert -f raw -O qcow2 input.img output.qcow2\n```\n\n元の入力ファイルは削除されません。\n\n"
    },
    {
      "title": "インストールメディアを準備する",
      "level": 3,
      "content": "ディスクイメージにオペレーティングシステムをインストールするには、オペレーティングシステムのインストールメディア (例: 光ディスク、USB ドライブ、ISO イメージ) が必要です。QEMU はメディアに直接アクセスできないのでインストールメディアをマウントしてはいけません。\n\n```\n$ dd if=/dev/cdrom of=cd_image.iso bs=4k\n```\n\n"
    },
    {
      "title": "オペレーティングシステムのインストール",
      "level": 3,
      "content": "ここで初めてエミュレータを起動することになります。ディスクイメージにオペレーティングをインストールするには、ディスクイメージとインストールメディアの両方を仮想マシンに結びつけて、インストールメディアから起動するようにする必要があります。\n\n例えば i386 ゲストで、CD-ROM としてのブータブル ISO ファイルと raw ディスクイメージからインストールするには:\n\n```\n$ qemu-system-x86_64 -cdrom iso_image -boot order=d -drive file=disk_image,format=raw\n```\n\n他のメディアタイプ(フロッピー、ディスクイメージ、物理ドライブなど)の読み込みについては qemu(1) を、その他の便利なオプションについては #仮想化システムを実行する を見て下さい。\n\nオペレーティングシステムのインストールが終了したら、直接 QEMU イメージを起動することができます( #仮想化システムを実行する を参照)。\n\n- 少なくとも設定中や実験中は、 -boot order=x を指定する代わりにブートメニュー: -boot menu=on を使う方が快適だと感じるユーザもいるかもしれません。\n- QEMUをヘッドレスモードで実行する場合、QEMU はデフォルトでポート 5900 でローカル VNC サーバを起動します。ゲスト OS への接続に TigerVNC を使用することができます: vncviewer :5900\n- インストールプロセスでフロッピーや CD を替える必要がある場合、QEMU マシンモニター (仮想マシンのウィンドウで Ctrl+Alt+2 を押す) を使って仮想マシンからストレージデバイスを取り外したりアタッチすることができます。ブロックデバイスを見るには info block を、デバイスをスワップアウトするには change コマンドを使って下さい。Ctrl+Alt+1 を押せば仮想マシンに戻ります。\n\n"
    },
    {
      "title": "既製の仮想マシンイメージ",
      "level": 3,
      "content": "多くの場合、クラウド環境などで自分でオペレーティングシステムを手動でインストールする必要はありませんし、望ましくないこともあります。幸い、多くの既製のイメージがさまざまなプロバイダーからダウンロード可能です。\n\nArch Linux の場合、arch-boxes プロジェクトに 毎週のイメージリリース があります。\n\n同様のイメージは、Fedora や Debian でも利用可能です。\n\n"
    },
    {
      "title": "仮想化システムを実行する",
      "level": 2,
      "content": "qemu-system-* バイナリ (例えば qemu-system-i386 や qemu-system-x86_64 など、エミュレートするアーキテクチャによって異なります) を使って仮想化システムを実行します。使用方法は:\n\n```\n$ qemu-system-x86_64 options disk_image\n```\n\n全ての qemu-system-* バイナリでオプションは共通です、全てのオプションのドキュメントは qemu(1) を見て下さい。\n\n通常、オプションに多くの可能な値がある場合は、\n\n```\n$ qemu-system-x86_64 option help\n```\n\nを使って、すべての可能な値をリストアップできます。プロパティをサポートしている場合は\n\n```\n$ qemu-system-x86_64 option value,help\n```\n\nを使って、使用可能なプロパティをすべて一覧表示できます。\n\n例えば:\n\n```\n$ qemu-system-x86_64 -machine help\n$ qemu-system-x86_64 -machine q35,help\n$ qemu-system-x86_64 -device help\n$ qemu-system-x86_64 -device qxl,help\n```\n\nこれらのメソッドと qemu(1) ドキュメントを使用して、以降のセクションで使用されるオプションを理解できます。\n\nデフォルトで、QEMU は仮想マシンのビデオ出力をウィンドウに表示します。注意: QEMU ウィンドウの中をクリックすると、マウスポインタが取り込まれます。ポインタを戻すには、Ctrl+Alt+g を押して下さい。\n\n"
    },
    {
      "title": "KVM を有効にする",
      "level": 3,
      "content": "VM (Kernel-based Virtual Machine) による完全な仮想化をあなたの Linux カーネルとハードウェアがサポートし、必要なカーネルモジュールがロードされている必要があります。詳細については、KVM を参照してください。\n\nQEMU を KVM モードで開始するには、開始オプションに -accel kvm を追加してください。実行中の仮想マシンで KVM が有効になっているかどうか確認するには、Ctrl+Alt+Shift+2 を使って #QEMU モニタ に入り、info kvm と入力してください。\n\n- -machine オプションの引数 accel=kvm は -enable-kvm または -accel kvm オプションと同等です。\n- CPU モデル host は KVM を必要とします。\n- GUI ツールを使って仮想マシンを開始した時にパフォーマンスが出ない場合、KVM サポートを確認してください。QEMU がソフトウェアエミュレーションにフォールバックしている可能性があります。\n- ブルースクリーンを出さずに Windows 7 や Windows 8 を正しく起動するには KVM を有効にする必要があります。\n\n"
    },
    {
      "title": "IOMMU (Intel VT-d/AMD-Vi) サポートを有効にする",
      "level": 3,
      "content": "最初に IOMMU を有効にします。OVMF による PCI パススルー#IOMMU の有効化 を参照してください。\n\n-device intel-iommu を追加して IOMMU デバイスを作成してください:\n\n```\n$ qemu-system-x86_64 -enable-kvm -machine q35 -device intel-iommu -cpu host ..\n```\n\n```\nDevice at bus pcie.0 addr 09.0 requires iommu notifier which is currently not supported by intel-iommu emulation\n```\n\n"
    },
    {
      "title": "UEFI モードでの起動",
      "level": 3,
      "content": "QEMU が使用するデフォルトのファームウェアは SeaBIOS で、これはレガシー BIOS の実装です。QEMU はデフォルトの読み取り専用(ROM)イメージとして /usr/share/qemu/bios-256k.bin (seabios で提供される) を使用します。他のファームウェアファイルを選択するには -bios 引数を使用します。しかし、UEFI が正しく動作するためには書き込み可能なメモリが必要であるため、代わりに PC システムフラッシュ をエミュレートする必要があります。\n\nOVMF は仮想マシンの UEFI サポートを有効にするための TianoCore プロジェクトです。 edk2-ovmf パッケージで インストール できます。\n\nOVMF をファームウェアとして使うには 2 つの方法があります。一つは /usr/share/edk2/x64/OVMF.4m.fd をコピーして書き込み可能にし、pflash ドライブとして利用する方法です:\n\n```\n-drive if=pflash,format=raw,file=/copy/of/OVMF.4m.fd\n```\n\nUEFI 設定への全ての変更はこのファイルに直接保存されます。\n\nもう一つのより好ましい方法は OVMF を二つのファイルに分割することです。最初のファイルは読み込み専用でファームウェアの実行ファイルを保存し、2番目のファイルは書き込み可能な変数ストアとして使われます。利点はファームウェアファイルをコピーせずに直接使うことができ、 pacman によって自動的にアップデートされることです。\n\n/usr/share/edk2/x64/OVMF_CODE.4m.fd を最初のリードオンリーの pflash ドライブとして使用します。/usr/share/edk2/x64/OVMF_VARS.4m.fd をコピーして書き込み可能にし、2台目の書き込み可能な pflash ドライブとして使用します:\n\n```\n-drive if=pflash,format=raw,readonly=on,file=/usr/share/edk2/x64/OVMF_CODE.4m.fd \\\n-drive if=pflash,format=raw,file=/copy/of/OVMF_VARS.4m.fd\n```\n\n"
    },
    {
      "title": "Enabling Secure Boot",
      "level": 4,
      "content": "The first requirement to enabling Secure Boot in a VM is to use the q35 machine type, and replace /usr/share/edk2/x64/OVMF_CODE.4m.fd with /usr/share/edk2/x64/OVMF_CODE.secboot.4m.fd.\n\nThe second requirement is to use an OVMF_VARS file that has Secure Boot keys installed, which is not provided by the upstream project[1].\n\nUnlike some other distributions, Arch does not yet provide (as of 2024-12-06) its own /usr/share/edk2/x64/OVMF_VARS.secboot.4m.fd file containing pre-enrolled keys: see テンプレート:Issue.\n\nA simple workaround is to use Fedora's version, as mentioned in this forum post:\n\n- download a recent noarch RPM of Fedora's edk2-ovmf for x86_64 from the builds list (direct link to latest F42 version as of 2024-12-06)\n- extract the archive and convert the desired file to a suitable format[2]:\n\n```\n$ qemu-img convert -O raw -f qcow2 edk2-ovmf-*.noarch/usr/share/edk2/ovmf/OVMF_VARS_4M.secboot.qcow2 OVMF_VARS_4M.secboot.fd\n```\n\nNow you can make copies of the created OVMF_VARS_4M.secboot.fd and use them for your Secure Boot-enabled VMs.\n\nSee also KVM#Secure Boot for how to manually enroll keys into a firmware image.\n\n"
    },
    {
      "title": "Trusted Platform Module のエミュレーション",
      "level": 3,
      "content": "QEMU は、Windows 11 (TPM 2.0 が必要)などの一部のシステムで必要とされる Trusted Platform Module をエミュレートできます。\n\nソフトウェア TPM の実装を提供する swtpm パッケージをインストール します。 TPM のデータを格納するディレクトリを作成します(/path/to/mytpmを例として使用します)。以下のコマンドを実行し、エミュレータを起動します:\n\n```\n$ swtpm socket --tpm2 --tpmstate dir=/path/to/mytpm --ctrl type=unixio,path=/path/to/mytpm/swtpm-sock\n```\n\n/path/to/mytpm/swtpm-sock は swtpm が作成します: これはQEMUが接続する UNIX ソケットです。どのディレクトリに置いてもかまいません。\n\nデフォルトでは、swtpm は TPM バージョン 1.2 エミュレータを起動します。 --tpm2 オプションを指定すると、TPM 2.0 のエミュレーションが有効になります。\n\n最後に、QEMU に以下のオプションを追加します:\n\n```\n-chardev socket,id=chrtpm,path=/path/to/mytpm/swtpm-sock \\\n-tpmdev emulator,id=tpm0,chardev=chrtpm \\\n-device tpm-tis,tpmdev=tpm0\n```\n\nすると、仮想マシン内で TPM が使用できるようになります。仮想マシンをシャットダウンすると、swtpm は自動的に終了します。\n\n詳しくは the QEMU documentation を参照してください。\n\nもしゲスト OS が TPM デバイスを認識しない場合、CPU モデルとトポロジー オプションを調整してみてください。問題を引き起こしているかもしれません。\n\n"
    },
    {
      "title": "ネットワーク",
      "level": 3,
      "content": "ファイルを転送できるネットワークプロトコルであれば NFS, SMB, NBD, HTTP, FTP, SSH など何でも使ってホストとゲスト OS 間でデータを共有することができます、ただしネットワークを適切に設定して適切なサービスを有効にする必要があります。\n\nデフォルトの SLIRP ベースのユーザーモードネットワークは IP アドレス 10.0.2.2 でゲストがホスト OS にアクセスするのを許可します。ホスト OS で動作する全てのサーバー、SSH サーバーや SMB サーバーなどは、この IP アドレスでアクセスすることが可能になります。そしてゲストでは、SMB や NFS でホストのディレクトリをマウントしたり、ホストの HTTP サーバーなどにアクセスすることが可能です。 ゲスト OS で動作しているサーバーにホスト OS がアクセスすることはできませんが、他のネットワーク設定を使えば不可能ではありません (#QEMU の Tap ネットワーク を参照)。\n\n"
    },
    {
      "title": "QEMU のポートフォワーディング",
      "level": 3,
      "content": "QEMU はホストからゲストへポートを転送し、例えばホストからゲスト上で動作している SSH サーバーへの接続を可能にします。\n\n例えば、ホストのポート 60022 とゲストのポート 22(SSH) をバインドするには、次のようなコマンドで QEMU を起動します:\n\n```\n$ qemu-system-x86_64 disk_image -nic user,hostfwd=tcp::60022-:22\n```\n\nゲストで sshd が動いていることを確認し、接続します:\n\n```\n$ ssh guest-user@127.0.0.1 -p 60022\n```\n\nSSHFS を使って共有読み込みと書き込みのためにゲストのファイルシステムをホストにマウントできます。\n\n複数のポートを転送するには、hostfwd を -nic 引数を繰り返します。例えば VNC のポートはこうなります:\n\n```\n$ qemu-system-x86_64 disk_image -nic user,hostfwd=tcp::60022-:22,hostfwd=tcp::5900-:5900\n```\n\n"
    },
    {
      "title": "vsock 経由で SSH にアクセスする",
      "level": 3,
      "content": "VM に接続するための安全で便利な方法は、vsock(7) を使用して SSH を使うことです。この方法を利用するには、VM が systemd ベースである必要があります。\n\nまず、特別なデバイスを使用して QEMU を起動します:\n\n```\n-device vhost-vsock-pci,id=vhost-vsock-pci0,guest-cid=555\n```\n\ncid はユーザーが有効な 32 ビット番号として選択する必要があります（vsock(7) を参照）。systemd が VM が vhost-vsock デバイスで起動されたことを検出すると、自動的に systemd-ssh-generator を使用して SSH サーバーを起動します。\n\n次に、以下のように VM に接続できます:\n\n```\n$ ssh user@vsock/555\n```\n\nこれが動作するのは、/etc/ssh/ssh_config.d/20-systemd-ssh-proxy.conf が SSH クライアントに systemd-ssh-proxy を使用して vsock 経由で SSH を使うよう指示しているためです。\n\nさらに、systemd.system-credentials(7) を使用することで、ダウンロードしたイメージを実行しようとする場合に便利な root ユーザーの認証済みキー ファイルを注入できます。これを次のように行うことができます:\n\n```\n-smbios type=11,value=io.systemd.credential.binary:ssh.authorized_keys.root=c3NoLWVkMjU1MTkgQUFBQUMzTnphQzFsWkRJMU5URTVBQUFBSU9sVFE4ejlpeWxoMTMreCtFVFJ1R1JEaHpIVVRnaCt2ekJLOGY3TEl5eTQ=\n```\n\n公開鍵行は base64 エンコードされた文字列として提供する必要があります。次のように行えます:\n\n```\necho \"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOlTQ8z9iylh13+x+ETRuGRDhzHUTgh+vzBK8f7LIyy4\" | base64\n```\n\n-smbios type=11,value=io.systemd... の仕組みを使って、systemd によって処理されるさまざまなその他の魔法のような変数を注入することもできます。詳細については、systemd ドキュメント: システムとサービスの資格情報 を参照してください。\n\n"
    },
    {
      "title": "QEMU の内蔵 SMB サーバ",
      "level": 3,
      "content": "QEMUのドキュメントには \"内蔵の\" SMB サーバーがあると書かれていますが、実際は /tmp/qemu-smb.ランダムな文字列 にある自動生成の smb.conf ファイルでホスト上で Samba を起動し、別の IP アドレス(デフォルトでは 10.0.2.4)でゲストからアクセス可能にするだけのものです。これはユーザーネットワークでのみ動作し、ホスト上で通常の Samba サービスを起動したくない場合に便利です。ホスト上で共有を設定した場合、ゲストもアクセスすることができます。\n\nオプション smb= で共有設定できるのは1つのディレクトリだけですが、QEMU がシンボリックリンクに従うように SMB を設定すれば、(仮想マシン実行中でも)共有ディレクトリにシンボリックリンクを作成するだけで簡単に他のディレクトリを追加できます。このようなことをすることはありませんが、実行中の SMB サーバーの設定を後述のように変更することができます。\n\nホスト上に Samba がインストールされている必要があります。この機能を有効にするには、次のようなコマンドで QEMU を起動します。\n\n```\n$ qemu-system-x86_64 -nic user,id=nic0,smb=shared_dir_path disk_image\n```\n\nshared_dir_path はゲストとホストで共有したいディレクトリです。\n\nこれで、ゲストから、ホスト 10.0.2.4 上の共有ディレクトリに 共有名 \"qemu\" でアクセスできるようになります。例えば、Windowsエクスプローラで \\\\10.0.2.4\\qemu に移動します。\n\n- -net user,smb=shared_dir_path1 -net user,smb=shared_dir_path2 や -net user,smb=shared_dir_path1,smb=shared_dir_path2 のように共有オプションを複数回使用した場合、最後に定義したものだけが共有されます。\n- ゲストシステムが Windows で共有フォルダにアクセスできない場合、NetBIOSプロトコルが有効になっているかどうかと NetBIOS プロトコルが使用する ポート がファイアウォールでブロックされていないか確認してください。\n- 共有フォルダーにアクセスできず、ゲストシステムが Windows 10 Enterprise または Education、Windows Server 2016 の場合、ゲストアクセスを有効にします 。\n- #QEMU の Tap ネットワーク を使用する場合、SMB を取得するには -device virtio-net,netdev=vmnic -netdev user,id=vmnic,smb=shared_dir_path を使います。\n\n複数のディレクトリを共有し、仮想マシンの実行中にディレクトリの追加や削除を行う方法として、空のディレクトリを共有し、共有ディレクトリ内のディレクトリへのシンボリックリンクを作成/削除する方法があります。これを機能させるには、実行中の SMB サーバーの設定を以下のスクリプトで変更します。これは、ホスト側で実行可能に設定されていないファイルのゲスト側での実行も許可します。\n\n```\n#!/bin/sh\neval $(ps h -C smbd -o pid,args | grep /tmp/qemu-smb | gawk '{print \"pid=\"$1\";conf=\"$6}')\necho \"[global]\nallow insecure wide links = yes\n[qemu]\nfollow symlinks = yes\nwide links = yes\nacl allow execute always = yes\" >> \"$conf\"\n# in case the change is not detected automatically:\nsmbcontrol --configfile=\"$conf\" \"$pid\" reload-config\n```\n\nこれはゲストが初めてネットワークドライブに接続した後にのみ、qemu によって起動された実行中のサーバーに適用することができます。この代わりに、次のように設定ファイルに追加の共有を追加する方法があります:\n\n```\necho \"[myshare]\npath=another_path\nread only=no\nguest ok=yes\nforce user=username\" >> $conf\n```\n\nこの共有はゲスト上で \\\\10.0.2.4\\myshare として利用可能になります。\n\n"
    },
    {
      "title": "9pfs VirtFS によるホストファイル共有",
      "level": 3,
      "content": "QEMU ドキュメント を参照してください。\n\n"
    },
    {
      "title": "virtiofsd によるホストファイル共有",
      "level": 3,
      "content": "virtiofsd は virtiofsd パッケージに含まれています。これは、ホストとゲストの間でファイルを簡単に共有するための最新の高性能な方法です。利用可能なオプションの全リストは オンラインドキュメントまたは /usr/share/doc/virtiofsd/README.md を参照してください。\n\nvirtiofsd は、root または通常のユーザーとして実行するか選択できます。\n\n"
    },
    {
      "title": "通常のユーザーとして virtiofsd を実行する",
      "level": 4,
      "content": "まず virtiofsd を実行するユーザーに subuid(5) および subgid(5) の設定項目があることを確認します。Podman 記事の関連するセクション も参照してください。\n\n次にvirtiofsd を起動します:\n\n```\n$ unshare -r --map-auto -- /usr/lib/virtiofsd --socket-path=/tmp/vm-share.sock --shared-dir /tmp/vm-share --sandbox chroot\n```\n\n- unshare -r は、以後の新しいコマンドが、現在のユーザが root にマップされた状態で新しいユーザ名前空間で起動されるようにします。virtiofsd は root として実行されることを想定しているため、これは重要です。\n- /tmp/vm-share.sock はソケットファイルです\n- /tmp/vm-share はホストとゲスト仮想マシン間の共有ディレクトリです\n\n"
    },
    {
      "title": "root として virtiofsd を実行する",
      "level": 4,
      "content": "QEMU を実行するユーザーを kvm ユーザーグループ に追加してください。これは virtiofsd のソケットにアクセスするために必要です。変更を反映させるには、一度ログアウトが必要な場合があります。\n\nroot で virtiofsd を開始します:\n\n```\n# /usr/lib/virtiofsd --socket-path=/tmp/vm-share.sock --shared-dir /tmp/vm-share\n```\n\nここで\n\n- /tmp/vm-share.sock はソケットファイルです\n- /tmp/vm-share はホストとゲスト仮想マシン間の共有ディレクトリです\n\n作成されたソケットファイルは root のみアクセス権を持っています。以下のようにグループ kvm にアクセス権を与えます:\n\n```\n# chgrp kvm /tmp/vm-share.sock; chmod g+rxw /tmp/vm-share.sock\n```\n\n"
    },
    {
      "title": "QEMU を起動する",
      "level": 4,
      "content": "仮想マシン開始時に以下の設定オプションを追加します:\n\n```\n-m 4G\n-object memory-backend-memfd,id=mem,size=4G,share=on\n-numa node,memdev=mem \\\n-chardev socket,id=char0,path=/tmp/vm-share.sock\n-device vhost-user-fs-pci,chardev=char0,tag=myfs\n```\n\nここで\n\n- size=4G は -m 4G オプションで指定したサイズと一致する必要がある\n- /tmp/vm-share.sock は先に開始されたソケットファイルを指す\n- myfs は後でゲストで共有をマウントするために使用する識別子\n\n"
    },
    {
      "title": "Linux ゲストで共有を使う",
      "level": 4,
      "content": "ゲストに root でログインしたら、最新のディストリビューションで共有をマウントするだけです:\n\n```\n# mount -t virtiofs myfs /mnt\n```\n\nこのディレクトリはホストとゲストで共有されるはずです。\n\n"
    },
    {
      "title": "Windows ゲストで共有を使う",
      "level": 4,
      "content": "関連する Windows section を参照してください。\n\n"
    },
    {
      "title": "ゲストのパーティションをホストにマウントする",
      "level": 3,
      "content": "ホストシステムの下にドライブイメージをマウントすると、ゲストへのファイルの出し入れに便利な場合があります。これは仮想マシンが動作していないときに行う必要があります。\n\nホストにドライブをマウントする手順は、qemu イメージの raw や qcow2 といった種類によって異なります。この2つの形式のドライブをマウントする手順については、#rawイメージからのパーティションのマウント と #qcow2イメージからのパーティションのマウント で詳しく説明します。完全なドキュメントは Wikibooks:QEMU/Images#Mounting an image on the host を参照してください。\n\n"
    },
    {
      "title": "raw イメージからパーティションをマウントする",
      "level": 4,
      "content": "ループバックデバイスとして設定することで、 raw ディスクイメージファイル内のパーティションをマウントできます。\n\nディスクイメージのパーティションをマウントする一つの方法として、次のようなコマンドを使って特定のオフセットでディスクイメージをマウントする方法があります:\n\n```\n# mount -o loop,offset=32256 disk_image mountpoint\n```\n\noffset=32256 オプションは、実際には losetup プログラムに渡され、ファイルのバイトオフセット 32256 から始まり最後まで続くループバックデバイスをセットアップします。そしてこのループバックデバイスがマウントされます。パーティションの正確なサイズを指定するために sizelimit オプションを使うこともできますが、これは通常は不要です。\n\nディスクイメージによっては、必要なパーティションがオフセット 32256 から始まってない可能性があります。 fdisk -l disk_image を実行してイメージ内のパーティションを確認してください。fdisk は 512 バイトセクタ単位で起点と終点を表示するので、512 を掛けて正しいオフセットを mount に渡してください。\n\nLinux の loop ドライバは、実際にはループバックデバイスのパーティションをサポートしていますが、デフォルトでは無効になっています。有効にするには、以下のようにしてください:\n\n- ループバックデバイスを全て取り除いてください (マウントされているイメージをすべてアンマウントするなど)。\n- loop カーネルモジュールを アンロード し、 max_part=15 パラメータを設定してロードしてください。また、loop デバイスの最大数は max_loop パラメータで制御できます。\n\nイメージをループバックデバイスとして設定:\n\n```\n# losetup -f -P disk_image\n```\n\nこれで、作成されたデバイスが /dev/loop0 の場合、追加のデバイス /dev/loop0pX が自動的に作成されます。X はパーティションの番号です。これらのパーティションのループバックデバイスは直接マウントすることができます。例えば:\n\n```\n# mount /dev/loop0p1 mountpoint\n```\n\nudisksctl でディスクイメージをマウントする方法は Udisks#ループデバイスのマウント を参照してください。\n\nmultipath-tools パッケージの kpartx はデバイス上のパーティションテーブルを読み込んでパーティションごとに新しいデバイスを作成することができます。例えば:\n\n```\n# kpartx -a disk_image\n```\n\nこれでループバックデバイスがセットアップされ、/dev/mapper/ に必要なパーティションデバイスが作成されます。\n\n"
    },
    {
      "title": "qcow2 イメージからパーティションをマウントする",
      "level": 4,
      "content": "NBD (network block device) プロトコルを使ってディスクイメージを共有することができる qemu-nbd を使用してみます。\n\nまず、nbdモジュールをロードする必要があります:\n\n```\n# modprobe nbd max_part=16\n```\n\n次に、ディスクを共有してデバイスエントリを作成します:\n\n```\n# qemu-nbd -c /dev/nbd0 /path/to/image.qcow2\n```\n\nパーティションを検出します:\n\n```\n# partprobe /dev/nbd0\n```\n\nfdisk を使用して nbd0 内のさまざまなパーティションに関する情報を取得できます:\n\n```\n# fdisk -l /dev/nbd0\n```\n\n```\nDisk /dev/nbd0: 25.2 GiB, 27074281472 bytes, 52879456 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0xa6a4d542\n\nDevice      Boot   Start      End  Sectors  Size Id Type\n/dev/nbd0p1 *       2048  1026047  1024000  500M  7 HPFS/NTFS/exFAT\n/dev/nbd0p2      1026048 52877311 51851264 24.7G  7 HPFS/NTFS/exFAT\n```\n\n次に、ドライブイメージの任意のパーティション、例えばパーティション 2 をマウントします:\n\n```\n# mount /dev/nbd0p2 mountpoint\n```\n\n使用後は、イメージをアンマウントし、前の手順を逆にすることが重要です。つまり、パーティションをアンマウントし nbd デバイスを切断します:\n\n```\n# umount mountpoint\n# qemu-nbd -d /dev/nbd0\n```\n\n"
    },
    {
      "title": "ネットワーク",
      "level": 2,
      "content": "仮想ネットワークのパフォーマンスはユーザーモードネットワークまたは vde よりも tap デバイスやブリッジの方が良くなるはずです。tap デバイスやブリッジはカーネル内で実装されているからです。\n\n加えて、デフォルトの e1000 NIC のエミュレーションではなく virtio ネットワークデバイスを仮想マシンに指定することでネットワークのパフォーマンスを向上させることができます。詳しくは #virtio ドライバーを使う を参照。\n\n"
    },
    {
      "title": "リンク層アドレス",
      "level": 3,
      "content": "QEMU に -net nic 引数を与えると、デフォルトで、仮想マシンにリンク層アドレス 52:54:00:12:34:56 のネットワークインターフェイスが割り当てられます。しかしながら、ブリッジネットワークで複数の仮想マシンを使用する場合、個別の仮想マシンには tap デバイスの仮想マシン側からそれぞれ固有のリンク層 (MAC) アドレスを設定しなくてはなりません。設定を行わないと、ブリッジが上手く動きません。同一のリンク層アドレスを持つ複数のソースからパケットを受け取ることになるからです。たとえ tap デバイス自体に固有のリンク層アドレスを設定していたとしても、ソースのリンク層アドレスは tap デバイスを通過するときに書き換えられないため、問題が発生します。\n\n個々の仮想マシンに固有のリンク層アドレスを設定、それも、どのアドレスも 52:54: で始まるように設定してください。以下のオプションを使って下さい (X は任意の16進数の数字に置き換えてください):\n\n```\n$ qemu-system-x86_64 -net nic,macaddr=52:54:XX:XX:XX:XX -net vde disk_image\n```\n\n固有のリンク層アドレスの生成は複数の方法で行うことができます:\n\n- NIC ごとに固有のリンク層アドレスを手動で指定する。仮想マシンを起動するたびに同じ IP アドレスが DHCP サーバーによって割り当てられるという利点がありますが、仮想マシンが大量にある場合は現実的ではありません。\n- 仮想マシンを起動するたびにランダムなリンク層アドレスを生成する。衝突する可能性はほとんどゼロですが、DHCP サーバーによって割り当てられる IP アドレスが毎回異なるのが欠点です。以下のコマンドをスクリプトで使うことで macaddr 変数にランダムなリンク層アドレスを生成できます:\n\n```\nprintf -v macaddr \"52:54:%02x:%02x:%02x:%02x\" $(( $RANDOM & 0xff)) $(( $RANDOM & 0xff )) $(( $RANDOM & 0xff)) $(( $RANDOM & 0xff ))\nqemu-system-x86_64 -net nic,macaddr=\"$macaddr\" -net vde disk_image\n```\n\n- 以下のスクリプト qemu-mac-hasher.py を使ってハッシュ関数を利用し仮想マシンの名前からリンク層アドレスを生成する。仮想マシンの名前を一意なものとして、上記の方法の良いところを組み合わせています。スクリプトが実行されるたびに同一のリンク層アドレスが生成される上、衝突する可能性はほとんどありません。\n\n```\nqemu-mac-hasher.py\n```\n\n```\n#!/usr/bin/env python\n# usage: qemu-mac-hasher.py <VMName>\n\nimport sys\nimport zlib\n\ncrc = str(hex(zlib.crc32(sys.argv[1].encode(\"utf-8\")))).replace(\"x\", \"\")[-8:]\nprint(\"52:54:%s%s:%s%s:%s%s:%s%s\" % tuple(crc))\n```\n\nスクリプトでは、例えば以下のように使うことができます:\n\n```\nvm_name=\"VM Name\"\nqemu-system-x86_64 -name \"$vm_name\" -net nic,macaddr=$(qemu-mac-hasher.py \"$vm_name\") -net vde disk_image\n```\n\n"
    },
    {
      "title": "SLIRP",
      "level": 4,
      "content": "デフォルトで、-netdev 引数をつけていないと、QEMU は DHCP サーバーが内蔵された SLIRP ベースの ユーザーモードネットワークを使用します。DHCP クライアントを実行したときに仮想マシンには IP アドレスが与えられ、QEMU による IP マスカレードを通して物理ホストのネットワークにアクセスできるようになります。\n\nホストがインターネットに接続されていれば、このデフォルトの設定で簡単に仮想マシンをインターネットにアクセスさせることができますが、外部ネットワークからは仮想マシンは直接は見えず、また、複数の仮想マシンを同時に起動していても仮想マシン同士が通信することはできません。\n\nQEMU のユーザーモードネットワークには内蔵の TFTP や SMB サーバー、ゲストを VLAN に追加してゲストの通信を可能にするなどの機能があります。詳しくは -net user フラグの QEMU ドキュメントを参照してください。\n\nただし、ユーザーモードネットワークには有用性とパフォーマンスの両方で制約があります。より高度なネットワーク設定をするには tap デバイスや他の方法を使って下さい。\n\n- ユーザーモードネットワークで virtio ドライバーを使用するためのオプションは次の通りです: -nic user,model=virtio-net-pci 。\n- restrict=y を追加することで、ユーザーモードネットワークをホストと外部から隔離できます。例: -net user,restrict=y\n\n"
    },
    {
      "title": "passt",
      "level": 4,
      "content": "Users can choose to use passt-based user-mode networking. passt has several advantages over SLIRP such as better performance, full IPv6 support (including ICMPv6), better security, and more control.\n\nTo get started, install passt. There are two ways to launch it: Either via socket-based communication or via shared vhost-user. The latter method has better performance.\n\nFor the socket-based way, first launch passt:\n\n```\n$ passt -f\n```\n\nThen, for your QEMU command, add these parameters:\n\n```\n-device virtio-net-pci,netdev=s\n-netdev stream,id=s,server=off,addr.type=unix,addr.path=/tmp/passt_1.socket\n```\n\nFor the vhost-user way, launch passt with --vhost-user\n\n```\n$ passt -f --vhost-user\n```\n\nThen, for your QEMU command, add these parameters:\n\n```\n-m 4G\n-chardev socket,id=chr0,path=/tmp/passt_1.socket\n-netdev vhost-user,id=netdev0,chardev=chr0\n-device virtio-net,netdev=netdev0\n-object memory-backend-memfd,id=memfd0,share=on,size=4G\n-numa node,memdev=memfd0\n```\n\nNotice the memory sizes of -m 4G and size=4G have to match exactly.\n\n"
    },
    {
      "title": "QEMU の Tap ネットワーク",
      "level": 3,
      "content": "Tap デバイスは Linux カーネルの機能で、本当のネットワークインターフェイスのように見える仮想ネットワークインターフェイスを作成することができます。tap インターフェイスに送られたパケットは、そのインターフェイスに bind された、QEMU などのユーザースペースプログラムに送信されます。\n\nQEMU は仮想マシンで tap ネットワークを利用して、tap インターフェイスに送られたパケットを仮想マシンに送信することで仮想マシンのネットワークインターフェイス (通常は Ethernet インターフェイス) から受け取ったように見せることが可能です。逆に、仮想マシンがネットワークインターフェイスを通して送信したものは全て tap インターフェイスが受け取ります。\n\nTap デバイスは Linux の bridge ドライバーによってサポートされているため、tap デバイスを互いに、または eth0 といったホストのインターフェイスとブリッジすることができます。これは、仮想マシンを互いに通信できるようにしたい場合や、LAN 上の他のマシンが仮想マシンに通信できるようにしたい場合に価値があります。\n\nユーザーモードネットワークセクションで示したように、tap デバイスはユーザーモードよりも高いネットワーク性能を提供します。ゲスト OS が virtio ネットワークドライバーをサポートする場合、ネットワーク性能も大幅に向上します。tap0 デバイスを使用し、virtio ドライバがゲストで使用され、ネットワークの開始/停止を補助するスクリプトが使用されていない場合、qemu コマンドの一部は次のようになります:\n\n```\n-device virtio-net,netdev=network0 -netdev tap,id=network0,ifname=tap0,script=no,downscript=no\n```\n\nしかし、すでに virtio ネットワークドライバで tap デバイスを使用している場合は、vhost を有効にすることでネットワーク性能を向上させることもできます:\n\n```\n-device virtio-net,netdev=network0 -netdev tap,id=network0,ifname=tap0,script=no,downscript=no,vhost=on\n```\n\n詳細は [4] を参照してください。\n\n"
    },
    {
      "title": "ホストオンリーネットワーク",
      "level": 4,
      "content": "ブリッジに IP アドレスが与えられていてそこへのトラフィックが許可されていながら、本当のインターフェイス (例: eth0) がブリッジに接続されていない場合、仮想マシンは互いに通信したりホストシステムと通信することができるようになります。しかしながら、物理ホストで IP マスカレードを設定しないかぎり外部ネットワークとは一切通信することができません。この設定は VirtualBox などの他の仮想化ソフトウェアではホストオンリーネットワークと呼ばれています。\n\n- IP マスカレードを設定したい場合、インターネット共有#NAT の有効化ページを参照してください。\n- ブリッジの作成に関する情報は ネットワークブリッジ を参照してください。\n- ブリッジインターフェイスで DHCP サーバーを実行して仮想ネットワークを構築することもできます。例えば 172.20.0.1/16 サブネットで DHCP サーバーとして dnsmasq を使うには:\n\n```\n# ip addr add 172.20.0.1/16 dev br0\n# ip link set br0 up\n# dnsmasq -C /dev/null --interface=br0 --bind-interfaces --dhcp-range=172.20.0.2,172.20.255.254\n```\n\n"
    },
    {
      "title": "内部ネットワーク",
      "level": 4,
      "content": "ブリッジに IP アドレスを与えずにブリッジへの全てのトラフィックを INPUT チェインで drop する iptables ルールを追加した場合、仮想マシンは互いに通信することはできても、物理ホストや外側のネットワークに接続できなくなります。この設定は VirtualBox などの他の仮想化ソフトウェアでは内部ネットワークと呼ばれています。仮想マシンに固定 IP アドレスを割り当てるかマシンのどれか一つで DHCP サーバーを実行する必要があります。\n\nデフォルトで iptables はブリッジネットワークのパケットを拒否します。ブリッジネットワークのパケットを許可する iptables のツールを使用する必要があります:\n\n```\n# iptables -I FORWARD -m physdev --physdev-is-bridged -j ACCEPT\n```\n\n"
    },
    {
      "title": "qemu-bridge-helper を使用したブリッジネットワーク",
      "level": 4,
      "content": "この方法にはスタートアップスクリプトが必要なく、すぐに複数の tap やブリッジに対応することができます。 /usr/lib/qemu/qemu-bridge-helper バイナリを使用して、既存のブリッジに tap デバイスを作成できます。\n\n- ブリッジの作成については ネットワークブリッジ を参照してください。\n- QEMU のネットワークヘルパーの詳細は https://wiki.qemu.org/Features/HelperNetworking を参照してください。\n\nまず、QEMU が使用するすべてのブリッジの名前を含む設定ファイルを作成します:\n\n```\n/etc/qemu/bridge.conf\n```\n\n```\nallow br0\nallow br1\n...\n```\n\n/etc/qemu/ の パーミッション が 755 であることを確認してください。そうでない場合、 QEMU の問題 と GNS3 の問題 が発生する可能性があります。\n\n次に仮想マシンを起動します; デフォルトのネットワークヘルパーとデフォルトのブリッジ br0 で QEMU を実行する最も基本的な使い方は:\n\n```\n$ qemu-system-x86_64 -nic bridge [...]\n```\n\nブリッジ br1 と virtio ドライバを使用するには:\n\n```\n$ qemu-system-x86_64 -nic bridge,br=br1,model=virtio-net-pci [...]\n```\n\n"
    },
    {
      "title": "ブリッジを手動で作成する",
      "level": 4,
      "content": "以下では仮想マシンを eth0 などのホストインターフェイスにブリッジする方法を説明しています。おそらく一番よく使われている設定です。この設定では、物理的なホストマシンと同一の Ethernet セグメントに、直接外部ネットワークに仮想マシンが位置するようになります。\n\n通常の Ethernet アダプタをブリッジアダプタで置き換えて、通常の Ethernet アダプタをブリッジアダプタに bind することにします。\n\n- ブリッジを制御するための brctl が入っている bridge-utils をインストール。\n\n- IPv4 フォワーディングを有効にする:\n\n```\n# sysctl -w net.ipv4.ip_forward=1\n```\n\n変更を永続的にするために、/etc/sysctl.d/99-sysctl.conf の net.ipv4.ip_forward = 0 を net.ipv4.ip_forward = 1 に変えます。\n\n- tun モジュールをロードして起動時にロードするように設定してください。詳しくはカーネルモジュールを参照。\n\n- オプションでブリッジを作成します。詳細は netctl でブリッジ接続 を参照してください。ブリッジに br0 という名前を付けるか、以下のスクリプトをブリッジの名前に変更してください。以下の run-qemu スクリプトでは、リストにない場合は br0 が設定されます。これは、デフォルトではホストがブリッジを介してネットワークにアクセスしていないと想定されているからです。\n\n- Create the script that QEMU uses to bring up the tap adapter with root:kvm 750 permissions:\n\n```\n/etc/qemu-ifup\n```\n\n```\n#!/bin/sh\n\necho \"Executing /etc/qemu-ifup\"\necho \"Bringing up $1 for bridged mode...\"\nsudo /usr/bin/ip link set $1 up promisc on\necho \"Adding $1 to br0...\"\nsudo /usr/bin/brctl addif br0 $1\nsleep 2\n```\n\n- QEMU 用に root:kvm 750 パーミッションで /etc/qemu-ifdown の tap アダプタを落とすスクリプトを作成:\n\n```\n/etc/qemu-ifdown\n```\n\n```\n#!/bin/sh\n\necho \"Executing /etc/qemu-ifdown\"\nsudo /usr/bin/ip link set $1 down\nsudo /usr/bin/brctl delif br0 $1\nsudo /usr/bin/ip link delete dev $1\n```\n\n- visudo を使って sudoers ファイルに以下を追加します:\n\n```\nCmnd_Alias      QEMU=/usr/bin/ip,/usr/bin/modprobe,/usr/bin/brctl\n%kvm     ALL=NOPASSWD: QEMU\n```\n\n- 以下の run-qemu スクリプトを使って QEMU を起動します:\n\n```\nrun-qemu\n```\n\n```\n#!/bin/bash\n: '\ne.g. with img created via:\nqemu-img create -f qcow2 example.img 90G\nrun-qemu -cdrom archlinux-x86_64.iso -boot order=d -drive file=example.img,format=qcow2 -m 4G -enable-kvm -cpu host -smp 4\nrun-qemu -drive file=example.img,format=qcow2 -m 4G -enable-kvm -cpu host -smp 4\n'\n\nnicbr0() {\n    sudo ip link set dev $1 promisc on up &> /dev/null\n    sudo ip addr flush dev $1 scope host &>/dev/null\n    sudo ip addr flush dev $1 scope site &>/dev/null\n    sudo ip addr flush dev $1 scope global &>/dev/null\n    sudo ip link set dev $1 master br0 &> /dev/null\n}\n_nicbr0() {\n    sudo ip link set $1 promisc off down &> /dev/null\n    sudo ip link set dev $1 nomaster &> /dev/null\n}\n\nHASBR0=\"$( ip link show | grep br0 )\"\nif [ -z $HASBR0 ] ; then\n    ROUTER=\"192.168.1.1\"\n    SUBNET=\"192.168.1.\"\n    NIC=$(ip link show | grep en | grep 'state UP' | head -n 1 | cut -d\":\" -f 2 | xargs)\n    IPADDR=$(ip addr show | grep -o \"inet $SUBNET\\([0-9]*\\)\" | cut -d ' ' -f2)\n    sudo ip link add name br0 type bridge &> /dev/null\n    sudo ip link set dev br0 up\n    sudo ip addr add $IPADDR/24 brd + dev br0\n    sudo ip route del default &> /dev/null\n    sudo ip route add default via $ROUTER dev br0 onlink\n    nicbr0 $NIC\n    sudo iptables -I FORWARD -m physdev --physdev-is-bridged -j ACCEPT\nfi\n\nUSERID=$(whoami)\nprecreationg=$(ip tuntap list | cut -d: -f1 | sort)\nsudo ip tuntap add user $USERID mode tap\npostcreation=$(ip tuntap list | cut -d: -f1 | sort)\nTAP=$(comm -13 <(echo \"$precreationg\") <(echo \"$postcreation\"))\nnicbr0 $TAP\n\nprintf -v MACADDR \"52:54:%02x:%02x:%02x:%02x\" $(( $RANDOM & 0xff)) $(( $RANDOM & 0xff )) $(( $RANDOM & 0xff)) $(( $RANDOM & 0xff ))\nqemu-system-x86_64 -net nic,macaddr=$MACADDR,model=virtio \\\n    -net tap,ifname=$TAP,script=no,downscript=no,vhost=on \\\n    $@\n\n_nicbr0 $TAP\nsudo ip link set dev $TAP down &> /dev/null\nsudo ip tuntap del $TAP mode tap\n\nif [ -z $HASBR0 ] ; then\n    _nicbr0 $NIC\n    sudo ip addr del dev br0 $IPADDR/24 &> /dev/null\n    sudo ip link set dev br0 down\n    sudo ip link delete br0 type bridge &> /dev/null\n    sudo ip route del default &> /dev/null\n    sudo ip link set dev $NIC up\n    sudo ip route add default via $ROUTER dev $NIC onlink &> /dev/null\nfi\n```\n\nそれから仮想マシンを起動するために、以下のようにコマンドを実行して下さい\n\n```\n$ run-qemu -hda myvm.img -m 512\n```\n\n- パフォーマンスとセキュリティ上の理由で ブリッジ上のファイアウォール は無効にすることをお勧めします:\n\n```\n/etc/sysctl.d/10-disable-firewall-on-bridge.conf\n```\n\n```\nnet.bridge.bridge-nf-call-ip6tables = 0\nnet.bridge.bridge-nf-call-iptables = 0\nnet.bridge.bridge-nf-call-arptables = 0\n```\n\n起動時に上記のパラメータを適用するには、ブート時に br-netfilter モジュールをロードする必要があります。そうしないと、sysctl がパラメータを変更しようとしたときに、そのパラメータが存在しないことになります。\n\n```\n/etc/modules-load.d/br_netfilter.conf\n```\n\n```\nbr_netfilter\n```\n\nすぐに変更を適用するには sysctl -p /etc/sysctl.d/10-disable-firewall-on-bridge.conf を実行してください。\n\nlibvirt wiki や Fedora bug 512206 を参照。起動中にファイルが存在しないというエラーが起こるときは、起動時に bridge モジュールをロードするようにしてください。カーネルモジュール#systemd を参照。\n\nまたは、次のようにルールを追加することで全てのトラフィックをブリッジで通すように iptables を設定することができます:\n\n```\n-I FORWARD -m physdev --physdev-is-bridged -j ACCEPT\n```\n\n"
    },
    {
      "title": "iptables による物理デバイスと Tap デバイスのネットワーク共有",
      "level": 4,
      "content": "ブリッジネットワークは、有線インターフェイス (eth0 など) 間では正常に動作し、セットアップも簡単です。ただし、ホストがワイヤレスデバイスを介してネットワークに接続されている場合、ブリッジはできません。\n\n参考として ネットワークブリッジ#ブリッジ上の無線インターフェイス を参照。\n\nこれを克服する1つの方法は、tap デバイスに静的 IP を設定し、linux に自動的にルーティングを処理させ、iptables ルールで tap インターフェイスとネットワークに接続されたデバイス間のトラフィックを転送することです。\n\n参考として インターネット共有 を参照。\n\ntap や tun など、デバイス間でネットワークを共有するために必要なものを見つけることができます。次に、必要なホスト構成のヒントを示します。上記の例で示したように、クライアントは、tap インターフェイスに割り当てられた IP をゲートウェイとして、静的 IP を設定する必要があります。注意点は、DNS サーバーがネットワークに接続されているホストデバイスから別のホストデバイスに変更された場合は、クライアント上の DNS サーバーを手動で編集する必要があることです。\n\n起動毎に IP 転送を行うようにするには、/etc/sysctl.d 内の sysctl 設定ファイルに次の行を追加する必要があります:\n\n```\nnet.ipv4.ip_forward = 1\nnet.ipv6.conf.default.forwarding = 1\nnet.ipv6.conf.all.forwarding = 1\n```\n\niptables のルールは以下のようになります:\n\n```\n# Forwarding from/to outside\niptables -A FORWARD -i ${INT} -o ${EXT_0} -j ACCEPT\niptables -A FORWARD -i ${INT} -o ${EXT_1} -j ACCEPT\niptables -A FORWARD -i ${INT} -o ${EXT_2} -j ACCEPT\niptables -A FORWARD -i ${EXT_0} -o ${INT} -j ACCEPT\niptables -A FORWARD -i ${EXT_1} -o ${INT} -j ACCEPT\niptables -A FORWARD -i ${EXT_2} -o ${INT} -j ACCEPT\n# NAT/Masquerade (network address translation)\niptables -t nat -A POSTROUTING -o ${EXT_0} -j MASQUERADE\niptables -t nat -A POSTROUTING -o ${EXT_1} -j MASQUERADE\niptables -t nat -A POSTROUTING -o ${EXT_2} -j MASQUERADE\n```\n\n上記は、ネットワークに接続された3つのデバイスが、1つの内部デバイスとトラフィックを共有していると仮定しています。例えば次のようなものです:\n\n```\nINT=tap0\nEXT_0=eth0\nEXT_1=wlan0\nEXT_2=tun0\n```\n\n上記は、tap デバイスとの有線および無線接続の共有を可能にする転送を示しています。\n\n示されている転送ルールはステートレスであり、純粋な転送のためのものです。特定のトラフィックを制限し、ゲストや他の人を保護するためにファイアウォールを設置することを考えることができます。しかし、これらはネットワークパフォーマンスを低下させます。一方、シンプルなブリッジにはそのようなものはありません。\n\nおまけ: 接続が有線または無線のいずれであっても、tun デバイスを使用してリモートサイトに VPN 経由で接続された場合、その接続用にオープンされた tun デバイスが tun0 であり、事前のiptablesルールが適用されていると仮定すると、リモート接続もゲストと共有されます。これにより、ゲストも VPN 接続をオープンする必要がなくなります。繰り返しますが、ゲストネットワークは静的である必要があるため、この方法でホストをリモート接続する場合、おそらくゲスト上の DNS サーバーを編集する必要あります。\n\n"
    },
    {
      "title": "VDE とは?",
      "level": 4,
      "content": "VDE は Virtual Distributed Ethernet の略です。uml_switch の拡張として始まりました。仮想ネットワークを管理するためのツールボックスです。\n\n基本的にはソケットである仮想スイッチを作成して、物理マシンと仮想マシンを両方ともスイッチに\"接続\"するという考えになります。以下で説明する設定はとてもシンプルです。ただし、VDE はさらに強力な力を持っており、仮想スイッチ同士を接続したり、別のホストでスイッチを動作させスイッチのトラフィックを監視することなどができます。プロジェクトのドキュメント を読むことを推奨。\n\nこの方法の利点はユーザーに sudo 権限を与える必要がないということです。通常ユーザーに modprobe の実行を許可する必要はありません。\n\n"
    },
    {
      "title": "基本",
      "level": 4,
      "content": "VDE サポートは vde2 パッケージでインストールできます。\n\nこの設定では、tun/tap を使ってホストに仮想インターフェイスを作成します。tun モジュールをロード (詳しくはカーネルモジュールを参照):\n\n```\n# modprobe tun\n```\n\n仮想スイッチを作成:\n\n```\n# vde_switch -tap tap0 -daemon -mod 660 -group users\n```\n\n上記のコマンドでスイッチと tap0 が作成され、接続され、そして users グループのユーザーがスイッチを使えるようにします。\n\nインターフェイスは接続されてもまだ設定がされていません。設定するには、次のコマンドを実行:\n\n```\n# ip addr add 192.168.100.254/24 dev tap0\n```\n\nそして、通常ユーザーで -net オプションを使って KVM を実行してください:\n\n```\n$ qemu-system-x86_64 -net nic -net vde -hda [...]\n```\n\n物理ネットワークでやるのと同じようにゲストのネットワークを設定してください。\n\n"
    },
    {
      "title": "起動スクリプト",
      "level": 4,
      "content": "VDE を起動するメインスクリプトの例:\n\n```\n/etc/systemd/scripts/qemu-network-env\n```\n\n```\n#!/bin/sh\n# QEMU/VDE network environment preparation script\n\n# The IP configuration for the tap device that will be used for\n# the virtual machine network:\n\nTAP_DEV=tap0\nTAP_IP=192.168.100.254\nTAP_MASK=24\nTAP_NETWORK=192.168.100.0\n\n# Host interface\nNIC=eth0\n\ncase \"$1\" in\n  start)\n        echo -n \"Starting VDE network for QEMU: \"\n\n        # If you want tun kernel module to be loaded by script uncomment here\n\t#modprobe tun 2>/dev/null\n\t## Wait for the module to be loaded\n \t#while ! lsmod | grep -q \"^tun\"; do echo \"Waiting for tun device\"; sleep 1; done\n\n        # Start tap switch\n        vde_switch -tap \"$TAP_DEV\" -daemon -mod 660 -group users\n\n        # Bring tap interface up\n        ip address add \"$TAP_IP\"/\"$TAP_MASK\" dev \"$TAP_DEV\"\n        ip link set \"$TAP_DEV\" up\n\n        # Start IP Forwarding\n        echo \"1\" > /proc/sys/net/ipv4/ip_forward\n        iptables -t nat -A POSTROUTING -s \"$TAP_NETWORK\"/\"$TAP_MASK\" -o \"$NIC\" -j MASQUERADE\n        ;;\n  stop)\n        echo -n \"Stopping VDE network for QEMU: \"\n        # Delete the NAT rules\n        iptables -t nat -D POSTROUTING -s \"$TAP_NETWORK\"/\"$TAP_MASK\" -o \"$NIC\" -j MASQUERADE\n\n        # Bring tap interface down\n        ip link set \"$TAP_DEV\" down\n\n        # Kill VDE switch\n        pgrep vde_switch | xargs kill -TERM\n        ;;\n  restart|reload)\n        $0 stop\n        sleep 1\n        $0 start\n        ;;\n  *)\n        echo \"Usage: $0 {start|stop|restart|reload}\"\n        exit 1\nesac\nexit 0\n```\n\n上のスクリプトを使う systemd サービスの例:\n\n```\n/etc/systemd/system/qemu-network-env.service\n```\n\n```\n[Unit]\nDescription=Manage VDE Switch\n\n[Service]\nType=oneshot\nExecStart=/etc/systemd/scripts/qemu-network-env start\nExecStop=/etc/systemd/scripts/qemu-network-env stop\nRemainAfterExit=yes\n\n[Install]\nWantedBy=multi-user.target\n```\n\nqemu-network-env に 実行可能属性 を付与するようにパーミッションを変更。\n\n通常通り qemu-network-env.service を 開始 できます。\n\n"
    },
    {
      "title": "他の方法",
      "level": 4,
      "content": "上の方法が動作しない場合やカーネル設定, TUN, dnsmasq, iptables を変えたくない場合は以下のコマンドで同じ結果になります。\n\n```\n# vde_switch -daemon -mod 660 -group users\n# slirpvde --dhcp --daemon\n```\n\nホストのネットワークの接続を使って仮想マシンを起動するには:\n\n```\n$ qemu-system-x86_64 -net nic,macaddr=52:54:00:00:EE:03 -net vde disk_image\n```\n\n"
    },
    {
      "title": "VDE2 Bridge",
      "level": 3,
      "content": "quickhowto: qemu networking using vde, tun/tap, and bridge に基づいています。vde に接続された仮想マシンは外部から参照できる状態になります。例えば、ADSL ルーターから直接 DHCP の設定を個々の仮想マシンが受け取ることが可能です。\n\n"
    },
    {
      "title": "基本",
      "level": 4,
      "content": "tun モジュールと bridge-utils パッケージが必要です。\n\nvde2/tap デバイスを作成:\n\n```\n# vde_switch -tap tap0 -daemon -mod 660 -group users\n# ip link set tap0 up\n```\n\nブリッジを作成:\n\n```\n# brctl addbr br0\n```\n\nデバイスを追加:\n\n```\n# brctl addif br0 eth0\n# brctl addif br0 tap0\n```\n\nブリッジインターフェイスを設定:\n\n```\n# dhcpcd br0\n```\n\n"
    },
    {
      "title": "起動スクリプト",
      "level": 4,
      "content": "全てのデバイスを設定する必要があります。ブリッジに必要なのは IP アドレスだけです。ブリッジの物理デバイスは (例: eth0)、netctl でカスタム Ethernet プロファイルを使います:\n\n```\n/etc/netctl/ethernet-noip\n```\n\n```\nDescription='A more versatile static Ethernet connection'\nInterface=eth0\nConnection=ethernet\nIP=no\n```\n\n以下のカスタム systemd サービスを使うことで users ユーザーグループで使用する VDE2 tap インターフェイスを作成することができます。\n\n```\n/etc/systemd/system/vde2@.service\n```\n\n```\n[Unit]\nDescription=Network Connectivity for %i\nWants=network.target\nBefore=network.target\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nExecStart=/usr/bin/vde_switch -tap %i -daemon -mod 660 -group users\nExecStart=/usr/bin/ip link set dev %i up\nExecStop=/usr/bin/ip addr flush dev %i\nExecStop=/usr/bin/ip link set dev %i down\n\n[Install]\nWantedBy=multi-user.target\n```\n\nそして最後に、netctl でブリッジネットワークを作成することが可能です。\n\n"
    },
    {
      "title": "省略記法の設定",
      "level": 3,
      "content": "QEMU をさまざまなネットワーク・オプションとともに頻繁に使用している場合、-netdev と -device の引数のペアを大量に作成している可能性があり、かなり反復的になっています。代わりに -nic 引数を使って、 -netdev と -device を結合することもできます。たとえば、次のような引数は:\n\n```\n-netdev tap,id=network0,ifname=tap0,script=no,downscript=no,vhost=on -device virtio-net-pci,netdev=network0\n```\n\nこうなります:\n\n```\n-nic tap,script=no,downscript=no,vhost=on,model=virtio-net-pci\n```\n\nネットワーク ID がないこと、およびデバイスが model= で作成されたことに注意してください。-nic パラメータの前半は -netdev パラメータですが、後半 (model= の後) はデバイスに関連付けられています。同じパラメータ (たとえば、 smb=) が使用されます。ネットワークを完全に無効にするには、 -nic none を使用します。\n\n使用できるパラメーターの詳細については、 QEMU ネットワークのドキュメント を参照してください。\n\n"
    },
    {
      "title": "グラフィックスカード",
      "level": 2,
      "content": "QEMU は -display curses コマンド・ライン・オプションを使用して、標準のグラフィックスカードのテキストモードをエミュレートできます。これにより、テキストターミナル内でテキストを入力しテキスト出力を直接見ることができます。代わりに、 -nographic も同様の目的を果たします。\n\nQEMU はいくつかのタイプの VGA カードをエミュレートできます。カードタイプは -vga type コマンドラインオプションで渡され、 std, qxl, vmware, virtio, cirrus または none のいずれかになります。\n\n"
    },
    {
      "title": "std",
      "level": 3,
      "content": "-vga std では 2560 x 1600 ピクセルまでの解像度を得ることができます。QEMU 2.2 からデフォルトとなっています。\n\n"
    },
    {
      "title": "qxl",
      "level": 3,
      "content": "QXL は、2D サポートのある準仮想化グラフィックスドライバーです。これを使用するには、-vga qxl オプションを渡して、ゲストにドライバーをインストールしてください。QXL を使用する場合、グラフィックのパフォーマンスを向上させるために #SPICE を使用するとよいでしょう。\n\nLinux ゲストでは、適切なパフォーマンスを得るために qxl と bochs_drm カーネルモジュールをロードしてください。\n\nQXL デバイスのデフォルトの VGA メモリサイズは 16M です。これは QHD(2560x1440) までの解像度を駆動するのに十分です。より高い解像度を有効にするには、 vga_memmb を増やします。\n\n"
    },
    {
      "title": "vmware",
      "level": 3,
      "content": "多少バグが存在しますが、std や cirrus よりもパフォーマンスが上です。Arch Linux ゲスト用の VMware ドライバー xf86-video-vmware と xf86-input-vmmouse をインストールします。\n\n"
    },
    {
      "title": "virtio",
      "level": 3,
      "content": "virtio-vga / virtio-gpu は virgl ベースの準仮想化 3D グラフィックスドライバです。成熟しており、現在はオプション galla-drivers=virgl でコンパイルされた mesa (>=11.2) を持つごく最近 (>=4.4) のLinux ゲストのみをサポートしています。\n\nゲストシステムで 3D アクセラレーションを有効にするには、-device virtio-vga-gl でこの vga を選択し、ディスプレイデバイスで sdl および gtk ディスプレイ出力に対してそれぞれ -display sdl,gl=on または -display gtk,gl=on を使用して OpenGL コンテキストを有効にします。ゲスト側のカーネルログを見ることで設定が問題ないか確認できます:\n\n```\n# dmesg | grep drm\n```\n\n```\n[drm] pci: virtio-vga detected\n[drm] virgl 3d acceleration enabled\n```\n\n"
    },
    {
      "title": "cirrus",
      "level": 3,
      "content": "cirrus グラフィカルアダプタは 2.2 以前まではデフォルト でした。新しいシステムでは 使用しないほうがよい とされています。\n\n"
    },
    {
      "title": "none",
      "level": 3,
      "content": "これは VGA カードが全くない PC と同じようになります。-vnc オプションを使ってもアクセスすることはできません。また、QEMU に VGA カードをエミュレートさせ SDL ディスプレイを無効にする -nographic オプションとは異なります。\n\n"
    },
    {
      "title": "SPICE",
      "level": 2,
      "content": "SPICE プロジェクト は、仮想マシンへのリモートアクセスをシームレスに行うための完全なオープンソースソリューションを提供することを目的としています。\n\n"
    },
    {
      "title": "ホストで SPICE サポートを有効にする",
      "level": 3,
      "content": "リモートデスクトッププロトコルとして SPICE を使用して起動する例を示します。これには、ホストからのコピーと貼り付けのサポートも含まれています:\n\n```\n$ qemu-system-x86_64 -vga qxl -device virtio-serial-pci -spice port=5930,disable-ticketing=on -device virtserialport,chardev=spicechannel0,name=com.redhat.spice.0 -chardev spicevmc,id=spicechannel0,name=vdagent\n```\n\nパラメータの意味は次のとおりです:\n\n1. -device virtio-serial-pci は virtio-serial デバイスを追加します\n1. -spice port=5930,disable-ticketing=on は spice チャネルを待ち受ける TCP ポート 5930 を設定し、クライアントが認証なしで接続できるようにします ヒント: TCP ポートの代わりに wikipedia:Unix_socket Unix ソケット を使用すると、ホストシステムでネットワークスタックを使用する必要が無くなります。ネットワークと関連プロトコルを使用するためにパケットをカプセル化したりカプセル化を解除することもありません。ソケットはハードドライブ上の i ノードによってのみ識別されます。したがって、パフォーマンス的にはこちらの方が優れていると考えられています。代わりに -spice unix=on,addr=/tmp/vm_spice.socket,disable-ticketing=on を使用します。\n1. -device virtserialport,chardev=spicechannel0,name=com.redhat.spice.0 は virtio-serial デバイスの spice vdagent 用のポートを開きます。\n1. -chardev spicevmc,id=spicechannel0,name=vdagent は、そのポートに spicevmc の chardev を追加します。virtserialport デバイスの chardev= オプションが、chardev オプション (この例では {ic|spicechannel0}}) に指定された id= オプションと一致することが重要です。また、ポート名が com.redhat.spice.0 であることも重要です。これは、vdagent がゲスト内で探している名前空間であるためです。最後に name=vdagent を指定して、spice がこのチャネルの目的を認識できるようにします。\n\n"
    },
    {
      "title": "SPICE クライアントでゲストに接続する",
      "level": 3,
      "content": "ゲストに接続するには SPICE クライアントが必要です。Arch では、次のクライアントを使用できます:\n\n- virt-viewer — プロトコル開発者が推奨する SPICE クライアント。virt-manager プロジェクトのサブセットです。\n\n- spice-gtk — SPICE GTK クライアント。SPICE プロジェクトのサブセットです。他のアプリケーションにウィジェットとして埋め込まれています。\n\nスマートフォンやその他のプラットフォームで動作するクライアントについては、spice-space download の その他のクライアント セクションを参照してください。\n\n"
    },
    {
      "title": "SPICE クライアントを手動で実行する",
      "level": 4,
      "content": "Unix ソケット /tmp/vm_spice.socket で待ち受けるゲストに接続する方法の1つは、望みのクライアントに応じて $remote-viewer spice+unix:///tmp/vm_spice.socket または $spicy--uri=\"spice+unix:///tmp/vm_spice.socket\" を使用して SPICE クライアントを手動で実行することです。SPICE モードの QEMU はリモートデスクトップサーバーのように振る舞うため、-daemonize パラメータを指定してデーモンモードで QEMU を実行する方が便利な場合があります。\n\n```\n$ ssh -fL 5999:localhost:5930 my.domain.org sleep 10; spicy -h 127.0.0.1 -p 5999\n```\n\nこの例では、spicy をローカルポート 5999 に接続し、SSH 経由でアドレス my.domain.org 、ポート 5930 で示されるゲストの SPICE サーバへ転送しています。 コマンド sleep 10 をバックグラウンドで実行するよう ssh に要求する -f オプションに注意してください。このようにして、ssh セッションはクライアントがアクティブな間実行され、クライアントが終了すると自動的に閉じます。\n\n"
    },
    {
      "title": "QEMU で SPICE クライアントを実行する",
      "level": 4,
      "content": "ディスプレイが -display spice-app パラメータを使用して SPICE に設定されている場合、QEMU は適切なソケットで SPICE クライアントを自動的に起動できます。これは、XDG MIME Applications#mimeapps.list mimeapps.list ファイルによって決定されたシステムのデフォルト SPICE クライアントをビューアとして使用します。\n\n"
    },
    {
      "title": "ゲストで SPICE サポートを有効にする",
      "level": 3,
      "content": "Arch Linux ゲスト では、マルチモニタまたはクリップボード共有のサポートを改善するために、以下のパッケージをインストールする必要があります:\n\n- spice-vdagent: クライアントと X-session などとの間でコピー&ペーストを可能にする Spice エージェント xorg クライアント。(GNOME 以外のデスクトップで動作させるための回避策については、修正されるまで、この イシュー を参照してください。)\n- xf86-video-qxl: Xorg X11 qxl ビデオドライバ\n- x-resizeAUR: GNOME 以外のデスクトップ環境では、SPICE クライアントウィンドウのサイズが変更されても自動的には反応しません。このパッケージは、udev ルールと xrandr を使用して、すべての X11 ベースのデスクトップ環境とウィンドウマネージャに自動リサイズ機能を実装します。\n\nその他のオペレーティングシステム のゲストについては、spice-space download の ゲスト セクションを参照してください。\n\n"
    },
    {
      "title": "SPICE によるパスワード認証",
      "level": 3,
      "content": "SPICE でパスワード認証を使用可能にする場合は、-spice 引数から disable-ticketing を削除し、代わりに password=yourpassword を追加する必要があります。たとえば:\n\n```\n$ qemu-system-x86_64 -vga qxl -spice port=5900,password=yourpassword -device virtio-serial-pci -device virtserialport,chardev=spicechannel0,name=com.redhat.spice.0 -chardev spicevmc,id=spicechannel0,name=vdagent\n```\n\nこれで、SPICE クライアントが SPICE サーバに接続するためのパスワードを要求するようになります。\n\n"
    },
    {
      "title": "SPICE による TLS 暗号化通信",
      "level": 3,
      "content": "SPICE サーバと通信するために TLS 暗号化を設定することもできます。まず、次のファイルを含むディレクトリを作成する必要があります(名前は指定されたとおりでなければなりません):\n\n- ca-cert.pem: CA マスター証明書。\n- server-cert.pem: ca-cert.pem で署名されたサーバ証明書。\n- server-key.pem: サーバの秘密キー。\n\nSpice User Manual に、サーバ用に独自に作成した認証局で自己署名証明書を生成する例が示されています。\n\nその後、上記の説明と同様に SPICE を使用して QEMU を実行しますが、-spice 引数として: -spice tls-port=5901,password=yourpassword,x509-dir=/path/to/pki_certs を使用します。、/path/to/pki_certs は、前述の3つの必要なファイルを含むディレクトリのパスとなります。\n\nこれで、virt-viewer を使用してサーバに接続できるようになりました:\n\n```\n$ remote-viewer spice://hostname?tls-port=5901 --spice-ca-file=/path/to/ca-cert.pem --spice-host-subject=\"C=XX,L=city,O=organization,CN=hostname\" --spice-secure-channels=all\n```\n\n--spice-host-subject パラメータは server-cert.pem サブジェクトに従って設定する必要があることに注意してください。また、サーバ証明書を検証するために ca-cert.pem を各クライアントにコピーしておく必要があります。\n\n```\n$ openssl x509 -noout -subject -in server-cert.pem | cut -d' ' -f2- | sed 's/\\///' | sed 's/\\//,/g'\n```\n\n同等の spice-gtk コマンドは:\n\n```\n$ spicy -h hostname -s 5901 --spice-ca-file=ca-cert.pem --spice-host-subject=\"C=XX,L=city,O=organization,CN=hostname\" --spice-secure-channels=all\n```\n\n"
    },
    {
      "title": "VNC",
      "level": 2,
      "content": "-vnc :X オプションを追加すると、QEMU に VGA ディスプレイを VNC セッションにリダイレクトさせることができます。ディスプレイ番号を X に置き換えます (0 は 5900 で、1 は 5901... でリッスンします)。\n\n```\n$ qemu-system-x86_64 -vnc :0\n```\n\n#ブート時に QEMU 仮想マシンを起動する セクションにも例が示されています。\n\n"
    },
    {
      "title": "基本的なパスワード認証",
      "level": 3,
      "content": "アクセスパスワードは password オプションを使用して簡単に設定できます。QEMU モニターでパスワードを指定する必要があり、パスワードが提供された場合にのみ接続が可能になります。\n\n```\n$ qemu-system-x86_64 -vnc :0,password -monitor stdio\n```\n\nQEMU モニターでは、change vnc password コマンドを使用してパスワードを設定し、次にパスワードを指定します。\n\n次のコマンドラインは、直接 vnc をパスワードを付きで実行します。\n\n```\n$ printf \"change vnc password\\n%s\\n\" MYPASSWORD | qemu-system-x86_64 -vnc :0,password -monitor stdio\n```\n\n"
    },
    {
      "title": "オーディオバックエンドを作成する",
      "level": 3,
      "content": "-audiodev フラグは、ホスト上のオーディオバックエンドドライバとそのオプションを設定します。\n\n利用可能なオーディオバックエンドドライバを一覧表示するには:\n\n```\n$ qemu-system-x86_64 -audiodev help\n```\n\nオプション設定のリストについては qemu(1) のマニュアルページに詳細があります。\n\n最低限、オーディオバックエンドを選択し、PulseAudio の ID を設定する必要があります。たとえば:\n\n```\n-audiodev pa,id=snd0\n```\n\n"
    },
    {
      "title": "Intel HD Audio",
      "level": 4,
      "content": "Intel HD Audio エミュレーションの場合は、コントローラーとコーデックデバイスの両方を追加してください。使用可能なインテル HDA Audio デバイスを一覧するには:\n\n```\n$ qemu-system-x86_64 -device help | grep hda\n```\n\nオーディオコントローラを追加するには:\n\n```\n-device ich9-intel-hda\n```\n\nそして、オーディオコーデックを追加し、ホストオーディオバックエンド ID にマップします:\n\n```\n-device hda-output,audiodev=snd0\n```\n\n"
    },
    {
      "title": "Intel 82801AA AC97",
      "level": 4,
      "content": "AC97 エミュレーションの場合は、オーディオカードデバイスを追加し、ホストオーディオバックエンド ID にマップするだけです:\n\n```\n-device AC97,audiodev=snd0\n```\n\n- audiodev バックエンドが提供されていない場合、QEMU はそれを検索して自動的に追加します。これは単一の audiodev に対してのみ機能します。例えば -device intel-hda -device hda-duplex はデフォルトの audiodev バックエンドを使用してゲスト上で intel-hda をエミュレートします。\n- ゲストマシン用のビデオグラフィックスカードでエミュレートされたドライバも音質の問題を引き起こす可能性があります。1つずつテストして動作させてください。qemu-system-x86_64 -h | grep vga で可能なオプションを一覧できます。\n\n"
    },
    {
      "title": "VirtIO sound",
      "level": 4,
      "content": "VirtIO sound も QEMU 8.2.0 より利用できます。使い方は:\n\n```\n-device virtio-sound-pci,audiodev=my_audiodev -audiodev alsa,id=my_audiodev\n```\n\n詳細な情報が QEMU documentation にあります。\n\n"
    },
    {
      "title": "virtio ドライバーを使う",
      "level": 2,
      "content": "QEMU は virtio ドライバを使って準仮想化ブロックデバイスとネットワークデバイスを使用する機能をゲストに提供し、より良いパフォーマンスとより低いオーバーヘッドを実現します。\n\n- virtio ブロックデバイスは、ディスクイメージを渡すためのオプション -drive と、パラメータ if=virtio を必要とします:\n\n```\n$ qemu-system-x86_64 -drive file=disk_image,if=virtio\n```\n\n- ネットワークでもほぼ同じです:\n\n```\n$ qemu-system-x86_64 -nic user,model=virtio-net-pci\n```\n\n"
    },
    {
      "title": "Arch Linux ゲストを用意する",
      "level": 3,
      "content": "Arch Linux ゲストをインストールした後 virtio デバイスを使うには、次のモジュールをゲストでロードする必要があります: virtio, virtio_pci, virtio_blk, virtio_net, virtio_ring。32ビットゲストの場合、特定の \"virtio\" モジュールは必要ありません。\n\nvirtio ディスクから起動したい場合、イニシャル ramdisk に必要なモジュールを含める必要があります。デフォルトでは、mkinitcpio の autodetect フックによって管理されています。もしくは /etc/mkinitcpio.conf の MODULES 配列を使用して必要なモジュールを組み込み、イニシャル ramdisk をリビルドしてください。\n\n```\n/etc/mkinitcpio.conf\n```\n\n```\nMODULES=(virtio virtio_blk virtio_pci virtio_net)\n```\n\nvirtio ディスクは前に v が付いて認識されます (例: vda, vdb など)。なので、virtio ディスクから起動する際は少なくとも /etc/fstab や /boot/grub/grub.cfg に変更を加える必要があります。\n\nKVM による準仮想化に関する詳細は ここ にあります。\n\nqemu-guest-agent をインストールすることでハイパーバイザの管理機能を拡張する QMP コマンドのサポートを得ることができます。\n\n"
    },
    {
      "title": "メモリバルーニング",
      "level": 4,
      "content": "In order to allow the guest's memory foot print to shrink as seen from the host, it needs to report to the host which pages are not needed anymore by the guest. The kernel has an API for that called Free Page Reporting and since it's built-in it's as easy as starting QEMU like this:\n\n```\n$ qemu-system-x86_64 ... -device virtio-balloon,free-page-reporting=on\n```\n\nAfter this, you should see the guest memory increasing and then shrinking again after running workloads in it.\n\n"
    },
    {
      "title": "Windows 用の virtio ドライバ",
      "level": 4,
      "content": "Windows には virtio ドライバは付属していません。最新の安定版バージョンのドライバは Fedora によって定期的にビルドされており、ドライバのダウンロードの詳細は virtio-win on GitHub で提供されています。以降のセクションでは、ここで提供されている安定版 ISO ファイル virtio-win.iso を主に使用します。または、virtio-winAUR を使用します。\n\n"
    },
    {
      "title": "ブロックデバイスドライバ",
      "level": 4,
      "content": "インストール時にドライバをロードする必要があります。手順としては、プライマリディスクデバイスおよび Windows ISO インストールメディアとともに cdrom デバイスで virtio ドライバを含む ISO イメージをロードします。\n\n```\n$ qemu-system-x86_64 ... \\\n-drive file=disk_image,index=0,media=disk,if=virtio \\\n-drive file=windows.iso,index=2,media=cdrom \\\n-drive file=virtio-win.iso,index=3,media=cdrom \\\n...\n```\n\nインストール中、Windows インストーラが \"Where do you want to install Windows?\" と尋ねる段階で、ディスクを見つけられないという警告が表示されます。以下の手順に従ってください (アップデート適用済みの Windows Server 2012 R2 がベース):\n\n- Load Drivers オプションを選択。\n- Hide drivers that are not compatible with this computer's hardware のチェックを外す。\n- Browse ボタンをクリックして virtio iso の CDROM を開く。通常 \"virtio-win-XX\" という名前になります。\n- E:\\viostor\\[your-os]\\amd64 を選択して OK を押す。\n\nこれで virtio ディスクが表示されるので、選択して、フォーマット・インストールすることができます。\n\nvirtio ディスクから起動するように既存の Windows ゲストを変更するには、起動時にゲストによって virtio ドライバがロードされる必要があります。 そのため、virtio モードでディスクイメージを起動できるようにする前に、起動時に virtio ドライバーをロードするように Windows に教える必要があります。\n\nこのためには、まず virtio モードで接続される新しいディスクイメージを作成し、ドライバの検索をトリガします:\n\n```\n$ qemu-img create -f qcow2 dummy.qcow2 1G\n```\n\nブートディスクは IDE モードのまま、フェイクディスクを virtio モード、ドライバ ISO イメージを使用して元の Windows ゲストを実行します。\n\n```\n$ qemu-system-x86_64 -m 4G -drive file=disk_image,if=ide -drive file=dummy.qcow2,if=virtio -cdrom virtio-win.iso\n```\n\nWindows はフェイクディスクを検出して適切なドライバを探します。失敗した場合は、デバイスマネージャ に移動し、感嘆符アイコン(開いているはず)が表示されている SCSI ドライブを探し、ドライバの更新 をクリックして仮想 CD-ROM を選択します。CD-ROM 内のドライバフォルダに移動せず、単に CD-ROM ドライブを選択するだけで、Windows は適切なドライバを自動的に検索します (Windows 7 SP1 でテスト済み)。\n\nWindows に次回起動時にセーフモードで起動するように要求します。これは、Windows の msconfig.exe ツールを使用して行うことができます。セーフモードでは新しい virtio ドライバを含むすべてのドライバが起動時にロードされます。Windows は、起動時に virtio ドライバが必要であることを認識すると、将来の起動のためにそれを記憶します。\n\nセーフモードで起動するように指示されたら、仮想マシンをオフにして再度起動できます。今度の起動ディスクは virtio モードで接続されています:\n\n```\n$ qemu-system-x86_64 -m 4G -drive file=disk_image,if=virtio\n```\n\nvirtio ドライバがロードされた状態のセーフモードで起動されているはずです。これで msconfig.exe に戻り、セーフモードでの起動を無効にして、Windows を再起動できます。\n\n"
    },
    {
      "title": "ネットワークドライバ",
      "level": 4,
      "content": "virtio ネットワークドライバーの使い方は少し簡単で、単に -nic 引数を追加するだけです。\n\n```\n$ qemu-system-x86_64 -m 4G -drive file=windows_disk_image,if=virtio -nic user,model=virtio-net-pci -cdrom virtio-win.iso\n```\n\nネットワークアダプタが検出され、そのドライバが検索されます。失敗した場合は、デバイスマネージャ に移動し、感嘆符アイコン(開いているはず)が表示されているネットワークアダプタを探し、ドライバの更新 をクリックして仮想 CD-ROM を選択してください。ディレクトリを再帰的に検索するチェックボックスを選択することを忘れないでください。\n\n"
    },
    {
      "title": "バルーンドライバ",
      "level": 4,
      "content": "(virsh コマンドの dommemstat などで) ゲストのメモリ状態を追跡したり実行時にゲストのメモリサイズを変えたい場合 (メモリサイズは変更できませんが、バルーンドライバを膨張させることでメモリ使用量を制限できます) 、ゲストにバルーンドライバをインストールする必要があります。\n\nこのためには、デバイス マネージャー にアクセスし、システム デバイス (またはほかのデバイス から認識されない PCI コントローラ) で PCI 標準 RAM コントローラ を検索し、ドライバの更新 を選択してください。ウィンドウが開いたら コンピュータを参照して... を選択し、CD-ROM を選択してください(そして サブフォルダーも検索する チェックボックスを忘れないでください)。インストール後に再起動してください。これによりドライバがインストールされ、バルーンを膨らませることができます (たとえば hmp コマンド balloon memory_size によって、バルーンはゲストの使用可能なメモリサイズを memory_size に縮小するために可能な限り多くのメモリを消費します)。しかし、それでもゲストのメモリ状態を追跡することはできません。これを行うには Balloon サービスを正しくインストールする必要があります。管理者としてコマンドラインを開いて、CD-ROM から Balloon ディレクトリに移動し、システムとアーキテクチャに応じてさらに深く移動してください。amd64 (x86) ディレクトリまで移動したら blnsrv.exe -i を実行するとインストールが実行されます。その後 virsh コマンド dommemstat はサポートされているすべての値を出力するはずです。\n\n"
    },
    {
      "title": "virtiofsd の共有を使う",
      "level": 4,
      "content": "このセクションに進む前に、まず virtiofsd によるホストファイル共有の設定 を済ませてください。\n\nまず、上流の手順 に従ってください。設定が完了すると、Windows の Z: ドライブに共有ディレクトリの内容が自動的にマップされます。\n\n以下のものがあれば、Windows 11 ゲストシステムは適切に設定されています:\n\n- VirtioFSSService windows サービス\n- WinFsp.Launcher windows サービス\n- Windows の \"デバイスマネージャー\" の \"システムデバイス\" の下の VirtIO FS Device driver\n\n上記がインストールされていても Z: ドライブが表示されない場合は、Windows の プログラムの追加と削除 から \"Virtio-win-guest-tools\" を修復してみてください。\n\n"
    },
    {
      "title": "FreeBSD ゲストを用意する",
      "level": 3,
      "content": "FreeBSD 8.3 以降を使っている場合は emulators/virtio-kmod port をインストールしてください、10.0-CURRENT ではカーネルに含まれています。インストール後、/boot/loader.conf ファイルに以下を追加します:\n\n```\nvirtio_load=\"YES\"\nvirtio_pci_load=\"YES\"\nvirtio_blk_load=\"YES\"\nif_vtnet_load=\"YES\"\nvirtio_balloon_load=\"YES\"\n```\n\nそして次を実行して /etc/fstab を修正してください:\n\n```\n# sed -ibak \"s/ada/vtbd/g\" /etc/fstab\n```\n\nそれから /etc/fstab が問題ないか確認してください。何かがおかしい場合、レスキュー CD で起動して /etc/fstab.bak を /etc/fstab にコピーして戻します。\n\n"
    },
    {
      "title": "QEMU モニタ",
      "level": 2,
      "content": "QEMU の実行中、実行中の仮想マシンと対話するためのいくつかの方法を提供するために、モニタコンソールが提供されます。QEMU モニタは、現在の仮想マシンに関する情報の取得、デバイスのホットプラグ、仮想マシンの現在の状態のスナップショットの作成など、興味深い機能を提供します。すべてのコマンドのリストを表示するには、QEMU モニタコンソールで help または ? を実行するか、QEMU 公式ドキュメント の関連セクションを参照してください。\n\n"
    },
    {
      "title": "グラフィカルビュー",
      "level": 4,
      "content": "デフォルトグラフィックスオプションの std を使用している場合、QEMU ウィンドウで Ctrl+Alt+2 を押すか、View > compatmonitor0 をクリックすることで QEMU モニタにアクセスできます。仮想マシンのグラフィカルビューに戻るには、Ctrl+Alt+1 を押すか、View > VGA をクリックします。\n\nただし、モニタにアクセスする標準的な方法は必ずしも便利ではなく、QEMU がサポートするすべてのグラフィック出力で機能するわけではありません。\n\n"
    },
    {
      "title": "Telnet",
      "level": 4,
      "content": "telnet を有効にするには、-monitor telnet:127.0.0.1:port,server,nowait パラメータを指定して QEMU を実行してください。仮想マシンが起動すると、telnet 経由でモニタにアクセスできるようになります:\n\n```\n$ telnet 127.0.0.1 port\n```\n\n"
    },
    {
      "title": "UNIX ソケット",
      "level": 4,
      "content": "-monitor unix:socketfile,server,nowait パラメータを指定して QEMU を実行します。その後、socat、nmap、または openbsd-netcat のいずれかで接続できます。\n\n例えば、QEMU を次のように実行した場合:\n\n```\n$ qemu-system-x86_64 -monitor unix:/tmp/monitor.sock,server,nowait [...]\n```\n\n以下のコマンドでモニタに接続できます:\n\n```\n$ socat - UNIX-CONNECT:/tmp/monitor.sock\n```\n\nまたは:\n\n```\n$ nc -U /tmp/monitor.sock\n```\n\nあるいは nmap で:\n\n```\n$ ncat -U /tmp/monitor.sock\n```\n\n"
    },
    {
      "title": "TCP",
      "level": 4,
      "content": "引数 -monitor tcp:127.0.0.1:port,server,nowait を使用して TCP 経由でモニタを公開できます。その後、openbsd-netcat または gnu-netcat のいずれかのnetcat を実行して接続します:\n\n```\n$ nc 127.0.0.1 port\n```\n\n"
    },
    {
      "title": "標準 I/O",
      "level": 4,
      "content": "引数 -monitor stdio で実行すると、QEMU が実行されているのと同じ端末から自動的にモニタにアクセスできます。\n\n"
    },
    {
      "title": "モニタコンソールを使って仮想マシンにキーボードの押下を送信する",
      "level": 3,
      "content": "設定によっては仮想マシン上で一部のキーの組み合わせがホストによって邪魔されて使えない場合があります (顕著な例として、アクティブな tty を切り替える Ctrl+Alt+F* キーの組み合わせなど) 。この問題を回避するために、問題のあるキーの組み合わせをモニタコンソール経由で代わりに送信することができます。モニタに切り替えてから sendkey コマンドを使って必要なキー入力を仮想マシンに転送します。例えば:\n\n```\n(qemu) sendkey ctrl-alt-f2\n```\n\n"
    },
    {
      "title": "モニタコンソールを使ってスナップショットを作成・管理する",
      "level": 3,
      "content": "ときには仮想マシンの現在の状態を保存して何か問題が発生したときに仮想マシンの状態を元に戻したいということもあるでしょう。QEMU モニタコンソールにはスナップショットの作成、管理、およびマシン状態を保存されたスナップショットに戻すために必要なユーティリティが備わっています。\n\n- savevm name を使用するとタグ name のスナップショットが作成されます。\n- loadvm name を使用すると仮想マシンがスナップショット name の状態に戻ります。\n- delvm name で name としてタグ付けされたスナップショットが削除されます。\n- info snapshots を使用すると保存済みのスナップショットのリストを表示します。スナップショットは自動増分される ID 番号とテキストタグ(スナップショット作成時にユーザが設定)の両方で識別されます。\n\n"
    },
    {
      "title": "immutable モードで仮想マシンを実行する",
      "level": 3,
      "content": "-snapshot パラメータを指定して QEMU を実行するだけで仮想マシンを frozen 状態で実行でき、仮想マシンの電源がオフになったときにすべての変更を破棄できます。ゲストによるディスクイメージの書き込みがあった場合、変更は /tmp 内の一時ファイルに保存され QEMU が停止したときに破棄されます。\n\nただし、マシンが frozen モードで実行している場合でも、後で必要に応じて、モニタコンソールを使用して次のコマンドを実行することにより、変更をディスクイメージに保存することができます。\n\n```\n(qemu) commit all\n```\n\nfrozen モードで実行中にスナップショットが作成された場合、変更が明示的にディスクにコミットされない限り、QEMU の終了時に破棄されます。\n\n"
    },
    {
      "title": "モニタコンソールによる一時停止と電源オプション",
      "level": 3,
      "content": "モニタコマンドを使って物理マシンの操作の一部を QEMU でエミュレートできます:\n\n- system_powerdown は仮想マシンに ACPI シャットダウンリクエストを送信します。物理マシンの電源ボタンを押したときと同じような効果があります。\n- system_reset は物理マシンのリセットボタンと同じように仮想マシンをリセットします。仮想マシンが正常に再起動されないためデータが消失したりファイルシステムが破損する可能性があります。\n- stop は仮想マシンを停止します。\n- cont は仮想マシンを以前に停止した状態から復帰します。\n\n"
    },
    {
      "title": "仮想マシンのスクリーンショットを取得する",
      "level": 3,
      "content": "モニタコンソールで次のコマンドを実行することで PPM 形式で仮想マシンのグラフィックディスプレイのスクリーンショットを取得できます:\n\n```\n(qemu) screendump file.ppm\n```\n\n"
    },
    {
      "title": "QEMU マシンプロトコル",
      "level": 2,
      "content": "QEMU マシンプロトコル (QMP) は、アプリケーションが QEMU インスタンスを制御できるようにする JSON ベースのプロトコルです。#QEMU モニタ の様に実行中のマシンと対話する方法を提供し、JSON プロトコルによりプログラム的に行うことを可能にします。すべての QMP コマンドの説明は qmp-commands に記載されています。\n\n"
    },
    {
      "title": "QMP を開始する",
      "level": 3,
      "content": "QMP プロトコルを使用してゲストを制御する通常の方法は、-qmp オプションを使用してマシンを起動するときに TCP ソケットを開くことです。ここでは、例えば TCP ポート 4444 を使用しています:\n\n```\n$ qemu-system-x86_64 [...] -qmp tcp:localhost:4444,server,nowait\n```\n\nQMP エージェントと通信する方法の1つは netcatを使用することです:\n\n```\nnc localhost 4444\n```\n\n```\n{\"QMP\": {\"version\": {\"qemu\": {\"micro\": 0, \"minor\": 1, \"major\": 3}, \"package\": \"\"}, \"capabilities\": []} }\n```\n\nこの段階で、認識できるコマンドは qmp_capabilities のみであるため、QMP はコマンドモードに入ります。次を入力:\n\n```\n{\"execute\": \"qmp_capabilities\"}\n```\n\nこれで、QMP はコマンドを受信できるようになりました。認識されたコマンドのリストを取得するには、次のコマンドを使用します:\n\n```\n{\"execute\": \"query-commands\"}\n```\n\n"
    },
    {
      "title": "親イメージへの子イメージのライブマージ",
      "level": 3,
      "content": "block-commit コマンドを発行すると、実行中のスナップショットを親にマージできます。最も単純な形式では、次の行は子を親にコミットします:\n\n```\n{\"execute\": \"block-commit\", \"arguments\": {\"device\": \"devicename\"}}\n```\n\nこのコマンドを受信すると、ハンドラはベースイメージを探し、読み取り専用モードから読み取り/書き込みモードに変換し、コミットジョブを実行します。\n\nblock-commit 操作が完了すると、イベント BLOCK_JOB_READY が発生し、同期が完了したことが通知されます。次のコマンド block-job-complete を発行すると、ジョブを正常に完了できます。\n\n```\n{\"execute\": \"block-job-complete\", \"arguments\": {\"device\": \"devicename\"}}\n```\n\nこのようなコマンドが発行されるまで、commit 操作はアクティブなままです。 正常に完了した後、ベースイメージは読み取り/書き込みモードのままとなり、新しいアクティブレイヤになります。一方、子イメージは無効になり、クリーンアップするのはユーザの責任となります。\n\n```\n{\"execute\": \"query-block\"}\n```\n\n```\n{\"return\": [{\"io-status\": \"ok\", \"device\": \"ide0-hd0\", \"locked\": false, \"removable\": false, \"inserted\": {\"iops_rd\": 0, \"detect_zeroes\": \"off\", \"image\": {\"backing-image\": {\"virtual-size\": 27074281472, \"filename\": \"parent.qcow2\", ... }\n```\n\n"
    },
    {
      "title": "新しいスナップショットのライブ作成",
      "level": 3,
      "content": "実行中のイメージから新しいスナップショットを作成するには、次のコマンドを実行します:\n\n```\n{\"execute\": \"blockdev-snapshot-sync\", \"arguments\": {\"device\": \"devicename\",\"snapshot-file\": \"new_snapshot_name.qcow2\"}}\n```\n\nこれにより new_snapshot_name.qcow2 という名前のオーバーレイファイルが作成され、新しいアクティブレイヤになります。\n\n"
    },
    {
      "title": "仮想マシンのパフォーマンスを向上させる",
      "level": 3,
      "content": "仮想マシンのパフォーマンスを向上させるために使用できるテクニックは数多くあります。例えば:\n\n- 完全な仮想化のために #KVM を有効にする。\n- -cpu host オプションで QEMU により一般的な CPU ではなくホストの正確な CPU をエミュレートさせる。\n- 特に Windows ゲストの場合、Hyper-V enlightenments を有効にする: -cpu host,hv_relaxed,hv_spinlocks=0x1fff,hv_vapic,hv_time。詳細とフラグについては QEMU documentation を参照。\n- -smp cores=x,threads=y,sockets=1,maxcpus=z オプションを使用して、複数のコアをゲストに割り当てることができます。threads パラメータは、SMT コア の割り当てに使用されます。QEMU、ハイパーバイザ、およびホストシステムがスムーズに動作できるように物理コアを残しておくことは、非常に有益です。\n- 仮想マシンに十分なメモリーが割り当てられていることを確認する。デフォルトでは、QEMU は各仮想マシンに 128 MiB のメモリーのみを割り当てます。より多くのメモリーを割り当てるには、-m オプションを使用します。たとえば、-m 1024 は 1024 MiB のメモリーを持つ仮想マシンを実行します。\n- ゲストオペレーティングシステムのドライバでサポートされている場合は、ネットワークデバイスやブロックデバイスに virtio を使用する。#virtio ドライバーを使う を参照してください。\n- ユーザーモードネットワーキングの代わりに TAP デバイスを使用する。#QEMU の Tap ネットワーク を参照してください。\n- ゲスト OS がディスクに大量の書き込みを行っている場合、ホストのファイルシステムの特定のマウントオプションの恩恵を受けられます。たとえば、barrier=0 オプションを指定して ext4 ファイルシステム をマウントできます。ファイルシステムのパフォーマンス向上オプションはデータ整合性を犠牲にする場合があるため、変更したオプションについてはドキュメントを参照してください。\n- raw ディスクまたはパーティションがある場合、キャッシュを無効にしても良いでしょう: $ qemu-system-x86_64 -drive file=/dev/disk,if=virtio,cache=none\n- native Linux AIO を使う: $ qemu-system-x86_64 -drive file=disk_image,if=virtio,aio=native,cache.direct=on\n- 同じオペレーティングシステムがインストールされている複数の仮想マシンを同時に実行している場合、kernel same-page merging を有効にすることでメモリを節約できます。#KSMの有効化 を参照してください。\n- 場合によっては、ゲストオペレーティングシステムでメモリバルーニングドライバを実行することでメモリを回収できることがあります。 #メモリバルーニング を参照してください。\n- ICH-9 AHCI コントローラのエミュレーションレイヤを使用することができます(ただし、不安定な場合があります)。AHCI エミュレーションは Wikipedia:Native_Command_Queuing NCQ をサポートしているため、複数の読み書き要求を同時に発行できます: $ qemu-system-x86_64 -drive id=disk,file=disk_image,if=none -device ich9-ahci,id=ahci -device ide-drive,drive=disk,bus=ahci.0\n\n```\n$ qemu-system-x86_64 -drive file=/dev/disk,if=virtio,cache=none\n```\n\n```\n$ qemu-system-x86_64 -drive file=disk_image,if=virtio,aio=native,cache.direct=on\n```\n\n```\n$ qemu-system-x86_64 -drive id=disk,file=disk_image,if=none -device ich9-ahci,id=ahci -device ide-drive,drive=disk,bus=ahci.0\n```\n\n詳しくは https://www.linux-kvm.org/page/Tuning_KVM を参照してください。\n\n"
    },
    {
      "title": "実際のパーティションをハードディスクイメージのシングルプライマリパーティションとして使う",
      "level": 3,
      "content": "場合によって、QEMU の中からシステムパーティションのどれかを使いたくなることもあるでしょう。仮想マシンでの raw パーティションの使用は、読み書きの操作が物理ホストのファイルシステムレイヤーを通過しないため、パフォーマンスが高くなります。そのようなパーティションをホストとゲストでのデータの共有手段として使うこともできます。\n\nArch Linux では、raw パーティションのデバイスファイルは、デフォルトで、root と disk グループが所有者です。root 以外のユーザーで raw パーティションに読み書きできるようにしたい場合は、パーティションのデバイスファイルの所有者をそのユーザーに変えるか、そのユーザーを disk グループに追加するか、ACL を使用してより詳細なアクセス制御を行う必要があります。\n\n- 仮想マシンにホストシステムのクリティカルなデータ (root パーティションなど) を変更する許可を与えるのは可能ではありますが、推奨はされません。\n- ホストとゲストの両方で同時にパーティションのファイルシステムを読み書き可能でマウントしてはいけません。そうすると、データが破壊される可能性があります。\n\nその後、パーティションを QEMU の仮想マシンに仮想ディスクとしてアタッチできます。\n\nただし、仮想マシン 全体 をパーティションに収めたいときは、事態は多少複雑になります。そのような場合、実際に仮想マシンを起動するディスクイメージファイルがないために、MBR でパーティション分けされたデバイスではなくファイルシステムとしてフォーマットされたパーティションにブートローダーをインストールすることができません。このような仮想マシンは次のいずれかの方法でブートできます: #カーネルと initrd を手動で指定する、#MBR で仮想ディスクをシミュレートする、#device-mapper を使う、#リニア RAID を使う、または#ネットワークブロックデバイスを使う。\n\n"
    },
    {
      "title": "カーネルと initrd を手動で指定する",
      "level": 4,
      "content": "QEMU は GRUB などのブートローダーを迂回して、Linux カーネルと init ramdisk を直接ロードすることをサポートしています。それから root ファイルシステムを含んでいる物理パーティションで、パーティションされていない仮想ディスクとして起動することができます。以下のようなコマンドを実行することで行います:\n\n```\n$ qemu-system-x86_64 -kernel /boot/vmlinuz-linux -initrd /boot/initramfs-linux.img -append root=/dev/sda /dev/sda3\n```\n\n上の例では、ゲストの root ファイルシステムに使われている物理パーティションはホストの /dev/sda3 で、ゲストでは /dev/sda として表示されます。\n\nもちろん、Arch Linux で使われているものとは違うカーネルや initrd を自由に指定することができます。\n\n複数のカーネルパラメータを -append オプションで指定するときは、クォートを使って囲む必要があります。例:\n\n```\n... -append 'root=/dev/sda1 console=ttyS0'\n```\n\n"
    },
    {
      "title": "MBR で仮想ディスクをシミュレートする",
      "level": 4,
      "content": "ファイルシステムでフォーマットされたパーティションを維持しながらゲストのパーティションをまるでディスクであるかのようなパーティションにしないで、仮想マシンで物理パーティションを使用するもっと複雑な方法として、MBR をシミュレートして GRUB などのブートローダーを使って起動できるようにする方法があります。\n\n次の例では、マウントされていないプレーンな /dev/hdaN' パーティションがあり、その中に QEMU ディスクイメージの一部にしたいファイルシステムがあるとします。秘訣は、 QEMU rawディスクイメージに組み込みたい実パーティションに、動的にマスターブートレコード (MBR) を付加することです。より一般的には、このパーティションは、より大きなシミュレートされたディスクの任意の部分、特に元の物理ディスクをシミュレートしますが /dev/hdaN だけを仮想マシンに公開するブロックデバイスにすることができます。\n\nこのタイプの仮想ディスクは、MBRとパーティションへの参照(コピー)を含む VMDK ファイルで表すことができますが、 QEMU はこの VMDK フォーマットをサポートしていません。たとえば、 以下のように作成された 仮想ディスクは\n\n```\n$ VBoxManage internalcommands createrawvmdk -filename /path/to/file.vmdk -rawdisk /dev/hda\n```\n\n以下のエラーメッセージとともに QEMU によって拒否されます\n\n```\nUnsupported image type 'partitionedDevice'\n```\n\nVBoxManage は file.vmdk と file-pt.vmdk の2つのファイルを作成します。後者は MBR のコピーであり、テキスト・ファイル file.vmdk が参照します。ターゲットパーティションまたは MBR の外側への読取り操作にはゼロが返され、書き込まれたデータは破棄されます。\n\nVMDK ディスクリプタ・ファイルを利用するのと同様の方法で、 device-mapper を利用して、 MBR ファイルに付属するループ・デバイスを対象パーティションにプリペンドする方法があります。仮想ディスクのサイズがオリジナルと同じである必要がない場合、まず MBR を保持するためのファイルを作成します。\n\n```\n$ dd if=/dev/zero of=/path/to/mbr count=2048\n```\n\nこれで、最新のディスクパーティションツールが使用するパーティションアライメントポリシーに従った 1 MiB (2048*512バイト) のファイルが作成されます。古いパーティショニングソフトウェアとの互換性のために、2048 セクタではなく 63 セクタが必要になる場合があります。MBR に必要なブロックは 512 バイトのみです。追加の空き領域は BIOS ブートパーティションや、ハイブリッドパーティショニングスキームの場合は GUID パーティションテーブルに使用できます。次に、ループデバイスを MBR ファイルにアタッチします:\n\n```\n# losetup --show -f /path/to/mbr\n```\n\n```\n/dev/loop0\n```\n\nこの例では、結果のデバイスは /dev/loop0 です。device mapper を使って MBR とパーティションを結合します:\n\n```\n# echo \"0 2048 linear /dev/loop0 0\n2048 `blockdev --getsz /dev/hdaN` linear /dev/hdaN 0\" | dmsetup create qemu\n```\n\n生成された /dev/mapper/qemu は、 QEMU の raw ディスクイメージとして使用します。仮想ディスク上にパーティションテーブル(例としてリニア RAID の使用について説明したセクションを参照)とブートローダーコード(/path/to/mbr に保存されます)を作成するには、追加の手順が必要です。\n\n次の設定例では、仮想ディスク上の {ic|/dev/hdaN}} の位置を物理ディスク上と同じにし、コピーとして提供される MBR を除き、ディスクの残りの部分を非表示にしています:\n\n```\n# dd if=/dev/hda count=1 of=/path/to/mbr\n# loop=`losetup --show -f /path/to/mbr`\n# start=`blockdev --report /dev/hdaN | tail -1 | awk '{print $5}'`\n# size=`blockdev --getsz /dev/hdaN`\n# disksize=`blockdev --getsz /dev/hda`\n# echo \"0 1 linear $loop 0\n1 $((start-1)) zero\n$start $size linear /dev/hdaN 0\n$((start+size)) $((disksize-start-size)) zero\" | dmsetup create qemu\n```\n\ndmsetup への標準入力として提供されるテーブルは、 VBoxManage によって作成された VMDK 記述子ファイル内のテーブルと同様のフォーマットを持ち、代わりに dmsetup create qemu--tabletable_file でファイルからロードできます。仮想マシンには /dev/hdaN だけがアクセス可能で、ハードディスクの残りはゼロとして読み取られ、書き込まれたデータは最初のセクタを除いて破棄されます。 dmsetup table qemu で /dev/mapper/qemu のテーブルを表示できます (udevadm info -rq name /sys/dev/block/major:minor で major:minor を /dev/blockdevice 名に変換してください) 。作成されたデバイスを削除するには dmsetup remove qemu と losetup -d $loop を使います。\n\nこの例が役に立つ状況は、マルチブート構成の既存の Windows XP インストールと、おそらくハイブリッドパーティションスキーム(物理ハードウェアでは、Windows XP が MBR パーティションテーブルを使用する唯一のオペレーティングシステムであり、同じコンピュータにインストールされた最新のオペレーティングシステムは GUID パーティションテーブルを使用できます)の場合です。Windows XP はハードウェアプロファイルをサポートしているため、同じインストールを異なるハードウェア構成で交互に使用できます(この場合はベアメタルと仮想)。Windows は新しく検出されたハードウェアのドライバをプロファイルごとに1回だけインストールする必要があります。この例ではコピーされた MBR のブートローダコードを更新して、元のシステムに存在するマルチブート対応のブートローダ(GRUB など)を起動するのではなく、 /dev/hdaN から直接 Windows XP をロードする必要があることに注意してください。または、ブートローダインストールを含むブートパーティションのコピーを MBR と同じ方法で仮想ディスクに含めることもできます。\n\nリニアモードのソフトウェア RAID (linear.ko カーネルドライバが必要です)とループバックデバイスを使うこともできます:\n\n最初に、MBR を保持する小さなファイルを作成します:\n\n```\n$ dd if=/dev/zero of=/path/to/mbr count=32\n```\n\nこれで 16 KB (32 * 512 バイト) のファイルが作成されます。(MBR は512バイトのブロックしか必要としませんが) あまり小さくしすぎないことが重要です。なぜならファイルを小さくすればするほど、ソフトウェア RAID デバイスのチャンクサイズも小さくしなくてはならなくなり、パフォーマンスに影響を与えるからです。次に、MBR ファイルのループバックデバイスを設定します:\n\n```\n# losetup -f /path/to/mbr\n```\n\n他にループバックを使っていないために、作成されるデバイスは /dev/loop0 になるとします。次のステップはソフトウェア RAID を使って \"マージされた\" MBR + /dev/hdaN ディスクイメージを作成することです:\n\n```\n# modprobe linear\n# mdadm --build --verbose /dev/md0 --chunk=16 --level=linear --raid-devices=2 /dev/loop0 /dev/hdaN\n```\n\n作成した /dev/md0 は QEMU の raw ディスクイメージとして使用します (エミュレータがアクセスできるようにパーミッションを設定するのを忘れないでください)。最後にプライマリパーティションの開始位置が /dev/md0 の /dev/hdaN に一致するようにディスクの設定 (ディスクのジオメトリとパーティションテーブルの設定) を行います (上の例では 16 * 512 = 16384 バイトのオフセットです)。ホストマシンで fdisk を使って行ってください。エミュレータの中で行ってはいけません: QEMU のデフォルトのディスク認識ルーチンによってキロバイトで割り切れないオフセットが生まれるとソフトウェア RAID コードで管理できなくなります。ホストから以下を実行してください:\n\n```\n# fdisk /dev/md0\n```\n\nX を押してエキスパートメニューを開きます。トラック毎のセクタ数を設定 ('s') してシリンダーの容量が MBR ファイルの容量に一致するようにしてください。ヘッダが 2 つでセクタサイズが 512 の場合、1 トラックあたりのセクタ数は 16 となり、シリンダのサイズは 2x16x512=16k になります。\n\nR を押してメインメニューに戻って下さい。\n\nP を押してシリンダーのサイズが 16k になってることを確認します。\n\n/dev/hdaN にあわせてシングルプライマリパーティションを作成してください。パーティションの開始位置はシリンダー 2 で終了位置はディスクの末尾になります (シリンダーの数は fdisk に入力したときの値で変わります)。\n\n最後に、結果をファイルに書き出してください ('w')。これでホストから直接パーティションをディスクイメージとしてマウントすることができます:\n\n```\n$ qemu-system-x86_64 -hdc /dev/md0 [...]\n```\n\nもちろん、元のパーティション /dev/hdaN に必要ツールが含まれていれば、QEMU を使ってディスクイメージにブートローダーを設定できます。\n\nNetwork Block Device によって、Linux はリモートサーバをブロックデバイスの1つとして使うことができます。 nbd-server (nbd パッケージ)を使って、QEMU 用の MBR ラッパーを作成することができます。\n\n上記のように MBR ラッパーファイルを設定したら、wrapper.img.0 に名前を変更してください。そして同じディレクトリに wrapper.img.1 という名前のシンボリックリンクを作成して、パーティションにリンクするようにしてください。また、同じディレクトリに以下のスクリプトを作成します:\n\n```\n#!/bin/sh\ndir=\"$(realpath \"$(dirname \"$0\")\")\"\ncat >wrapper.conf <<EOF\n[generic]\nallowlist = true\nlistenaddr = 127.713705\nport = 10809\n\n[wrap]\nexportname = $dir/wrapper.img\nmultifile = true\nEOF\n\nnbd-server \\\n    -C wrapper.conf \\\n    -p wrapper.pid \\\n    \"$@\"\n```\n\n.0 と .1 という拡張子が重要です。他は変えてもかまいません。上記のスクリプトを実行後 (nbd-server がパーティションにアクセスできるように root で実行してください)、以下のコマンドで QEMU を起動できます:\n\n```\nqemu-system-x86_64 -drive file=nbd:127.713705:10809:exportname=wrap [...]\n```\n\n"
    },
    {
      "title": "libvirt を使う",
      "level": 4,
      "content": "仮想マシンが libvirt でセットアップされている場合、virsh autostart または virt-manager GUI を使用して、仮想マシンの Boot Options に移動して Start virtual machine on host boot up を選択することで、ホストのブート時に仮想マシンを開始するように構成できます。\n\n"
    },
    {
      "title": "systemd サービスを使う",
      "level": 4,
      "content": "ブート時に QEMU 仮想マシンを実行するには、次の systemd ユニットと設定を使うことができます。\n\n```\n/etc/systemd/system/qemu@.service\n```\n\n```\n[Unit]\nDescription=QEMU virtual machine\n\n[Service]\nEnvironment=\"haltcmd=kill -INT $MAINPID\"\nEnvironmentFile=/etc/conf.d/qemu.d/%i\nExecStart=/usr/bin/qemu-system-x86_64 -name %i -enable-kvm -m 512 -nographic $args\nExecStop=/usr/bin/bash -c ${haltcmd}\nExecStop=/usr/bin/bash -c 'while nc localhost 7100; do sleep 1; done'\n\n[Install]\nWantedBy=multi-user.target\n```\n\n次に、変数 args と haltcmd がセットされた /etc/conf.d/qemu.d/vm_name という名前の VM 毎の設定ファイルを作成します。設定例は:\n\n```\n/etc/conf.d/qemu.d/one\n```\n\n```\nargs=\"-hda /dev/vg0/vm1 -serial telnet:localhost:7000,server,nowait,nodelay \\\n -monitor telnet:localhost:7100,server,nowait,nodelay -vnc :0\"\n\nhaltcmd=\"echo 'system_powerdown' | nc localhost 7100\" # or netcat/ncat\n```\n\n```\n/etc/conf.d/qemu.d/two\n```\n\n```\nargs=\"-hda /srv/kvm/vm2 -serial telnet:localhost:7001,server,nowait,nodelay -vnc :1\"\n\nhaltcmd=\"ssh powermanager@vm2 sudo poweroff\"\n```\n\n変数の説明は次のとおりです。\n\n- args - 使用する QEMU コマンドライン引数です。\n- haltcmd - 仮想マシンを安全にシャットダウンするためのコマンド。最初の例では、QEMU モニターは -monitor telnet:.. を使用して telnet 経由で公開され、仮想マシンは nc コマンドで system_powerdown をモニターに送信することで ACPI 経由で電源がオフになります。他の例では、SSH が使われます。\n\nブートアップ時にどの仮想マシンを開始するかを設定するには、qemu@vm_name.service systemd ユニットを 有効化 します。\n\n"
    },
    {
      "title": "マウスの統合",
      "level": 3,
      "content": "ゲストオペレーティングシステムのウィンドウをクリックしたときにマウスをつかまれないようにするには、-usb -device usb-tablet オプションを追加します。これにより、 QEMU はマウスをつかむことなくマウスの位置を伝えることができるようになります。また、このコマンドは有効化されている場合 PS/2 マウスエミュレーションを上書きします。例えば:\n\n```\n$ qemu-system-x86_64 -hda disk_image -m 512 -usb -device usb-tablet\n```\n\nそれでもうまくいかない場合、-vga qxl パラメータを使ってみてください。また、#マウスカーソルが敏感すぎたり迷走する も参照してみて下さい。\n\n"
    },
    {
      "title": "ホスト USB デバイスのパススルー",
      "level": 3,
      "content": "ゲストからホストの USB ポートに接続された物理デバイスにアクセスできます。最初のステップはデバイスが接続されている場所を特定することです。これは lsusb コマンドを実行して確認できます。例えば:\n\n```\n$ lsusb\n```\n\n```\n...\nBus 003 Device 007: ID 0781:5406 SanDisk Corp. Cruzer Micro U3\n```\n\n上記の太字の出力は、それぞれ host_bus と host_addr 、または vendor_id と product_id を識別するのに役立ちます。\n\nqemu では、-device usb-ehci,id=ehci または -device qemu-xhci,id=xhci オプションを使用して EHCI (USB 2) または XHCI (USB 1.1 USB 2 USB 3) コントローラーをエミュレートし、次に -device usb-host,.. オプションを使用して物理デバイスをこのコントローラーに接続するという考え方になっています。このセクションの残りの部分では controller_id が ehci または xhci であるとみなします。\n\n次に、qemu でホストの USB に接続する方法は2つあります:\n\n1. デバイスを識別し、ホスト上でデバイスが接続されているバスとアドレスでデバイスに接続します。一般的な構文は次のとおりです: -device usb-host,bus=controller_id.0,vendorid=0xvendor_id,productid=0xproduct_id上の例で使用されているデバイスに適用すると、次のようになります:-device usb-ehci,id=ehci -device usb-host,bus=ehci.0,vendorid=0x0781,productid=0x5406 前のオプションに...,port=port_number 設定を追加して、デバイスを接続する仮想コントローラの物理ポートを指定することもできます。これは、複数の USB デバイスを仮想マシンに追加したい場合に便利です。もう1つのオプションは QEMU 5.1.0 以降で利用可能な usb-host の新しい hostdevice プロパティを使用することで、構文は次のとおりです: -device qemu-xhci,id=xhci -device usb-host,hostdevice=/dev/bus/usb/003/007\n1. 任意の USB バスとアドレスに接続されているものを接続します。構文は次のようになります:-device usb-host,bus=controller_id.0,hostbus=host_bus,host_addr=host_addr 上記の例のバスとアドレスに適用すると、次のようになります:-device usb-ehci,id=ehci -device usb-host,bus=ehci.0,hostbus=3,hostaddr=7\n\n```\n-device usb-host,bus=controller_id.0,vendorid=0xvendor_id,productid=0xproduct_id\n```\n\n```\n-device usb-ehci,id=ehci -device usb-host,bus=ehci.0,vendorid=0x0781,productid=0x5406\n```\n\n```\n-device qemu-xhci,id=xhci -device usb-host,hostdevice=/dev/bus/usb/003/007\n```\n\n```\n-device usb-host,bus=controller_id.0,hostbus=host_bus,host_addr=host_addr\n```\n\n```\n-device usb-ehci,id=ehci -device usb-host,bus=ehci.0,hostbus=3,hostaddr=7\n```\n\n詳しくは QEMU/USB エミュレーション を参照してください。\n\n"
    },
    {
      "title": "SPICE による USB リダイレクト",
      "level": 3,
      "content": "#SPICE を使用しているのであれば、QEMU コマンドで指定しなくてもクライアントから仮想マシンに USB デバイスをリダイレクトすることが可能です。リダイレクトされたデバイスが利用できる USB スロットの数を設定することができます (スロットの数によって、同時にリダイレクトできるデバイスの最大数が決まります)。前述の -usbdevice 方式と比較して、リダイレクトに SPICE を使用する主な利点は、仮想マシンの開始後に USB デバイスをホットスワップできることで、リダイレクトから USB デバイスを削除したり新しいデバイスを追加したりするために USB デバイスを停止する必要がありません。また、ネットワーク経由でクライアントからサーバーに USB デバイスをリダイレクトすることもできます。まとめると、これは QEMU 仮想マシンで USB デバイスを使用する最も柔軟な方法です。\n\n利用可能な USB リダイレクトスロットごとに1つの EHCI/UHCI コントローラを追加し、さらにスロットごとに1つの SPICE リダイレクションチャネルを追加する必要があります。たとえば、SPICE モードで仮想マシンを開始するために使用する QEMU コマンドに以下の引数を追加すると、リダイレクトに利用可能な3つの USB スロットを持つ仮想マシンが開始されます:\n\n```\n-device ich9-usb-ehci1,id=usb \\\n-device ich9-usb-uhci1,masterbus=usb.0,firstport=0,multifunction=on \\\n-device ich9-usb-uhci2,masterbus=usb.0,firstport=2 \\\n-device ich9-usb-uhci3,masterbus=usb.0,firstport=4 \\\n-chardev spicevmc,name=usbredir,id=usbredirchardev1 -device usb-redir,chardev=usbredirchardev1,id=usbredirdev1 \\\n-chardev spicevmc,name=usbredir,id=usbredirchardev2 -device usb-redir,chardev=usbredirchardev2,id=usbredirdev2 \\\n-chardev spicevmc,name=usbredir,id=usbredirchardev3 -device usb-redir,chardev=usbredirchardev3,id=usbredirdev3\n```\n\n詳しくは SPICE/usbredir を参照してください。\n\nspice-gtk (Input>Select USB Devices for redirection) の spicy と virt-viewer (File>USB device selection) の remote-viewer の両方がこの機能をサポートしています。この機能が期待どおりに動作するために必要な SPICE ゲストツールが仮想マシンにインストールされていることを確認してください (詳細については、#SPICE セクションを参照してください)。\n\n"
    },
    {
      "title": "udev による自動 USB 転送",
      "level": 4,
      "content": "通常、転送されるデバイスは仮想マシンの起動時に利用可能になっている必要があります。デバイスが切断されると、転送されなくなります。\n\nudev を使用して、デバイスがオンラインになったときに自動的にデバイスを接続できます。ディスク上のどこかに hostdev エントリを作成します。root に chown し、他のユーザーが変更できないようにします。\n\n```\n/usr/local/hostdev-mydevice.xml\n```\n\n```\n<hostdev mode='subsystem' type='usb'>\n  <source>\n    <vendor id='0x03f0'/>\n    <product id='0x4217'/>\n  </source>\n</hostdev>\n```\n\n次に、デバイスをアタッチ/デタッチする udev ルールを作成します。\n\n```\n/usr/lib/udev/rules.d/90-libvirt-mydevice\n```\n\n```\nACTION==\"add\", \\\n    SUBSYSTEM==\"usb\", \\\n    ENV{ID_VENDOR_ID}==\"03f0\", \\\n    ENV{ID_MODEL_ID}==\"4217\", \\\n    RUN+=\"/usr/bin/virsh attach-device GUESTNAME /usr/local/hostdev-mydevice.xml\"\nACTION==\"remove\", \\\n    SUBSYSTEM==\"usb\", \\\n    ENV{ID_VENDOR_ID}==\"03f0\", \\\n    ENV{ID_MODEL_ID}==\"4217\", \\\n    RUN+=\"/usr/bin/virsh detach-device GUESTNAME /usr/local/hostdev-mydevice.xml\"\n```\n\n出典および詳細情報。\n\n"
    },
    {
      "title": "KSM の有効化",
      "level": 3,
      "content": "Kernel Samepage Merging (KSM) はアプリケーションがページをマージするように登録した他のプロセスとページをマージするようにカーネルに登録できるようにする Linux カーネルの機能です。この KSM 機構によってゲストの仮想マシンは互いにページを共有することが可能になります。同じようなゲストオペレーティングシステムが多数動作する環境では、メモリの使用量を著しく節約することができます。\n\nKSM を有効にするには:\n\n```\n# echo 1 > /sys/kernel/mm/ksm/run\n```\n\nsystemd の一時ファイルを使って KSM を永続的に有効にできます:\n\n```\n/etc/tmpfiles.d/ksm.conf\n```\n\n```\nw /sys/kernel/mm/ksm/run - - - - 1\n```\n\nKSM が動作しているとき、マージされるページが存在するために (つまり少なくとも2つの同じような仮想マシンが動いている)、/sys/kernel/mm/ksm/pages_shared はゼロになりません。詳しくは https://docs.kernel.org/admin-guide/mm/ksm.html を参照。\n\n```\n$ grep -r . /sys/kernel/mm/ksm/\n```\n\n"
    },
    {
      "title": "マルチモニターのサポート",
      "level": 3,
      "content": "Linux QXL ドライバーはデフォルトで4台までのマルチモニター (仮想スクリーン) をサポートしています。qxl.heads=N カーネルパラメータで変更することができます。\n\nQXL デバイスのデフォルトの VGA メモリサイズは 16M です (VRAM のサイズは 64M です)。1920x1200 のモニターを2台使用しようとすると 2 × 1920 × 4 (色深度) × 1200 = 17.6 MiB の VGA メモリが必要になるためメモリが不足します。-vga qxl を -vga none -device qxl-vga,vgamem_mb=32 に置き換えることでメモリ容量を変更できます。vgamem_mb を 64M 以上に増やした場合、vram_size_mb オプションも増やす必要があります。\n\n"
    },
    {
      "title": "Custom display resolution",
      "level": 3,
      "content": "A custom display resolution can be set with -device VGA,edid=on,xres=1280,yres=720 (see EDID and display resolution).\n\n"
    },
    {
      "title": "SPICE",
      "level": 4,
      "content": "ホストとゲストの間でクリップボードを共有する方法の1つは、SPICE リモートデスクトッププロトコルを有効にし、SPICE クライアントを使用してクライアントにアクセスすることです。 #SPICE で説明されている手順に従う必要があります。この方法で実行されるゲストは、ホストとのコピーペーストをサポートします。\n\n"
    },
    {
      "title": "qemu-vdagent",
      "level": 4,
      "content": "QEMU は qemu-vdagent という spice vdagent chardev の独自の実装を提供しています。この実装は、スパイス-vdagent ゲストサービスとのインタフェースとなり、ゲストとホストがクリップボードを共有できるようにします。\n\nQEMU の GTK ディスプレイでこの共有クリップボードにアクセスするには、--enable-gtk-clipboard 設定パラメータを指定して ソースから QEMU をコンパイルする必要があります。インストールされている qemu-ui-gtk パッケージを置き換えるだけで十分です。\n\n- 公式パッケージで機能を有効にするための機能リクエスト FS#79716 が提出されました。\n- qemu-ui-gtk の共有クリップボードは、特定の状況下で Linux ゲストをフリーズさせる 可能性があるため、実験的なものに戻されました。アップストリームでこの問題を解決するための修正が提案されています。\n\n以下の QEMU コマンドライン引数を追加します:\n\n```\n-device virtio-serial,packed=on,ioeventfd=on\n-device virtserialport,name=com.redhat.spice.0,chardev=vdagent0\n-chardev qemu-vdagent,id=vdagent0,name=vdagent,clipboard=on,mouse=off\n```\n\nこれらの引数は、libvirt 形式 に変換した場合にも有効です。\n\nLinux ゲストでは、spice-vdagent.service ユーザーユニット を手動で 開始 できます。Windows ゲストでは、spice-agent のスタートアップタイプを自動に設定します。\n\n"
    },
    {
      "title": "Windows 特有のノート",
      "level": 3,
      "content": "QEMU は Windows 95 から Windows 11 まで全てのバージョンの Windows を動かすことができます。\n\nQEMU で Windows PE を実行することも可能です。\n\n"
    },
    {
      "title": "高速スタートアップ",
      "level": 4,
      "content": "Windows 8 (またはそれ以降) のゲストでは、次の フォーラムページ で説明されているようにコントロールパネルの電源オプションから \"高速スタートアップを有効にする(推奨)\" を無効にすることをお勧めします。この設定は、1回おきの起動時にゲストがハングする原因となるためです。\n\n-smpオプションへの変更を正しく適用するには、高速スタートアップを無効にする必要がある場合もあります。\n\n"
    },
    {
      "title": "リモートデスクトッププロトコル",
      "level": 4,
      "content": "MS Windows ゲストを使っている場合、RDP を使ってゲスト仮想マシンに接続する方法もあります。VLAN を使用していてゲストが同じネットワークにない場合、次を使って下さい:\n\n```\n$ qemu-system-x86_64 -nographic -nic user,hostfwd=tcp::5555-:3389\n```\n\n次に、rdesktop または freerdp を使用してゲストに接続します。例えば:\n\n```\n$ xfreerdp -g 2048x1152 localhost:5555 -z -x lan\n```\n\n"
    },
    {
      "title": "物理機器にインストールされた Linux システムのクローン",
      "level": 3,
      "content": "物理的な機器にインストールされた Linux システムをクローンして、QEMU 仮想マシン上で動作させることができます。QEMU 仮想マシンのためにハードウェアから Linux システムをクローンする を参照してください。\n\n"
    },
    {
      "title": "x86_64 から arm/arm64 環境への chrooting",
      "level": 3,
      "content": "実際の ARM ベースのデバイスではなく、ディスクイメージを直接操作する方が簡単な場合もあります。これは、root パーティションを含む SD カード/ストレージをマウントし、そこに chroot することで実現できます。\n\nARM chroot のもうひとつのユースケースは、x86_64 マシン上で ARM パッケージを構築することです。ここで、chroot 環境を Arch Linux ARM のイメージ tarball から作成することができます - このアプローチの詳細は [5] を参照してください。\n\nいずれにせよ、chroot から pacman を実行し、より多くのパッケージをインストールしたり、大きなライブラリをコンパイルしたりできるようになるはずです。実行可能ファイルは ARM アーキテクチャ用なので、x86 への変換は QEMU で行う必要があります。\n\nx86_64 マシン/ホストに qemu-user-static を、qemu バイナリを binfmt サービスに登録するために qemu-user-static-binfmt インストールしてください。\n\nqemu-user-static は他のアーキテクチャーからコンパイルされたプログラムの実行を許可するために使用されます。これは qemu-emulators-full で提供されるているものと似ていますが、chroot には static バリアントが必要です。例えば:\n\n```\nqemu-arm-static path_to_sdcard/usr/bin/ls\nqemu-aarch64-static path_to_sdcard/usr/bin/ls\n```\n\nこれらの2行はそれぞれ 32ビット ARM と 64ビット ARM 用にコンパイルされた ls コマンドを実行します。これは、ホストシステムに存在しないライブラリを探すため、chroot なしでは動作しないことに注意してください。\n\nqemu-user-static-binfmt では、ARM 実行可能ファイルの前に qemu-arm-static または qemu-aarch64-static を自動的に付けることができます。\n\nARM 実行可能サポートがアクティブであることを確認します:\n\n```\n$ ls /proc/sys/fs/binfmt_misc\n```\n\n```\nqemu-aarch64  qemu-arm\t  qemu-cris  qemu-microblaze  qemu-mipsel  qemu-ppc64\t    qemu-riscv64  qemu-sh4    qemu-sparc\tqemu-sparc64  status\nqemu-alpha    qemu-armeb  qemu-m68k  qemu-mips\t      qemu-ppc\t   qemu-ppc64abi32  qemu-s390x\t  qemu-sh4eb  qemu-sparc32plus\tregister\n```\n\nそれぞれの実行可能ファイルがリストアップされている必要があります。\n\nアクティブでない場合は、systemd-binfmt.service を 再起動 してください。\n\nSD カードを /mnt/sdcard にマウントしてください(デバイス名は異なる場合があります)。\n\n```\n# mount --mkdir /dev/mmcblk0p2 /mnt/sdcard\n```\n\n必要に応じてブートパーティションをマウントします(ここでも適切なデバイス名を使用します):\n\n```\n# mount /dev/mmcblk0p1 /mnt/sdcard/boot\n```\n\n最後に Chroot#chroot を使う の説明に従って SD カードのルートに chroot してください:\n\n```\n# chroot /mnt/sdcard /bin/bash\n```\n\narch-install-scripts の arch-chroot を使用することもできます。これはネットワークサポートを得るためのより簡単な方法を提供します:\n\n```\n# arch-chroot /mnt/sdcard /bin/bash\n```\n\nsystemd-nspawn を使用して ARM 環境に chroot することもできます:\n\n```\n# systemd-nspawn -D /mnt/sdcard -M myARMMachine --bind-ro=/etc/resolv.conf\n```\n\n--bind-ro=/etc/resolv.conf はオプションで、chroot内部で動作中のネットワーク DNS を提供します。\n\n"
    },
    {
      "title": "sudo in chroot",
      "level": 4,
      "content": "chroot に sudo をインストールし、使用する際に次ののエラーが発生した場合:\n\n```\nsudo: effective uid is not 0, is /usr/bin/sudo on a file system with the 'nosuid' option set or an NFS file system without root privileges?\n```\n\nbinfmt フラグを変更する必要がある場合があります。例えば、aarch64 の場合:\n\n```\n# cp /usr/lib/binfmt.d/qemu-aarch64-static.conf /etc/binfmt.d/\n# vi /etc/binfmt.d/qemu-aarch64-static.conf\n```\n\nこのファイルの最後に C を追加:\n\n```\n:qemu-aarch64:M::\\x7fELF\\x02\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\xb7\\x00:\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfe\\xff\\xff\\xff:/usr/bin/qemu-aarch64-static:FPC\n```\n\n次に、systemd-binfmt.service を 再起動 し、変更が有効になっていることを確認します(flags 行の C に注意してください):\n\n```\n# cat /proc/sys/fs/binfmt_misc/qemu-aarch64\n```\n\n```\nenabled\ninterpreter /usr/bin/qemu-aarch64-static\nflags: POCF\noffset 0\nmagic 7f454c460201010000000000000000000200b700\nmask ffffffffffffff00fffffffffffffffffeffffff\n```\n\n詳細については、カーネル binfmt ドキュメント の \"flags\" セクションを参照してください。\n\n"
    },
    {
      "title": "マウス入力をつかまない",
      "level": 3,
      "content": "タブレットモードには、QEMU ウィンドウでマウス入力をつかまないという副作用があります:\n\n```\n-usb -device usb-tablet\n```\n\nいくつかの -vga バックエンドで動作しますが、そのうちのひとつは virtio です。\n\n"
    },
    {
      "title": "マウスカーソルが敏感すぎたり迷走する",
      "level": 3,
      "content": "カーソルが画面を飛び回って手に負えない場合、QEMU を起動する前にターミナルに次を入力することで直るかもしれません:\n\n```\n$ export SDL_VIDEO_X11_DGAMOUSE=0\n```\n\nこのコマンドで直ったら、~/.bashrc ファイルにコマンドを追加することができます。\n\n"
    },
    {
      "title": "カーソルが表示されない",
      "level": 3,
      "content": "マウスカーソルを表示するには -display default,show-cursor=on を QEMU のオプションに追加してください。\n\nオプションを追加しても表示されない場合、ディスプレイデバイスが正しく設定されているか確認してください。例: -vga qxl。\n\n#マウスの統合 で説明されているように -usb-device usb-tablet を試すこともできます。これはデフォルトの PS/2 マウスエミュレーションを上書きし、追加のボーナスとしてホストとゲスト間でポインタ位置を同期させます。\n\n"
    },
    {
      "title": "2つの異なるマウスカーソルが表示される",
      "level": 3,
      "content": "ヒント #マウスの統合 を適用してください。\n\n"
    },
    {
      "title": "VNC 使用時のキーボードの問題",
      "level": 3,
      "content": "VNC の使用中、ここに (生々しく詳細に) 書かれているキーボードの問題を経験するかもしれません。解決策は QEMU で -k オプションを使用 しない ことと、gtk-vnc の gvncviewer を使用することです。libvirt のメーリングリストに投稿された この メッセージも参照してください。\n\n"
    },
    {
      "title": "キーボードが壊れているまたは矢印キーが動作しない",
      "level": 3,
      "content": "キーの一部が動かなかったり間違ったキーが \"押されてしまう\" (特に矢印キー) ときは、おそらくキーボードレイアウトをオプションとして指定する必要があります。キーボードレイアウトは /usr/share/qemu/keymaps で探すことができます。\n\n```\n$ qemu-system-x86_64 -k keymap disk_image\n```\n\n"
    },
    {
      "title": "キーマップファイルを読み込めない",
      "level": 3,
      "content": "```\nqemu-system-x86_64: -disnplay vnc=0.0.0.0:0: could not read keymap file: 'en'\n```\n\n-k 引数に渡された無効な keymap が原因です。たとえば、en は無効ですが、en-us は有効です。/usr/share/qemu/keymaps/ を参照してください。\n\n"
    },
    {
      "title": "ウィンドウのリサイズでゲストのディスプレイが引き伸ばされる",
      "level": 3,
      "content": "デフォルトのウィンドウサイズに戻すには、Ctrl+Alt+u を押して下さい。\n\n"
    },
    {
      "title": "ioctl(KVM_CREATE_VM) failed: 16 Device or resource busy",
      "level": 3,
      "content": "-enable-kvm オプションを使って QEMU を起動した時に以下のようなエラーメッセージが表示される場合:\n\n```\nioctl(KVM_CREATE_VM) failed: 16 Device or resource busy\nfailed to initialize KVM: Device or resource busy\n```\n\n他の ハイパーバイザ が動作しています。同時に複数のハイパーバイザを動かすのは推奨されていません、もしくは不可能です。\n\n"
    },
    {
      "title": "libgfapi エラーメッセージ",
      "level": 3,
      "content": "起動時に以下のエラーメッセージが表示される場合:\n\n```\nFailed to open module: libgfapi.so.0: cannot open shared object file: No such file or directory\n```\n\nglusterfs を インストール するか無視してください。GlusterFS はオプションの依存関係です。\n\n"
    },
    {
      "title": "ライブ環境でカーネルパニックが発生する",
      "level": 3,
      "content": "ライブ環境を起動した(あるいはシステムを起動)際に以下が発生する:\n\n```\n[ end Kernel panic - not syncing: VFS: Unable to mount root fs on unknown block(0,0)\n```\n\nまたは起動を妨げる他の処理(例えば initramfs をアンパックできない、サービス foo を起動できない)など。 -m VALUE スイッチを付けて適当な量の RAM を指定して仮想マシンを開始してみてください。メモリスイッチがない場合、RAM が足りなくなると上記のような問題が発生することがあります。\n\n"
    },
    {
      "title": "Windows 7 ゲストの音質が酷い",
      "level": 3,
      "content": "Windows 7 ゲストで hda オーディオドライバを使用すると、音質が低下する場合があります。-soundhw ac97 引数をQEMU に渡してオーディオドライバを ac97 に変更し、ゲストに Realtek AC'97 Audio Codecs[リンク切れ 2025-01-22] の AC97 ドライバをインストールすると問題が解決する場合があります。詳しくは Red Hat Bugzilla - Bug 1176761 を参照してください。\n\n"
    },
    {
      "title": "Could not access KVM kernel module: Permission denied",
      "level": 3,
      "content": "以下のエラーが表示される場合:\n\n```\nlibvirtError: internal error: process exited while connecting to monitor: Could not access KVM kernel module: Permission denied failed to initialize KVM: Permission denied\n```\n\nSystemd 234 は kvm グループに動的 ID を割り当てます (FS#54943 を参照)。このエラーを回避するには、/etc/libvirt/qemu.conf ファイルを編集して group = \"78\" の行を group = \"kvm\" に変更する必要があります。\n\n"
    },
    {
      "title": "Windows 仮想マシンを起動したときに \"System Thread Exception Not Handled\"",
      "level": 3,
      "content": "Windows 8 や Windows 10 ゲストは起動時に \"System Thread Exception Not Handled\" という一般的な互換性例外を発生させることがあります。これは実機で奇妙な振る舞いをするレガシードライバ原因であることが多いようです。KVM マシンでは一般的に CPU モデルをcore2duo に設定することでこの問題を解決できます。\n\n"
    },
    {
      "title": "特定の Windows のゲームやアプリケーションでクラッシュやブルスクリーンが発生する",
      "level": 3,
      "content": "物理マシンでは問題なく動作するのに、仮想マシンで実行すると予期せずにクラッシュすることがあります。root で dmesg -wH を実行したときに MSR というエラーが発生した場合、クラッシュの原因はゲストがサポートされていない Model-specific registers (MSRs) にアクセスしようとすると、KVM が一般保護違反 (GPF) を起こすためです。これにより、ゲストアプリケーション/OS がクラッシュすることがよくあります。これらの問題の多くは、KVM モジュールに ignore_msrs=1 オプションを指定して実装されていない MSR を無視することで解決できます。\n\n```\n/etc/modprobe.d/kvm.conf\n```\n\n```\n...\noptions kvm ignore_msrs=1\n...\n```\n\n上記のオプションが役に立つのは以下のような場合です:\n\n- GeForce Experience でサポートされていない CPU が存在するとエラーが表示される。\n- StarCraft 2 や L.A.Noire で KMODE_EXCEPTION_NOT_HANDLED が発生して Windows 10 が確実にブルースクリーンになる。これらの場合、ブルースクリーン情報はドライバファイルを識別しません。\n\n"
    },
    {
      "title": "高い割り込みレイテンシとマイクロスタッタリング",
      "level": 3,
      "content": "この問題は小さな一時停止(カクつき)として現れ、特にゲームなどのグラフィックスを多用するアプリケーションで顕著になります。\n\n- 原因の1つは、CPU 周波数スケーリング によって制御される CPU の省電力機能です。すべてのプロセッサコアについて performance に変更してください。\n- もう1つの原因として、PS/2 入力が考えられます。PS/2 入力から Virtio 入力に切り替えてください。OVMF_による_PCI_パススルー#Evdev でキーボード・マウスを接続 を参照してください。\n\n"
    },
    {
      "title": "QXL ビデオの低解像度化",
      "level": 3,
      "content": "QEMU 4.1.0 では、QXL ビデオがスパイスで表示されると低解像度に戻るというリグレッションが発生しました。[6] たとえば、KMS が開始すると、テキストの解像度が 4x10 文字に低下することがあります。GUI の解像度を上げようとすると、サポートされている最低の解像度になることがあります。\n\n回避策として、次の形式でデバイスを作成してください:\n\n```\n-device qxl-vga,max_outputs=1...\n```\n\n"
    },
    {
      "title": "セキュアブート対応 OVMF を使用すると仮想マシンが起動しない",
      "level": 3,
      "content": "edk2-ovmf の OVMF_CODE.secboot.4m.fd と OVMF_CODE.secboot.fd ファイルは SMM サポート付きでビルドされています。仮想マシンで S3 サポートが無効になっていない場合、仮想マシンがまったく起動しない可能性があります。\n\nqemu コマンドに -global ICH9-LPC.disable_s3=1 オプションを追加してください。\n\nQEMU でセキュアブートを使用するために必要なオプションの詳細は FS#59465および https://github.com/tianocore/edk2/blob/master/OvmfPkg/README を参照してください。\n\n"
    },
    {
      "title": "仮想マシンが Arch ISO で起動しない",
      "level": 3,
      "content": "Arch ISO イメージから初めて仮想マシンを起動しようとすると、ブートプロセスがハングします。ブートメニューで e を押して console=ttyS0 をカーネルブートオプションに追加すると、さらに多くのブートメッセージと次のエラーが表示されます:\n\n```\n:: Mounting '/dev/disk/by-label/ARCH_202204' to '/run/archiso/bootmnt'\nWaiting 30 seconds for device /dev/disk/by-label/ARCH_202204 ...\nERROR: '/dev/disk/by-label/ARCH_202204' device did not show up after 30 seconds...\n   Falling back to interactive prompt\n   You can try to fix the problem manually, log out when you are finished\nsh: can't access tty; job control turned off\n```\n\nこのエラーメッセージは、実際の問題が何なのかを明確に示すものではありません。問題は、QEMU が仮想マシンに割り当てるデフォルトの 128MB の RAM にあります。-m 1024 で制限を 1024MB に増やすと問題が解決し、システムが起動します。その後、通常どおり Arch Linux のインストールを続けることができます。インストールが完了したら、仮想マシンへのメモリ割り当てを減らすことができます。1024MB が必要になるのは、RAM ディスクの要件とインストールメディアのサイズによるものです。 arch-releng メーリングリストのこのメッセージ とこのフォーラムのスレッド を参照してください。\n\n"
    },
    {
      "title": "ゲスト CPU の割り込みが発生しない",
      "level": 3,
      "content": "OSDev wiki に従って独自のオペレーティングシステムを作成している場合や、QEMU の gdb インターフェースで -s フラグを使用してゲストアーキテクチャアセンブリコードをステップ実行している場合、QEMU を含む多くのエミュレーターが、通常はいくつかの CPU 割り込みを実装し、多くのハードウェア割り込みは実装していないということを知っておくと役に立ちます。あなたのコードが割り込みを発生させているかどうかを知る1つの方法は、以下を使用して:\n\n```\n-d int\n```\n\n標準出力に割り込み/例外の表示を有効にすることです。\n\nQEMU が提供するその他のゲストデバッグ機能については、以下を参照してください:\n\n```\nqemu-system-x86_64 -d help\n```\n\nもしくは、x86_64 をあなたの選択したゲストアーキテクチャに置き換えてください。\n\n"
    },
    {
      "title": "sddm を使用した KDE でログイン時に spice-vdagent が自動的に起動しない",
      "level": 3,
      "content": "/etc/xdg/autostart/spice-vdagent.desktop から X-GNOME-Autostart-Phaseテンプレート:=WindowManager を削除するかコメントアウトしてください。 [7]\n\n"
    },
    {
      "title": "参照",
      "level": 2,
      "content": "- QEMU 公式ウェブサイト\n- KVM 公式ウェブサイト\n- QEMU エミュレータユーザドキュメント\n- QEMU Wikibook\n- Hardware virtualization with QEMU by AlienBOB (2008年最終更新)\n- Building a Virtual Army by Falconindy\n- QEMU documentation\n- QEMU on Windows\n- Wikipedia\n- Debian Wiki - QEMU\n- Networking QEMU Virtual BSD Systems\n- QEMU on gnu.org\n- QEMU on FreeBSD as host\n- Managing Virtual Machines with QEMU - openSUSE documentation\n- KVM on IBM Knowledge Center\n\n"
    }
  ]
}