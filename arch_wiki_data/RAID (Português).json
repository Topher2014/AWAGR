{
  "title": "RAID (Português)",
  "url": "https://wiki.archlinux.org/title/RAID_(Portugu%C3%AAs)",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "Artigos relacionados\n\n- Software RAID and LVM\n- LVM#RAID\n- Instalando Arch Linux com FakeRAID\n- Convert a single drive system to RAID\n- ZFS\n- ZFS/Virtual disks\n- Swap#Distribuição\n- Btrfs#RAID\n\nRedundant Array of Independent Disks (Conjunto Redundante de Discos Independentes, sigla em inglês RAID) é uma tecnologia de armazenamento que combina múltiplos componentes de unidade de disco (tipicamente unidades de disco ou partições da mesma) em uma unidade lógica. Dependendo da implementação do RAID, essa unidade lógica pode ser um sistema de arquivos ou uma camada transparente adicional que pode conter várias partições. Os dados são distribuídos pelas unidades de uma das várias maneiras chamadas níveis de RAID, dependendo do nível de redundância e desempenho necessário. O nível de RAID escolhido pode assim prevenir perda de dados na ocorrência de falha de disco rígido, aumentar performance ou ser uma combinação de ambos.\n\nEsse artigo explica como criar/gerenciar um software RAID array usando mdadm.\n\n"
    },
    {
      "title": "Níveis de RAID",
      "level": 2,
      "content": "Apesar da redundância implicada pela maioria dos níveis de RAID, RAID não garante que os dados estão seguros. Um RAID não vai proteger os dados se houver incêndio, se o computador for roubado ou se multiplos discos rígidos falharem de uma só vez. Além disso, instalar um sistema com RAID é um processo complexo que pode destruir dados.\n\n"
    },
    {
      "title": "Níveis padrão de RAID",
      "level": 3,
      "content": "Existem muitos níveis de RAID, veja a seguir os mais usados.\n\nNote: **esperada** \n\n"
    },
    {
      "title": "Comparação entre os níveis de RAID",
      "level": 3,
      "content": "Table content:\nNível RAID | redundância de dados | Utilização de unidade física | Performance de leitura | Performance de escrita | Min de unidades\n0 | Não | 100% | nX Melhor | nX Melhor | 2\n1 | Sim | 50% | Até nX se vários processos estiverem lendo, caso contrário, 1X | 1X | 2\n5 | Sim | 67% - 94% | (n−1)X Superior | (n−1)X Superior | 3\n6 | Sim | 50% - 88% | (n−2)X | (n−2)X | 4\n10,far2 | Sim | 50% | nX Melhor; em paridade com RAID0, mas redudante | (n/2)X | 2\n10,near2 | Sim | 50% | Até nX se vários processos estiverem lendo, caso contrário, 1X | (n/2)X | 2\n\nMelhor\n\nMelhor\n\nSuperior\n\nSuperior\n\nMelhor; em paridade com RAID0, mas redudante\n\n* Onde n representa o número de discos dedicados.\n\n"
    },
    {
      "title": "Implementação",
      "level": 2,
      "content": "Os dispositivos RAID podem ser gerenciados de diferentes maneiras:\n\n- uma camada de abstração (e.g. mdadm); Nota: Este é o método que usaremos posteriormente neste guia.\n- um gerenciador de volume lógico (e.g. LVM);\n- um componente de um sistema de arquivos (e.g. ZFS, Btrfs).\n\n"
    },
    {
      "title": "Que tipo de RAID eu possuo?",
      "level": 3,
      "content": "Como o RAID de software é implementado pelo usuário, o tipo de RAID é facilmente conhecido por ele.\n\nNo entanto, discernir entre o FakeRAID e o verdadeiro RAID de hardware pode ser mais difícil. Como mostrado, os fabricantes muitas vezes distinguem incorretamente esses dois tipos de RAID e a propaganda enganosa sempre é possível. A melhor solução neste caso é executar o comando lspci e olhar a saída para encontrar o controlador RAID. Em seguida, faça uma pesquisa para ver quais informações podem ser localizadas sobre o controlador RAID. Os controladores RAID de hardware aparecem nesta lista, mas as implementações FakeRAID não. Além disso, o verdadeiro controlador RAID de hardware costuma ser bastante caro, portanto, se alguém personalizou o sistema, é muito provável que a escolha de uma configuração de RAID de hardware tenha feito uma mudança muito perceptível no preço do computador.\n\n"
    },
    {
      "title": "Instalação",
      "level": 2,
      "content": "Instale mdadm. mdadm é usado para administrar RAID de software puro usando dispositivos de bloco simples: o hardware subjacente não fornece nenhuma lógica RAID, apenas um suprimento de discos. mdadm funcionará com qualquer coleção de dispositivos de bloco. Mesmo que incomum. Por exemplo, pode-se, portanto, fazer um array RAID a partir de uma coleção de pen drives.\n\n"
    },
    {
      "title": "Prepare os dispositivos",
      "level": 3,
      "content": "Se o dispositivo estiver sendo reutilizado ou reaproveitado de um array existente, apague todas as informações de configuração RAID antigas:\n\n```\n# mdadm --misc --zero-superblock /dev/<dispositivo>\n```\n\nou se uma partição específica em uma unidade deve ser excluída:\n\n```\n# mdadm --misc --zero-superblock /dev/<partição>\n```\n\n- Apagar o superbloco de uma partição não deve afetar as outras partições no disco.\n- Devido à natureza da funcionalidade RAID, é muito difícil limpar seguramente o disco totalmente em um array em execução. Considere se é útil fazer isso antes de criá-lo.\n\n"
    },
    {
      "title": "Particione os dispositivos",
      "level": 3,
      "content": "É altamente recomendado particionar os discos a serem usados no array. Como a maioria dos usuários de RAID está selecionando unidades de disco maiores que 2 TiB, o GPT é necessário e recomendado. Veja Particionamento para mais informações sobre particionamento e as ferramentas de particionamento disponíveis.\n\n"
    },
    {
      "title": "Tabela de Partição GUID",
      "level": 4,
      "content": "- Depois de criar as partições, seus GUIDs de tipo de partição devem ser A19D880F-05FC-4D3B-A006-743F0F84911E (pode ser atribuído selecionando o tipo de partição Linux RAID em fdisk ou FD00 no gdisk).\n- Se uma matriz de disco maior for empregada, considere atribuir rótulos do sistema de arquivos ou rótulos de partição para facilitar a identificação de um disco individual posteriormente.\n- Recomenda-se a criação de partições do mesmo tamanho em cada um dos dispositivos.\n\n"
    },
    {
      "title": "Master Boot Record",
      "level": 4,
      "content": "Para quem está criando partições em HDDs com uma tabela de partição MBR, os IDs de tipos de partição disponíveis para uso são:\n\n- 0xFD para arrays de detecção automática de raid (Linux raid autodetect em fdisk)\n- 0xDA para dados não-fs (Non-FS data em fdisk)\n\nVeja Linux Raid Wiki:Partition Types para mais informações.\n\n"
    },
    {
      "title": "Contrua o array",
      "level": 3,
      "content": "Use mdadm para construir o array. Veja mdadm(8) para opções suportadas. Vários exemplos são fornecidos abaixo.\n\n- Se este for um array RAID1 que deve ser inicializado a partir do Syslinux, uma limitação do syslinux v4.07 requer que o valor dos metadados seja 1.0 em vez do padrão 1.2.\n- Ao criar um array a partir do meio de instalação do Arch use a opção --homehost=myhostname (ou --homehost=any para ter sempre o mesmo nome, independentemente do host) para definir o hostname, caso contrário, o hostname archiso será escrito nos metadados do array.\n\nO exemplo a seguir mostra a construção de um array RAID1 de 2 dispositivos:\n\n```\n# mdadm --create --verbose --level=1 --metadata=1.2 --raid-devices=2 /dev/md/MeuArrayRAID1 /dev/sdb1 /dev/sdc1\n```\n\nO exemplo a seguir mostra a construção de uma matriz RAID5 com 4 dispositivos ativos e 1 dispositivo de reserva:\n\n```\n# mdadm --create --verbose --level=5 --metadata=1.2 --chunk=256 --raid-devices=4 /dev/md/MeuArrayRAID5 /dev/sdb1 /dev/sdc1 /dev/sdd1 /dev/sde1 --spare-devices=1 /dev/sdf1\n```\n\nO exemplo a seguir mostra a construção de um array RAID10,far2 com 2 dispositivos:\n\n```\n# mdadm --create --verbose --level=10 --metadata=1.2 --chunk=512 --raid-devices=2 --layout=f2 /dev/md/MeuArrayRAID10 /dev/sdb1 /dev/sdc1\n```\n\nO array é criado no dispositivo virtual /dev/mdX, montado e pronto para uso (em modo degradado). Pode-se começar a usá-lo diretamente enquanto o mdadm ressincroniza o array em segundo plano. Pode levar muito tempo para restaurar a paridade. Verifique o progresso com:\n\n```\n$ cat /proc/mdstat\n```\n\n"
    },
    {
      "title": "Atualize o arquivo de configuração",
      "level": 3,
      "content": "Por padrão, a maior parte de mdadm.conf é comentada e contém apenas o seguinte:\n\n```\n/etc/mdadm.conf\n```\n\n```\n...\nDEVICE partitions\n...\n```\n\nEsta diretiva diz ao mdadm para examinar os dispositivos referenciados por /proc/partitions e montar tantos arrays quanto possível. Isto é bom se você realmente deseja iniciar todos os arrays disponíveis e está confiante de que nenhum superbloco inesperado será encontrado (como depois de instalar um novo dispositivo de armazenamento). Uma abordagem mais precisa é adicionar explicitamente as matrizes em /etc/mdadm.conf:\n\n```\n# mdadm --detail --scan >> /etc/mdadm.conf\n```\n\nIsso resulta em algo como o seguinte:\n\n```\n/etc/mdadm.conf\n```\n\n```\n...\nDEVICE partitions\n...\nARRAY /dev/md/MeuArrayRAID1 metadata=1.2 name=pine:MeuArrayRAID1 UUID=27664f0d:111e493d:4d810213:9f291abe\n```\n\nIsso também faz com que o mdadm examine os dispositivos referenciados por /proc/partitions. No entanto, apenas dispositivos que possuem superblocos com um UUID de 27664… são montados em arrays ativos.\n\nVeja mdadm.conf(5) para mais informações.\n\n"
    },
    {
      "title": "Monte o array",
      "level": 3,
      "content": "Uma vez que o arquivo de configuração tenha sido atualizado, o array pode ser montado usando mdadm:\n\n```\n# mdadm --assemble --scan\n```\n\n"
    },
    {
      "title": "Formate o sistema de arquivos RAID",
      "level": 3,
      "content": "O array agora pode ser formatado com um sistema de arquivos como qualquer outra partição, apenas tenha em mente que:\n\n- Devido ao grande tamanho do volume, nem todos os sistemas de arquivos são adequados (veja: Wikipedia:Comparison of file systems#Limits).\n- O sistema de arquivos deve suportar crescimento e redução enquanto online (veja: Wikipedia:Comparison of file systems#Features).\n- Deve-se calcular o stride correto e a largura de stripe para um desempenho ideal.\n\n"
    },
    {
      "title": "Calculando o stride e a largura do stripe",
      "level": 4,
      "content": "Dois parâmetros são necessários para otimizar a estrutura do sistema de arquivos para se encaixar perfeitamente na estrutura RAID subjacente: o stride e a largura de stripe. Esses são derivados do tamanho do chunk do RAID, tamanho do bloco do sistema de arquivos, e o número de \"discos de dados\".\n\nO tamanho do chunk é uma propriedade do array RAID, decidida no momento de sua criação. O padrão atual do mdadm é 512 KiB. Pode ser encontrado com mdadm:\n\n```\n# mdadm --detail /dev/mdX | grep 'Chunk Size'\n```\n\nO tamanho do bloco é uma propriedade do sistema de arquivos, decidido na sua criação. O padrão para muitos sistemas de arquivos, incluindo ext4, é 4 KiB. Veja /etc/mke2fs.conf para detalhes sobre ext4.\n\nO número de \"discos de dados\" é o número mínimo de dispositivos no array necessários para reconstruí-lo completamente sem perda de dados. Por exemplo, este é N para um array raid0 de N dispositivos e N-1 para raid5.\n\nDepois de ter essas três quantidades, o stride e a largura do stripe podem ser calculadas usando as seguintes fórmulas:\n\n```\nstride = tamando do chunk / tamanho do bloco\nlargura do stripe = número dso discos de dados * stride\n```\n\nExemplo de formatação para ext4 com a largura de stripe e stride corretos:\n\n- O array RAID0 hipotético é composto por 2 discos físicos.\n- Tamanho do chunk é 512 KiB.\n- Tamanho do bloco é 4 KiB.\n\nstride = tamanho do chunk / tamanho do bloco. Neste exemplo, a matemática é 512/4, então o stride = 128.\n\nlargura do stripe = # de discos físicos de dados * stride. Neste exemplo, a matemática é 2*128, então a largura do stripe = 256.\n\n```\n# mkfs.ext4 -v -L meuarray -b 4096 -E stride=128,stripe-width=256 /dev/md0\n```\n\nExemplo de formatação para ext4 com a largura de stripe e stride corretos:\n\n- O array RAID5 hipotético é composto por 4 discos físicos; 3 discos de dados e 1 disco de paridade.\n- Tamanho do chunk é 512 KiB.\n- Tamanho do bloco é 4 KiB.\n\nstride = tamanho do chunk / tamanho do bloco. Neste exemplo, a matemática é 512/4, então o stride = 128.\n\nlargura do stripe = # de discos físicos de dados * stride. Neste exemplo, a matemática é 3*128, então a largura do stripe = 384.\n\n```\n# mkfs.ext4 -v -L meuarray -b 4096 -E stride=128,stripe-width=384 /dev/md0\n```\n\nPara mais sobre stride e largura de stripe, veja: RAID Math.\n\nExemplo de formatação para ext4 com a largura de stripe e stride corretos:\n\n- O array RAID10 hipotético é composto por 2 discos físicos. Por causa das propriedades do RAID10 no layout far2, ambos contam como discos de dados.\n- Tamanho do chunk é 512 KiB.\n- Tamanho do bloco é 4 KiB.\n\nstride = tamanho do chunk / tamanho do bloco. Neste exemplo, a matemática é 512/4, então o stride = 128.\n\nlargura do stripe = # de discos físicos de dados * stride. Neste exemplo, a matemática é 2*128 então a largura do stripe = 256.\n\n```\n# mkfs.ext4 -v -L meuarray -b 4096 -E stride=128,stripe-width=256 /dev/md0\n```\n\n"
    },
    {
      "title": "Montando a partir de um Live CD",
      "level": 2,
      "content": "Os usuários que desejam montar a partição RAID a partir de um Live CD, utilize:\n\n```\n# mdadm --assemble /dev/md<número> /dev/<disco1> /dev/<disco2> /dev/<disco3> /dev/<disco4>\n```\n\nSe o seu RAID 1 sem uma matriz de disco foi erroneamente detectado automaticamente como RAID 1 (conforme mdadm --detail /dev/md<número>) e reportado como inativo (conforme cat /proc/mdstat), pare o array primeiro:\n\n```\n# mdadm --stop /dev/md<número>\n```\n\n"
    },
    {
      "title": "Instalando Arch Linux em RAID",
      "level": 2,
      "content": "Você deve criar a matriz RAID entre as estapas de particionamento e formatação do processo de instalação. Em vez de formatar diretamente uma partição para ser seu sistema de arquivos raiz, ela será criada em um array RAID. Siga a seção #Instalação para criar o array RAID. Em seguida, continue com o procedimento de instalação até que a etapa pacstrap seja concluída. Ao usar UEFI boot, também leia ESP no RAID.\n\n"
    },
    {
      "title": "Atualize o arquivo de configuração",
      "level": 3,
      "content": "Depois que o sistema básico é instalado, o arquivo de configuração padrão, mdadm.conf, deve ser atualizado assim:\n\n```\n# mdadm --detail --scan >> /mnt/etc/mdadm.conf\n```\n\nSempre verifique o arquivo de configuração mdadm.conf usando um editor de texto depois de executar este comando para garantir que seu conteúdo pareça razoável.\n\nContinue com o procedimento de instalação até chegar na etapa Guia de instalação#Initramfs, então siga a próxima seção.\n\n"
    },
    {
      "title": "Configure mkinitcpio",
      "level": 3,
      "content": "Adicione mdadm_udev à seção HOOKS do mkinitcpio.conf para adicionar suporte para mdadm na imagem initramfs:\n\n```\n/etc/mkinitcpio.conf\n```\n\n```\n...\n HOOKS=(base udev autodetect keyboard modconf block mdadm_udev filesystems fsck)\n...\n```\n\nSe você usar o hook mdadm_udev com um array FakeRAID, é recomendado incluir mdmon no vetor BINARIES:\n\n```\n/etc/mkinitcpio.conf\n```\n\n```\n...\nBINARIES=(mdmon)\n...\n```\n\nEm seguida, gere o intiramfs novamente.\n\nVeja também Mkinitcpio (Português)#Usando RAID.\n\n"
    },
    {
      "title": "Dispositivo raiz",
      "level": 4,
      "content": "Aponte o parâmetro root para o dispositivo mapeado. E.g.:\n\n```\nroot=/dev/md/MeuArrayRAID\n```\n\nSe a inicialização a partir de uma partição raid de software falhar usando o método de nó de dispositivo de kernel acima, uma maneira alternativa é usar um dos métodos de nomeação de dispositivo de bloco persistente, por exemplo:\n\n```\nroot=LABEL=Root_Label\n```\n\nVeja também GRUB (Português)#RAID.\n\n"
    },
    {
      "title": "Layout RAID0",
      "level": 4,
      "content": "Desde a versão 5.3.4 do kernel Linux, você precisa informar explicitamente ao kernel qual layout RAID0 deve ser usado: RAID0_ORIG_LAYOUT (1) ou RAID0_ALT_MULTIZONE_LAYOUT (2).[1] Você pode fazer isso fornecendo o parâmetro do kernel conforme a seguir:\n\n```\nraid0.default_layout=2\n```\n\nO valor correto depende da versão do kernel que foi usada para criar o array raid: use 1 se criado usando kernel 3.14 ou anterior, use 2 se estiver usando uma versão mais recente do kernel . Uma maneira de verificar isso é observar o tempo de criação da matriz raid:\n\n```\nmdadm --detail /dev/md1\n```\n\n```\n/dev/md1:\n           Version : 1.2\n     Creation Time : Thu Sep 24 10:17:41 2015\n        Raid Level : raid0\n        Array Size : 975859712 (930.65 GiB 999.28 GB)\n      Raid Devices : 3\n     Total Devices : 3\n       Persistence : Superblock is persistent\n\n       Update Time : Thu Sep 24 10:17:41 2015\n             State : clean\n    Active Devices : 3\n   Working Devices : 3\n    Failed Devices : 0\n     Spare Devices : 0\n\n        Chunk Size : 512K\n\nConsistency Policy : none\n\n              Name : archiso:root\n              UUID : 028de718:20a81234:4db79a2c:e94fd560\n            Events : 0\n\n    Number   Major   Minor   RaidDevice State\n       0     259        2        0      active sync   /dev/nvme0n1p1\n       1     259        6        1      active sync   /dev/nvme2n1p1\n       2     259        5        2      active sync   /dev/nvme1n1p2\n```\n\nAqui podemos ver que este array raid foi criado em 24 de setembro de 2015. A data de lançamento do Linux Kernel 3.14 foi 30 de março de 2014 e, como tal, este array raid é provavelmente criado usando um layout de várias zonas (2).\n\n"
    },
    {
      "title": "Scrubbing",
      "level": 3,
      "content": "É uma boa prática executar regularmente um scrubbing de dados para verificar e corrigir erros. Dependendo do tamanho/configuração do array, um scrub pode levar várias horas para ser concluído.\n\nPara iniciar um scrub de dados:\n\n```\n# echo check > /sys/block/md0/md/sync_action\n```\n\nA operação check verifica as unidades em busca de setores defeituosos e os repara automaticamente. Se encontrar setores bons que contêm dados ruins (os dados de um setor não correspondem com o que os dados de outro disco indicam que deveria ser, por exemplo o bloco de paridade + os outros blocos de dados nos faria pensar que este bloco de dados está incorreto), nenhuma ação é realizada, mas o evento é registrado em um log (veja abaixo). Este \"não fazer nada\" permite que os administradores inspecionem os dados no setor e os dados que seriam produzidos pela reconstrução dos setores a partir de informações redundantes e escolham os dados corretos para manter.\n\nTal como acontece com muitas tarefas/itens relacionados ao mdadm, o status do scrub pode ser consultado lendo /proc/mdstat.\n\nExemplo:\n\n```\n$ cat /proc/mdstat\n```\n\n```\nPersonalities : [raid6] [raid5] [raid4] [raid1]\nmd0 : active raid1 sdb1[0] sdc1[1]\n      3906778112 blocks super 1.2 [2/2] [UU]\n      [>....................]  check =  4.0% (158288320/3906778112) finish=386.5min speed=161604K/sec\n      bitmap: 0/30 pages [0KB], 65536KB chunk\n```\n\nPara interromper um scrub de dados em execução com segurança:\n\n```\n# echo idle > /sys/block/md0/md/sync_action\n```\n\nQuando a limpeza é concluída, os administradores podem verificar quantos blocos (se houver) foram sinalizados como ruins:\n\n```\n# cat /sys/block/md0/md/mismatch_cnt\n```\n\n"
    },
    {
      "title": "Notas gerais sobre scrubbing",
      "level": 4,
      "content": "Note: **repair** \n\nÉ uma boa ideia configurar um cron job como root para agendar um scrub periódico. Veja raid-checkAUR que pode ajudar com isso. Para realizar um scrub periódico usando cronômetros systemd em vez de cron, consulte raid-check-systemdAUR que contém o mesmo script junto com arquivos de unidade de cronômetro systemd associados.\n\nNote: **seis segundos por gigabyte** \n\n"
    },
    {
      "title": "Notas sobre scrubbing de RAID1 e RAID10",
      "level": 4,
      "content": "Devido ao fato de que as gravações RAID1 e RAID10 no kernel não são bufferizadas, um array pode ter contagens de incompatibilidade diferente de 0, mesmo quando o array está saudável. Essas contagens diferentes de 0 só existirão em áreas de dados transitórios onde não representam um problema. No entanto, não podemos dizer a diferença entre uma contagem diferente de 0 que está apenas em dados transitórios ou uma contagem diferente de 0 que significa um problema real. Este fato é uma fonte de falsos positivos para arrays RAID1 e RAID10. No entanto, ainda é recomendado realizar scrub regularmente a fim de detectar e corrigir quaisquer setores defeituosos que possam estar presentes nos dispositivos.\n\n"
    },
    {
      "title": "Removendo dispositivos de um array",
      "level": 3,
      "content": "Pode-se remover um dispositivo do array após marcá-lo como defeituoso:\n\n```\n# mdadm --fail /dev/md0 /dev/sdxx\n```\n\nAgora remova-o do array:\n\n```\n# mdadm --remove /dev/md0 /dev/sdxx\n```\n\nRemova o dispositivo permanentemente (por exemplo, para usá-lo individualmente a partir de agora): Emita os dois comandos descritos acima, então:\n\n```\n# mdadm --zero-superblock /dev/sdxx\n```\n\n- Não emita este comando em arrays lineares ou RAID0 ou ocorrerá perda de dados!\n- Reutilizar o disco removido sem zerar o superbloco causará perda de todos os dados na próxima inicialização. (Depois mdadm tentará usá-lo como parte do array raid).\n\nPare de usar um array:\n\n1. Desmonte o array alvo\n1. Pare o array com: mdadm --stop /dev/md0\n1. Repita os três comandos descritos no início desta seção em cada dispositivo.\n1. Remova a linha correspondente de /etc/mdadm.conf.\n\n"
    },
    {
      "title": "Adicionando um novo dispositivo a um array",
      "level": 3,
      "content": "Adicionar novos dispositivos com mdadm pode ser feito em um sistema em execução com os dispositivos montados. Particione o novo dispositivo usando o mesmo layout de um dos que já estão nos arrays, conforme discutido acima.\n\nMonte o array RAID caso ainda não esteja montado:\n\n```\n# mdadm --assemble /dev/md0 /dev/sda1 /dev/sdb1\n```\n\nAdicione o novo dispositivo ao array:\n\n```\n# mdadm --add /dev/md0 /dev/sdc1\n```\n\nIsso não deve demorar muito para o mdadm fazer.\n\nDependendo do tipo de RAID (por exemplo, com RAID1), o mdadm pode adicionar o dispositivo como extra sem sincronizar dados com ele. Você pode aumentar o número de discos que o RAID usa usando --grow com a opção --raid-devices. Por exemplo, para aumentar um array para quatro discos:\n\n```\n# mdadm --grow /dev/md0 --raid-devices=4\n```\n\nVocê pode verificar o progresso com:\n\n```\n# cat /proc/mdstat\n```\n\nVerifique se o dispositivo foi adicionado com o comando:\n\n```\n# mdadm --misc --detail /dev/md0\n```\n\nNote: Isso ocorre porque os comandos acima irão adicionar o novo disco como um \"sobressalente\", mas o RAID0 não possui sobressalentes. Se você deseja adicionar um dispositivo a uma matriz RAID0, você precisa \"crescer\" e \"adicionar\" no mesmo comando, conforme demonstrado abaixo:\n\n```\nmdadm: add new device failed for /dev/sdc1 as 2: Invalid argument\n```\n\nIsso ocorre porque os comandos acima irão adicionar o novo disco como um \"sobressalente\", mas o RAID0 não possui sobressalentes. Se você deseja adicionar um dispositivo a uma matriz RAID0, você precisa \"crescer\" e \"adicionar\" no mesmo comando, conforme demonstrado abaixo:\n\n```\n# mdadm --grow /dev/md0 --raid-devices=3 --add /dev/sdc1\n```\n\n"
    },
    {
      "title": "Aumentando o tamanho de um volume RAID",
      "level": 3,
      "content": "Se discos maiores forem instalados em um array RAID ou o tamanho da partição tiver aumentado, pode ser desejável aumentar o tamanho do volume RAID para preencher o maior espaço disponível. Este processo pode ser iniciado seguindo primeiro as seções acima relacionadas à substituição de discos. Uma vez que o volume RAID tenha sido reconstruído em discos maiores, ele deve ser \"aumentado\" para preencher o espaço.\n\n```\n# mdadm --grow /dev/md0 --size=max\n```\n\nEm seguida, as partições presentes no volume RAID /dev/md0 pode precisar ser redimensionadas. Veja Particionamento para mais detalhes. Finalmente, o sistema de arquivos na partição mencionada acima precisará ser redimensionado. Se o particionamento foi executado com gparted isso será feito automaticamente. Se outras ferramentas foram usadas, desmonte e redimensione o sistema de arquivos manualmente.\n\n```\n# umount /storage\n# fsck.ext4 -f /dev/md0p1\n# resize2fs /dev/md0p1\n```\n\n"
    },
    {
      "title": "Alterar os limites de velocidade de sincronização",
      "level": 3,
      "content": "Sincronização pode demorar um pouco. Se a máquina não for necessária para outras tarefas, o limite de velocidade pode ser aumentado.\n\n```\n# cat /proc/mdstat\n```\n\n```\nPersonalities : [raid1]\n md0 : active raid1 sda3[2] sdb3[1]\n       155042219 blocks super 1.2 [2/1] [_U]\n       [>....................]  recovery =  0.0% (77696/155042219) finish=265.8min speed=9712K/sec\n\n unused devices: <none>\n```\n\nVerifique o limite de velocidade atual.\n\n```\n# cat /proc/sys/dev/raid/speed_limit_min\n```\n\n```\n1000\n```\n\n```\n# cat /proc/sys/dev/raid/speed_limit_max\n```\n\n```\n200000\n```\n\nAumente os limites.\n\n```\n# echo 400000 >/proc/sys/dev/raid/speed_limit_min\n# echo 400000 >/proc/sys/dev/raid/speed_limit_max\n```\n\nEm seguida, verifique a velocidade de sincronização e o tempo estimado de término.\n\n```\n# cat /proc/mdstat\n```\n\n```\nPersonalities : [raid1]\n md0 : active raid1 sda3[2] sdb3[1]\n       155042219 blocks super 1.2 [2/1] [_U]\n       [>....................]  recovery =  1.3% (2136640/155042219) finish=158.2min speed=16102K/sec\n\n unused devices: <none>\n```\n\nVeja também sysctl#MDADM.\n\n"
    },
    {
      "title": "Performance do RAID5",
      "level": 3,
      "content": "Para melhorar o desempenho do RAID5 para armazenamento rápido (e.g. NVMe), aumente /sys/block/mdx/md/group_thread_cnt para mais threads. Por exemplo, para usar 8 threads para operar em um dispositivo RAID5:\n\n```\n# echo 8 > /sys/block/md0/md/group_thread_cnt\n```\n\nVeja git kernel commit 851c30c9badf.\n\n"
    },
    {
      "title": "Atualizar o superbloco RAID",
      "level": 3,
      "content": "Para atualizar o superbloco RAID, você precisa primeiro desmontar o array e depois parar o array com o seguinte comando:\n\n```\n# mdadm --stop /dev/md0\n```\n\nEntão você pode atualizar certos parâmetros remontando o matriz. Por exemplo, para atualizar o homehost:\n\n```\n# mdadm --assemble --update=homehost --homehost=NAS /dev/md0 /dev/sda1 /dev/sdb1\n```\n\nVeja os argumentos de --update para detalhes.\n\n"
    },
    {
      "title": "Monitoramento",
      "level": 2,
      "content": "Uma linha simples que imprime o status dos dispositivos RAID:\n\n```\n# awk '/^md/ {printf \"%s: \", $1}; /blocks/ {print $NF}' </proc/mdstat\n```\n\n```\nmd1: [UU]\nmd0: [UU]\n```\n\n"
    },
    {
      "title": "Observe o status com mdstat",
      "level": 3,
      "content": "```\n# watch -t 'cat /proc/mdstat'\n```\n\nOu de preferência usando tmux\n\n```\n# tmux split-window -l 12 \"watch -t 'cat /proc/mdstat'\"\n```\n\n"
    },
    {
      "title": "Rastrear ES com iotop",
      "level": 3,
      "content": "O pacote iotop exibe as estatísticas de entrada/saída para processos. Use este comando para ver a ES para threads de raid.\n\n```\n# iotop -a $(sed 's/^/-p /g' <<<`pgrep \"_raid|_resync|jbd2\"`)\n```\n\n"
    },
    {
      "title": "Rastrear ES com iostat",
      "level": 3,
      "content": "O utilitário iostat do pacote sysstat exibe as estatísticas de entrada/saída para dispositivos e partições.\n\n```\n# iostat -dmy 1 /dev/md0\n# iostat -dmy 1 # all\n```\n\n"
    },
    {
      "title": "Notificações de email",
      "level": 3,
      "content": "mdadm fornece o serviço systemd mdmonitor.service que pode ser útil para monitorar a integridade de seus arrays raid e notificar por e-mail se algo der errado.\n\nEste serviço é especial porque não pode ser ativado manualmente como um serviço normal; mdadm cuidará de ativá-lo via udev ao montar seus arrays na inicialização do sistema, mas apenas fará isso se um endereço de e-mail tiver sido configurado para suas notificações (veja abaixo).\n\nPara habilitar esta funcionalidade, edite /etc/mdadm.conf e defina o endereço de e-mail:\n\n```\nMAILADDR usuário@domínio\n```\n\nEntão, para verificar se tudo está funcionando como deveria, execute o seguinte comando:\n\n```\n# mdadm --monitor --scan --oneshot --test\n```\n\nSe o teste for bem-sucedido e o e-mail for entregue, você está pronto; da próxima vez que seus arrays forem remontados, mdmonitor.service começará a monitorá-los em busca de erros.\n\n"
    },
    {
      "title": "Solução de problemas",
      "level": 2,
      "content": "Se você estiver recebendo um erro ao reinicializar sobre \"invalid raid superblock magic\" e tiver discos rígidos adicionais além dos que você instalou, verifique se a ordem do seu disco rígido está correta. Durante a instalação, seus dispositivos RAID podem ser hdd, hde e hdf, mas durante a inicialização eles podem ser hda, hdb e hdc. Ajuste sua linha de kernel de acordo.\n\n"
    },
    {
      "title": "Error: \"kernel: ataX.00: revalidation failed\"",
      "level": 3,
      "content": "Se você repentinamente (após reiniciar, alterou as configurações do BIOS) receber mensagens de erro como:\n\n```\nFeb  9 08:15:46 hostserver kernel: ata8.00: revalidation failed (errno=-5)\n```\n\nIsso não significa necessariamente que uma unidade está quebrada. Frequentemente, você encontra links de pânico na web que apontam o pior. Em poucas palavras, sem pânico. Talvez você tenha alterado as configurações de APIC ou ACPI dentro de seus parâmetros de BIOS ou Kernel de alguma forma. Troque-os de volta e deverá voltar a funcionar bem. Normalmente, desligar ACPI e/ou ACPI deve ajudar.\n\n"
    },
    {
      "title": "Iniciar arrays em somente leitura",
      "level": 3,
      "content": "Quando um array md é iniciado, o superbloco será escrito e a ressincronização pode começar. Para iniciar somente leitura, defina o módulo do kernel md_mod com o parâmetro start_ro. Quando isso é definido, os novos arrays obtêm um modo 'auto-ro', que desativa todos as ES internas (atualizações de superbloco, ressincronização, recuperação) e é automaticamente alterado para 'rw' quando a primeira solicitação de gravação chega.\n\nPara definir o parâmetro na inicialização, adicione md_mod.start_ro=1 à sua linha de kernel.\n\nOu defina-o no tempo de carregamento do módulo no arquivo /etc/modprobe.d/ ou diretamente em /sys/:\n\n```\n# echo 1 > /sys/module/md_mod/parameters/start_ro\n```\n\n"
    },
    {
      "title": "Recuperando de uma unidade quebrada ou perdida no raid",
      "level": 3,
      "content": "Você pode obter o erro mencionado acima também quando uma das unidades quebrar por qualquer motivo. Nesse caso, você terá que forçar o raid a ainda ligar, mesmo com um disco curto. Digite isto (mude onde necessário):\n\n```\n# mdadm --manage /dev/md0 --run\n```\n\nAgora você deve ser capaz de montá-lo novamente com algo assim (se você o tinha no fstab):\n\n```\n# mount /dev/md0\n```\n\nAgora o raid deve estar funcionando novamente e disponível para uso, porém com um disco a menos! Portanto, para adicionar essa partição de um disco da maneira descrita acima em #Prepare os dispositivos. Depois de fazer isso, você pode adicionar o novo disco ao raid fazendo:\n\n```\n# mdadm --manage --add /dev/md0 /dev/sdd1\n```\n\nSe você digitar:\n\n```\n# cat /proc/mdstat\n```\n\nvocê provavelmente verá que o raid agora está ativo e reconstruindo.\n\nVocê também pode querer atualizar suas configurações (veja: #Atualize o arquivo de configuração).\n\n"
    },
    {
      "title": "Benchmarking",
      "level": 2,
      "content": "Existem várias ferramentas para fazer benchmarking em um RAID. A melhoria mais notável é o aumento da velocidade quando várias threads estão lendo do mesmo volume RAID.\n\nbonnie++ testa o analisa o tipo de acesso ao banco de dados para um ou mais arquivos e criação, leitura e exclusão de pequenos arquivos que podem simular o uso de programas como Squid, INN ou formato Maildir e-mail. O programa anexo ZCAV testa o desempenho de diferentes zonas de um disco rígido sem gravar nenhum dado no disco.\n\nhdparm não deve ser usado para fazer benchmarking em um RAID, porque fornece resultados muito inconsistentes.\n\n"
    },
    {
      "title": "Veja também",
      "level": 2,
      "content": "- Linux Software RAID (thomas-krenn.com)\n- Linux RAID wiki entry[link inativo 2024-10-12 ⓘ] on The Linux Kernel Archives\n- How Bitmaps Work\n- Chapter 15: Redundant Array of Independent Disks (RAID)[link inativo 2024-07-30 ⓘ] of Red Hat Enterprise Linux 6 Documentation\n- Linux-RAID FAQ on the Linux Documentation Project\n- BAARF(Archive.org) including Why should I not use RAID 5?(Archive.org) by Art S. Kagel\n- Introduction to RAID, Nested-RAID: RAID-5 and RAID-6 Based Configurations, Intro to Nested-RAID: RAID-01 and RAID-10, and Nested-RAID: The Triple Lindy in Linux Magazine\n- HowTo: Speed Up Linux Software Raid Building And Re-syncing\n- Wikipedia:Non-RAID drive architectures\n\nLista de discução\n\n- Kernel Linux-Raid mailing list\n\nmdadm\n\n- mdadm source code\n- Software RAID on Linux with mdadm in Linux Magazine\n- Wikipedia - mdadm\n\nTópicos de fórum\n\n- Raid Performance Improvements with bitmaps\n- GRUB and GRUB2\n- Can't install grub2 on software RAID\n- Use RAID metadata 1.2 in boot and root partition\n\n"
    }
  ]
}