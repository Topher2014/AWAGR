{
  "title": "ZFS (日本語)",
  "url": "https://wiki.archlinux.org/title/ZFS_(%E6%97%A5%E6%9C%AC%E8%AA%9E)",
  "sections": [
    {
      "title": "Introduction",
      "level": 1,
      "content": "関連記事\n\n- ファイルシステム\n- ZFS/仮想ディスク\n- ZFS に Arch Linux をインストール\n\nZFS は (現在は Oracle によって吸収合併された) Sun Microsystems によって作成された先進的なファイルシステムで、2005年11月に OpenSolaris でリリースされました。ZFS には以下の機能があります: ストレージプール (統合ボリューム管理 -- zpool), Copy-on-write, スナップショット, データ整合性のチェックと自動修復 (スクラブ), RAID-Z, 最大16エクサバイトのファイルサイズ、最大256ゼタバイトのボリュームサイズ。ファイルシステム (データセット) やファイルの数は無制限です [1]。ZFS は Common Development and Distribution License (CDDL) でライセンスされています。\n\n\"The last word in filesystems\" では ZFS は安定していて高速、セキュアで、そして将来性を考えた設計をしていると述べられています。ライセンスが CDDL で、GPL と互換がないため、ZFS は Linux カーネルと一緒に配布することができません。しかしながら、サードパーティによってネイティブの Linux カーネルモジュールを開発・配布することは可能です。それが ZFSonLinux (ZOL) になります。\n\nZOL はローレンス・リバモア国立研究所による後押しを受けているプロジェクトで、その大規模なストレージ要件とスーパーコンピュータに合うようなネイティブの Linux カーネルモジュールを開発することを目標としています。\n\n- ZFS は Arch User Repository に含まれており非公式の archzfs リポジトリから使うこともできます。\n- ZFSonLinux プロジェクトは最新の Linux カーネルに追従しています。ZFSonLinux の安定版がリリースされた後に Arch の ZFS メンテナも同バージョンをリリースします。\n- 新しいバージョンのカーネルがリリースされてから ZFSonLinux によってサポートされるまでタイムラグがあるため、場合によっては通常のローリングアップデートができなくなる可能性があります。\n\n"
    },
    {
      "title": "目次",
      "level": 2,
      "content": "- 1 インストール 1.1 一般 1.2 Root on ZFS 1.3 DKMS\n- 2 ZFS の実験\n- 3 設定 3.1 自動起動 3.1.1 zfs-mount.service の使用 3.1.2 zfs-mount-generator の使用\n- 4 ストレージプールの作成 4.1 Advanced Format ディスク 4.2 プールの作成の確認 4.3 GRUB 互換のプールを作成する\n- 5 チューニング 5.1 一般 5.2 SSD キャッシュ 5.3 データベース 5.4 /tmp 5.5 zvol 5.5.1 RAIDZ と Advanced Format 物理ディスク\n- 6 使用方法 6.1 ネイティブ暗号化 6.2 スクラブ 6.3 zfs プールの状態を確認 6.4 ストレージプールを破壊 6.5 ストレージプールをエクスポート 6.6 Zpool の名前を変更 6.7 別のマウントポイントを設定 6.8 アクセス制御リスト 6.9 スワップボリューム 6.10 自動スナップショット 6.10.1 Linux の ZFS 自動スナップショットサービス 6.10.2 ZFS スナップショットマネージャ\n- 7 トラブルシューティング 7.1 ZPool の作成が失敗する 7.2 ZFS の使用 RAM が多すぎる 7.3 Does not contain an EFI label 7.4 No hostid found 7.5 SAS/SCSI デバイスからの起動中にプールが見つかりません 7.6 起動時に zfs プールがマウントされない: \"pool may be in use from other system\" 7.6.1 プールがエクスポートされていない 7.6.2 hostid が間違っている 7.7 デバイスのセクタアライメントが異なっている 7.8 プールの再同期が停止/再起動/遅いですか? 7.9 initramfs zpool.cache 内の使用できないプールのインポートの失敗によって引き起こされる起動の遅さを修正しました。 7.10 ZFS コマンド履歴\n- 8 ヒントとテクニック 8.1 archzfs パッケージを archiso に埋め込む 8.2 ZFS on Linux で暗号化 8.3 archzfs による chroot の応急処置 8.4 バインドマウント 8.4.1 fstab 8.5 モニタリングやイベント時にメール\n- 9 参照\n\n- 1.1 一般\n- 1.2 Root on ZFS\n- 1.3 DKMS\n\n- 3.1 自動起動 3.1.1 zfs-mount.service の使用 3.1.2 zfs-mount-generator の使用\n\n- 3.1.1 zfs-mount.service の使用\n- 3.1.2 zfs-mount-generator の使用\n\n- 4.1 Advanced Format ディスク\n- 4.2 プールの作成の確認\n- 4.3 GRUB 互換のプールを作成する\n\n- 5.1 一般\n- 5.2 SSD キャッシュ\n- 5.3 データベース\n- 5.4 /tmp\n- 5.5 zvol 5.5.1 RAIDZ と Advanced Format 物理ディスク\n\n- 5.5.1 RAIDZ と Advanced Format 物理ディスク\n\n- 6.1 ネイティブ暗号化\n- 6.2 スクラブ\n- 6.3 zfs プールの状態を確認\n- 6.4 ストレージプールを破壊\n- 6.5 ストレージプールをエクスポート\n- 6.6 Zpool の名前を変更\n- 6.7 別のマウントポイントを設定\n- 6.8 アクセス制御リスト\n- 6.9 スワップボリューム\n- 6.10 自動スナップショット 6.10.1 Linux の ZFS 自動スナップショットサービス 6.10.2 ZFS スナップショットマネージャ\n\n- 6.10.1 Linux の ZFS 自動スナップショットサービス\n- 6.10.2 ZFS スナップショットマネージャ\n\n- 7.1 ZPool の作成が失敗する\n- 7.2 ZFS の使用 RAM が多すぎる\n- 7.3 Does not contain an EFI label\n- 7.4 No hostid found\n- 7.5 SAS/SCSI デバイスからの起動中にプールが見つかりません\n- 7.6 起動時に zfs プールがマウントされない: \"pool may be in use from other system\" 7.6.1 プールがエクスポートされていない 7.6.2 hostid が間違っている\n- 7.7 デバイスのセクタアライメントが異なっている\n- 7.8 プールの再同期が停止/再起動/遅いですか?\n- 7.9 initramfs zpool.cache 内の使用できないプールのインポートの失敗によって引き起こされる起動の遅さを修正しました。\n- 7.10 ZFS コマンド履歴\n\n- 7.6.1 プールがエクスポートされていない\n- 7.6.2 hostid が間違っている\n\n- 8.1 archzfs パッケージを archiso に埋め込む\n- 8.2 ZFS on Linux で暗号化\n- 8.3 archzfs による chroot の応急処置\n- 8.4 バインドマウント 8.4.1 fstab\n- 8.5 モニタリングやイベント時にメール\n\n- 8.4.1 fstab\n\n"
    },
    {
      "title": "一般",
      "level": 3,
      "content": "Arch User Repository または archzfs リポジトリから以下のパッケージのどれかをインストールしてください:\n\n- zfs-linuxAUR - 安定版のリリース [3]。\n- zfs-linux-gitAUR - 開発版のリリース (新しいバージョンのカーネルをサポート) [4]。\n- zfs-linux-ltsAUR - LTS カーネル用の安定版リリース。\n- zfs-linux-lts-gitAUR - LTS カーネル用の 開発版 リリース\n- zfs-linux-hardenedAUR - hardened カーネル用の安定版リリース。\n- zfs-linux-hardened-gitAUR - hardened カーネル用の 開発版 リリース。\n- zfs-linux-zenAUR - zen カーネル用の安定版リリース。\n- zfs-linux-zen-gitAUR - zen カーネル用の 開発版 リリース。\n- zfs-dkmsAUR - ダイナミックカーネルモジュールをサポートしているバージョン。\n- zfs-dkms-gitAUR - ダイナミックカーネルモジュールをサポートしている 開発版 リリース。\n\n上記のパッケージにはそれぞれ zfs-utils, spl, spl-utils パッケージに依存しています。SPL (Solaris Porting Layer) は ZFS を使用するために Solaris の API を実装する Linux カーネルモジュールです。\n\nパッケージをインストールしたらコマンドラインで zpool status を実行してテストしてください。\"insmod\" エラーが表示される場合、depmod -a を試してみてください。\n\n"
    },
    {
      "title": "Root on ZFS",
      "level": 3,
      "content": "こちらを参照 ZFS に Arch Linux をインストール#インストール\n\n"
    },
    {
      "title": "DKMS",
      "level": 3,
      "content": "ユーザーは DKMS を利用して、カーネルのアップグレードごとに ZFS モジュールを自動的に再構築できます。\n\nzfs-dkmsAUR または zfs-dkms-gitAUR をインストールします。\n\n"
    },
    {
      "title": "ZFS の実験",
      "level": 2,
      "content": "~/zfs0.img ~/zfs1.img ~/zfs2.img などのシンプルなファイルの仮想ブロックデバイス (ZFS では VDEV と呼称) を使って、データを消失する危険性なく、ZFS の実験を行いたいユーザーは ZFS/仮想ディスクの記事を見て下さい。RAIDZ アレイの作成や、意図的にデータを破損させてそれを復元したり、データセットのスナップショットなどの一般的な作業を取り扱っています。\n\n"
    },
    {
      "title": "設定",
      "level": 2,
      "content": "ZFS は、その作成者によって \"管理不要\" のファイルシステムとみなされています。したがって、ZFS の構成は非常に簡単です。設定は主に、zfs と zpool の 2 つのコマンドで行われます。\n\n"
    },
    {
      "title": "自動起動",
      "level": 3,
      "content": "ZFS を\"ゼロアドミニストレーション\"の名に負うように使うには、zfs デーモンを起動時にロードする必要があります。このため /etc/fstab で zpool をマウントする必要はありません。zfs デーモンが自動的に zfs プールをインポートしてマウントを行います。デーモンは /etc/zfs/zpool.cache ファイルを読み込んで zfs プールをマウントします。\n\nzfs デーモンによって自動でマウントしたいプール毎に次を実行:\n\n```\n# zpool set cachefile=/etc/zfs/zpool.cache <pool>\n```\n\n関連するサービスを 有効化 (zfs-import-cache.service) とターゲット (zfs.target および zfs-import.target) プールは起動時に自動的にインポートされます。\n\nZFS ファイルシステムをマウントするには、次の 2 つの選択肢があります。\n\n- zfs-mount.service を有効にします\n- zfs-mount-generator を使用する\n\n"
    },
    {
      "title": "zfs-mount.service の使用",
      "level": 4,
      "content": "起動時に ZFS ファイルシステムを自動的にマウントするには、zfs-mount.service と zfs.target を 有効化 する必要があります。\n\n"
    },
    {
      "title": "zfs-mount-generator の使用",
      "level": 4,
      "content": "zfs-mount-generator を使用して、起動時に ZFS ファイルシステムの systemd マウント ユニットを作成することもできます。systemd は、zfs-mount.service を使用せずに、マウント ユニットに基づいてファイルシステムを自動的にマウントします。そのためには、次のことを行う必要があります。\n\n1. /etc/zfs/zfs-list.cache ディレクトリを作成します。\n1. マウント可能な ZFS ファイルシステムのリストを作成するために必要な ZFS イベント デーモン (ZED) スクリプト (ZEDLET と呼ばれる) を有効にします。(このリンクは、OpenZFS 2.0.0 以上を使用している場合に自動的に作成されます。) # ln -s /usr/lib/zfs/zed.d/history_event-zfs-list-cacher.sh /etc/zfs /zed.d\n1. 有効化 zfs.target および 起動/有効化 ZFS イベントデーモン (zfs-zed.service) このサービスは、前のステップでスクリプトを実行する役割を果たします。\n1. /etc/zfs/zfs-list.cache にプールにちなんだ名前の空のファイルを作成する必要があります。ZEDLET は、プールのファイルがすでに存在する場合にのみファイルシステムのリストを更新します。# touch /etc/zfs/zfs-list.cache/<プール名>\n1. /etc/zfs/zfs-list.cache/<プール名> の内容を確認します。空の場合は、zfs-zed.service が実行されていることを確認し、次を実行して ZFS ファイルシステムのいずれかの canmount プロパティを変更します: {{bc|1=zfs set canmount=off zroot/この変更により、ZFS は ZED によってキャプチャされるイベントを発生させ、ZEDLET を実行して /etc/zfs/zfs-list.cache 内のファイルを更新します。/etc/zfs/zfs-list.cache 内のファイルが更新された場合は、次のコマンドを実行してファイルシステムの canmount プロパティを設定し直すことができます: zfs set canmount=on zroot/fs1\n\n```\n# ln -s /usr/lib/zfs/zed.d/history_event-zfs-list-cacher.sh /etc/zfs /zed.d\n```\n\n```\n# touch /etc/zfs/zfs-list.cache/<プール名>\n```\n\n```\nzfs set canmount=on zroot/fs1\n```\n\nシステム内の ZFS プールごとに、/etc/zfs/zfs-list.cache にファイルを追加する必要があります。zfs-import-cache.service と zfs-import.target を 上で説明 として有効にして、プールがインポートされていることを確認します。\n\n"
    },
    {
      "title": "ストレージプールの作成",
      "level": 2,
      "content": "利用できるドライブの一覧を見るには # parted --list を使います。zfs ファイルシステムを作成する前にドライブをパーティションする必要はありません、推奨もされていません。\n\nドライブの一覧を確認できたら、次は zpool を追加するドライブの id を取得します。10デバイス以下の ZFS ストレージプールを作成する際はデバイス id を使うことを zfs on Linux の開発者は推奨しています。id を調べるには:\n\n```\n# ls -lh /dev/disk/by-id/\n```\n\nid は以下のように表示されます:\n\n```\nlrwxrwxrwx 1 root root  9 Aug 12 16:26 ata-ST3000DM001-9YN166_S1F0JKRR -> ../../sdc\nlrwxrwxrwx 1 root root  9 Aug 12 16:26 ata-ST3000DM001-9YN166_S1F0JTM1 -> ../../sde\nlrwxrwxrwx 1 root root  9 Aug 12 16:26 ata-ST3000DM001-9YN166_S1F0KBP8 -> ../../sdd\nlrwxrwxrwx 1 root root  9 Aug 12 16:26 ata-ST3000DM001-9YN166_S1F0KDGY -> ../../sdb\n```\n\nそして、いよいよ ZFS プールを作成します:\n\n```\n# zpool create -f -m <mount> <pool> raidz <ids>\n```\n\n- create: プールを作成するサブコマンド。\n\n- -f: 強制的にプールを作成する。\"EFI label error\" を無視します。#Does not contain an EFI label を見て下さい。\n\n- -m: プールのマウントポイント。指定されない場合、プールは /<pool> にマウントされます。\n\n- pool: プールの名前。\n\n- raidz: デバイスのプールから作成される仮想デバイスのタイプ。Raidz は raid5 の特殊な実装です。raiz に関する詳細は Jeff Bonwick's Blog -- RAID-Z を見て下さい。RAID-1 を使用する場合は代わりに mirror を使用したほうが良いでしょう [5]。\n\n- ids: プールに含まれるドライブまたはパーティションの名前。/dev/disk/by-id で取得。\n\n完全なコマンドは以下のようになります:\n\n```\n# zpool create -f -m /mnt/data bigdata raidz ata-ST3000DM001-9YN166_S1F0KDGY ata-ST3000DM001-9YN166_S1F0JKRR ata-ST3000DM001-9YN166_S1F0KBP8 ata-ST3000DM001-9YN166_S1F0JTM1\n```\n\n"
    },
    {
      "title": "Advanced Format ディスク",
      "level": 3,
      "content": "セクターサイズが512バイトではなく4096バイトの Advanced Format ディスクを使用する場合、レガシーなシステムとの後方互換性のため ZFS のセクターサイズ自動検出アルゴリズムが512バイトと検出して、パフォーマンスが落ちてしまうことがあります。正しいセクターサイズが使われるように、ashift=12 オプションを使用して下さい (ZFS on Linux FAQ を参照)。この場合の完全なコマンドは以下の通りになります:\n\n```\n# zpool create -f -o ashift=12 -m /mnt/data bigdata raidz ata-ST3000DM001-9YN166_S1F0KDGY ata-ST3000DM001-9YN166_S1F0JKRR ata-ST3000DM001-9YN166_S1F0KBP8 ata-ST3000DM001-9YN166_S1F0JTM1\n```\n\n"
    },
    {
      "title": "プールの作成の確認",
      "level": 3,
      "content": "コマンドが成功しても、何も出力はされません。$ mount コマンドを使うことでプールがマウントされているかどうか表示できます。# zpool status を使えばプールが作成されたか表示されます。\n\n```\n# zpool status\n```\n\n```\npool: bigdata\n state: ONLINE\n scan: none requested\nconfig:\n\n        NAME                                       STATE     READ WRITE CKSUM\n        bigdata                                    ONLINE       0     0     0\n          -0                                       ONLINE       0     0     0\n            ata-ST3000DM001-9YN166_S1F0KDGY-part1  ONLINE       0     0     0\n            ata-ST3000DM001-9YN166_S1F0JKRR-part1  ONLINE       0     0     0\n            ata-ST3000DM001-9YN166_S1F0KBP8-part1  ONLINE       0     0     0\n            ata-ST3000DM001-9YN166_S1F0JTM1-part1  ONLINE       0     0     0\n\nerrors: No known data errors\n```\n\nこの段階で、起動時に ZFS プールがマウントされるか確認するためにマシンを再起動すると良いでしょう。データを移し替える前に全てのエラーに対処するのがベストです。\n\n"
    },
    {
      "title": "GRUB 互換のプールを作成する",
      "level": 3,
      "content": "zpool はプールの全ての機能を有効にします。/boot を ZFS 上に配置して GRUB を使用する場合、GRUB によってサポートされている機能だけを有効にしてください。そうしないと GRUB がプールを読み込むことができなくなります。GRUB 2.02 は lz4_compress, hole_birth, embedded_data, extensible_dataset, large_blocks をサポートしています。これは ZFSonLinux 0.7.1 の全ての機能ではないため、サポートされていない機能は無効化する必要があります。\n\n未対応の機能を無効化してプールを作成するには:\n\n```\n# zpool create -o feature@multi_vdev_crash_dump=disabled \\\n               -o feature@large_dnode=disabled           \\\n               -o feature@sha512=disabled                \\\n               -o feature@skein=disabled                 \\\n               -o feature@edonr=disabled                 \\\n               $POOL_NAME $VDEVS\n```\n\nZFS on Linux の git 版を使う場合は -o feature@encryption=disabled も追加してください。\n\n"
    },
    {
      "title": "一般",
      "level": 3,
      "content": "zfs ファイルシステムでは多数のパラメータを使うことができます。パラメータの完全なリストは zfs get all <pool> で見ることが可能です。調整するパラメータとしては atime と compression が一般的です。\n\natime はデフォルトで有効にされていますが、ほとんどのユーザーにとっては zpool に余分な書き込みになります。zfs コマンドを使うことで atime を無効にできます:\n\n```\n# zfs set atime=off <pool>\n```\n\natime を完全にオフにする代わりとして、relatime を使うこともできます。ext4/xfs のデフォルトの atime の処理と同じで、変更日時が更新されたとき、または24時間で一度もアクセス日時が更新されなかった場合にのみ、ファイルのアクセス日時が更新されます。atime=off と atime=on の折衷案になります。このプロパティは atime が on になっているときだけ適用されます:\n\n```\n# zfs set relatime=on <pool>\n```\n\ncompression はつまり、データの透過圧縮です。ZFS は様々なアルゴリズムに対応しており、現在は lz4 はデフォルトです。あまり書き込みを行わないようなデータなら gzip を有効にすることで高い圧縮率を実現できます。詳しくは man ページを読んでください。zfs コマンドを使って圧縮を有効にするには:\n\n```\n# zfs set compression=lz4 <pool>\n```\n\nまた、zfs コマンドを使うことで zfs の他のオプションを表示できます:\n\n```\n# zfs get all <pool>\n```\n\n"
    },
    {
      "title": "SSD キャッシュ",
      "level": 3,
      "content": "SSD デバイスを書き込みインテントログ (ZIL または SLOG) として追加して l2arc (layer 2 adaptive replacement cache) として使うことができます。手順は新しい VDEV を作成するのと似ています。以下のコマンドで使用する <device-id> は ls -lh /dev/disk/by-id/ で確認できます。\n\nZIL を追加するには:\n\n```\n# zpool add <pool> log <device-id>\n```\n\nまたはミラー ZIL を追加するには:\n\n```\n# zpool add <pool> log mirror <device-id-1> <device-id-2>\n```\n\nl2arc を追加するには:\n\n```\n# zpool add <pool> cache <device-id>\n```\n\nまたはミラー l2arc を追加するには:\n\n```\n# zpool add <pool> cache mirror <device-id-1> <device-id-2>\n```\n\n"
    },
    {
      "title": "データベース",
      "level": 3,
      "content": "ZFS は他のファイルシステムとは異なり、(一般的にはブロックサイズと呼ばれる) レコードサイズを変えることができます。デフォルトでは、ZFS のレコードサイズは 128KiB で、書き込むファイルのサイズにあわせて 512B から 128KiB までのサイズのブロックを動的に割り当てます。数バイトの書き込みの時でも新規に 128KiB のブロックを割り当てなくてはならないという代償を払って、断片化やファイルアクセスが補助されます。\n\nほとんどの RDBMS はデフォルトで 8KiB サイズのブロックで動作します。MySQL/MariaDB, PostgreSQL, Oracle のブロックサイズは調整することできますが、デフォルトのブロックサイズは3つとも 8KiB を使っています。パフォーマンスの影響を考え、また、(バックアップ用の) スナップショットの差異を最小限にするために、以下のようなコマンドを使って ZFS をデータベースに適応するよう設定するのが望ましいでしょう:\n\n```\n# zfs set recordsize=8K <pool>/postgres\n```\n\nこれらの RDBMS にはそれぞれ独自のキャッシュアルゴリズムが実装されており、大抵は ZFS の ARC と同じようなメカニズムです。メモリの節約の観点から、ZFS によるデータベースのファイルデータのキャッシュを無効にして、キャッシュについてはデータベースに任せるのが良いでしょう:\n\n```\n# zfs set primarycache=metadata <pool>/postgres\n```\n\nプールにログデバイスが設定されていない場合、ZFS はインテントログ (ZIL) のためにプールのデータディスク上に領域を取っておきます。ZFS はクラッシュからのリカバリの際にこのスペースを使いますが、データベースは大抵トランザクションがコミットされるたびにデータファイルをファイルシステムに同期させます。この結果 ZFS が二回データをデータディスクに引き渡すことになるので、パフォーマンスにかなり影響が出てしまう可能性があります。ZFS が ZIL を使わないように設定することができ、この場合は、データがファイルシステムに渡されるのは一度だけになります。データベース以外のファイルシステムや、ログデバイスが設定されているプールでこの設定を行うと、逆にパフォーマンスが下がってしまうことになるので、注意してください:\n\n```\n# zfs set logbias=throughput <pool>/postgres\n```\n\nファイルシステムの作成時に以上の設定を行うこともできます、例えば:\n\n```\n# zfs create -o recordsize=8K \\\n             -o primarycache=metadata \\\n             -o mountpoint=/var/lib/postgres \\\n             -o logbias=throughput \\\n              <pool>/postgres\n```\n\n"
    },
    {
      "title": "/tmp",
      "level": 3,
      "content": "ZFS を使って /tmp ディレクトリを保存したい場合、ファイルシステムの同期を無効にすることで /tmp への書き込みを行うアプリケーションのパフォーマンスを向上させることがあります。これによって ZFS はアプリケーションの同期リクエスト (例: fsync や O_SYNC) を無視して、すぐに戻るようになります。アプリケーションサイドのデータの一貫性を保つのが難しくなる一方 (データベースで同期を無効化してはいけません)、/tmp のファイルはあまり重要でないことが多いため、影響は限られます。ZFS 自体の整合性には影響を与えないので注意してください。アプリケーションがディスク上にあると期待しているデータが実際に書きだされなくてクラッシュが起こる可能性があるくらいです。\n\n```\n# zfs set sync=disabled <pool>/tmp\n```\n\nさらに、セキュリティ上の理由で、/tmp ファイルシステムの setuid と devices を無効化すると良いでしょう。特権昇格攻撃やデバイスノードの使用をふせぐことができます:\n\n```\n# zfs set setuid=off <pool>/tmp\n# zfs set devices=off <pool>/tmp\n```\n\n上記の全てをまとめると create コマンドは以下のようになります:\n\n```\n# zfs create -o setuid=off -o devices=off -o sync=disabled -o mountpoint=/tmp <pool>/tmp\n```\n\nまた、ZFS に /tmp を配置するときは、systemd の tmpfs による自動的な /tmp をマスク (無効化) する必要があるので注意してください。そうしないと ZFS が起動時やインポート時にデータセットをマウントできなくなってしまいます:\n\n```\n# systemctl mask tmp.mount\n```\n\n"
    },
    {
      "title": "zvol",
      "level": 3,
      "content": "zvol は RDBMS と同じようにブロックサイズ関連の問題を被りますが、zvol のデフォルトの recordsize は始めから 8KiB になっているので大丈夫です。可能であれば、zvol に含まれているパーティションは recordsize にあわせて (fdisk と gdisk の最新版ではデフォルトで 1MiB セグメントに合わせます)、ファイルシステムのブロックサイズも同じサイズに合わせるのがベストです。それ以外にも、必要に応じて recordsize を zvol 内のデータに適合させることもできます (ほとんどのファイルシステムでは 8KiB で十分ですが、4KiB のブロックを使用した方が良いときもあります)。\n\n"
    },
    {
      "title": "RAIDZ と Advanced Format 物理ディスク",
      "level": 4,
      "content": "zvol の各ブロックにはそれぞれパリティディスクが存在しており、もし、論理ブロックサイズが 4096B, 8192B などの物理メディアを使用していて、パリティを物理ブロック全体に保存する必要がある場合、zvol に必要な容量が劇的に増えてしまう可能性があります。zvol の論理容量の2倍位上の物理ストレージ容量が必要になります。recordsize を 16k や 32k に設定することでこの必要容量をかなり減らすことができるかもしれません。\n\n詳しくは ZFS on Linux issue #1807 を参照。\n\n"
    },
    {
      "title": "使用方法",
      "level": 2,
      "content": "zpool の下に手動でディレクトリを作成するのに対して、任意で zpool の下にデータセットを作成することができます。データセットはスナップショットに加えて制御のレベルを増加させます (クォータなど)。データセットの作成とマウントをするときは、同じ名前のディレクトリが zpool に存在してはいけません。データセットを作成するには、次を使用:\n\n```\n# zfs create <nameofzpool>/<nameofdataset>\n```\n\nデータセットには ZFS の特定属性を適用させることができます。例えば、データセット内の特定のディレクトリにクォータ制限をかけることが可能です:\n\n```\n# zfs set quota=20G <nameofzpool>/<nameofdataset>/<directory>\n```\n\nZFS で利用できるコマンドを全て表示するには、次のコマンドを使用:\n\n```\n$ man zfs\n```\n\nまたは:\n\n```\n$ man zpool\n```\n\n"
    },
    {
      "title": "ネイティブ暗号化",
      "level": 3,
      "content": "ZFS のネイティブ暗号化は 0.7.0.r26 以上のバージョンから使うことができ zfs-linux-gitAUR, zfs-dkms-gitAUR などの開発ビルドのパッケージで利用できます。バージョン 0.7 はリリースされていますが、0.7.3 現在、安定版では機能は有効になっていないために開発ビルドが必要になります。/usr/src/zfs-*/include/sys/fs/zfs.h の ZFS_PROP_ENCRYPTION の定義をチェックすることで、インストールしている zfs で暗号化が使えるかどうか確認できます。\n\n- サポートされている暗号化オプション: aes-128-ccm, aes-192-ccm, aes-256-ccm, aes-128-gcm, aes-192-gcm, aes-256-gcm。暗号化が on に設定されている場合、aes-256-ccm が使われます。\n- サポートされているキーフォーマット: passphrase, raw, hex。\n\n-o pbkdf2iters <n> で PBKDF2 の繰り返しを指定することもできます (キーの復号化に時間がかかるようになります)。\n\nパスフレーズによるネイティブ暗号化を有効にしてデータセットを作成するには:\n\n```\n# zfs create -o encryption=on -o keyformat=passphrase <nameofzpool>/<nameofdataset>\n```\n\nパスフレーズの代わりにキーを使うには:\n\n```\n# dd if=/dev/urandom of=/path/to/key bs=1 count=32\n# zfs create -o encryption=on -o keyformat=raw -o keylocation=file:///path/to/key <nameofzpool>/<nameofdataset>\n```\n\n手動でキーをロードしてから暗号化されたデータセットをマウントすることも可能です:\n\n```\n# zfs load-key <nameofzpool>/<nameofdataset> # load key for a specific dataset\n# zfs load-key -a # load all keys\n# zfs load-key -r zpool/dataset # load all keys in a dataset\n```\n\n暗号化されたデータセットを含むプールをインポートすると、ZFS はデフォルトではデータセットを復号化しません。復号化するには -l を使います:\n\n```\n# zpool import -l pool\n```\n\nsystemd カスタムユニットを使うことで起動時に復号化を自動的に行うことができます。例:\n\n```\n/etc/systemd/system/zfs-key@.service\n```\n\n```\n[Unit]\nDescription=Load storage encryption keys\nBefore=systemd-user-sessions.service\nBefore=zfs-mount.service\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nExecStart=/usr/bin/bash -c '/usr/bin/zfs load-key zpool/%i <<< $(systemd-ask-password \"Encrypted storage password (%i): \")'\n\n[Install]\nWantedBy=zfs-mount.service\n```\n\n暗号化ボリュームを指定してサービスインスタンスを有効化してください: # systemctl enable zfs-key@dataset。\n\nsystemd-user-sessions.service の Before= によってローカル IO デバイスがシステム UI に移る前に systemd-ask-password が呼び出されます。\n\n"
    },
    {
      "title": "スクラブ",
      "level": 3,
      "content": "少なくとも週に一度は ZFS プールをスクラブするべきです。プールをスクラブするには:\n\n```\n# zpool scrub <pool>\n```\n\n週一で自動的にスクラブするようにするには、root の crontab に次の行を設定してください:\n\n```\n# crontab -e\n```\n\n```\n...\n30 19 * * 5 zpool scrub <pool>\n...\n```\n\n<pool> は ZFS プールの名前に置き換えて下さい。\n\n"
    },
    {
      "title": "zfs プールの状態を確認",
      "level": 3,
      "content": "読み書きエラーなどの情報を含む、ZFS プールの統計が書かれた表を表示するには、次を使用:\n\n```\n# zpool status -v\n```\n\n"
    },
    {
      "title": "ストレージプールを破壊",
      "level": 3,
      "content": "ZFS ではマウントされているストレージプールを破壊して、ZFS デバイスに関する全てのメタデータを簡単に削除することができます。次のコマンドはプールに含まれているデータを全て破壊します:\n\n```\n# zpool destroy <pool>\n```\n\n状態を確認すると:\n\n```\n# zpool status\n```\n\n```\nno pools available\n```\n\nプールの名前を確認するには、#zfs プールの状態を確認 を見て下さい。\n\n"
    },
    {
      "title": "ストレージプールをエクスポート",
      "level": 3,
      "content": "ストレージプールを他のシステムで使用するときは、まずエクスポートする必要があります。また、archiso からプールをインポートしたときも、archiso での hostid と起動環境での hostid が異なるのでプールをエクスポートしなくてはなりません。zpool コマンドはエクスポートされていないストレージプールのインポートを拒否します。-f 引数を使って強制的にインポートすることもできますが、良い手段とは言えません。\n\nエクスポートされていないストレージプールをインポートするとストレージプールが他のシステムによって使用されているというエラーが発生します。このエラーは起動時に発生することがあり、busybox コンソールでシステムが突然停止して、archiso による応急処置が必要になります。プールをエクスポートするか、またはカーネルのブートパラメータに zfs_force=1 を追加してください (こちらの方法はあまり芳しくありません)。#起動時に zfs プールがマウントされない: \"pool may be in use from other system\" を参照。\n\nプールをエクスポートするには:\n\n```\n# zpool export <pool>\n```\n\n"
    },
    {
      "title": "Zpool の名前を変更",
      "level": 3,
      "content": "既に作成済みの zpool の名前の変更は2ステップで行います:\n\n```\n# zpool export oldname\n# zpool import oldname newname\n```\n\n"
    },
    {
      "title": "別のマウントポイントを設定",
      "level": 3,
      "content": "指定の zpool のマウントポイントは一つのコマンドで随意に移動することができます:\n\n```\n# zfs set mountpoint=/foo/bar poolname\n```\n\n"
    },
    {
      "title": "アクセス制御リスト",
      "level": 3,
      "content": "ZFS プールで ACL を使うには:\n\n```\n# zfs set acltype=posixacl <nameofzpool>/<nameofdataset>\n# zfs set xattr=sa <nameofzpool>/<nameofdataset>\n```\n\nパフォーマンス上の理由から xattr を設定することが推奨されています [6]。\n\n"
    },
    {
      "title": "スワップボリューム",
      "level": 3,
      "content": "ZFS ではスワップファイルを使うことはできませんが、ユーザーは ZFS ボリューム (ZVOL) をスワップとして利用することができます。ZVOL のブロックサイズをシステムページサイズと一致するように設定するのが重要です。システムページサイズは getconf PAGESIZE コマンドで取得することができます (x86_64 でのデフォルトは 4KiB)。また、メモリが少ないシチュエーションでシステムを上手く動作させ続けるには zvol データのキャッシュを行わないほうが良いでしょう。\n\n8 GiB の zfs ボリュームを作成:\n\n```\n# zfs create -V 8G -b $(getconf PAGESIZE) \\\n              -o logbias=throughput -o sync=always\\\n              -o primarycache=metadata \\\n              -o com.sun:auto-snapshot=false <pool>/swap\n```\n\nスワップパーティションとして設定:\n\n```\n# mkswap -f /dev/zvol/<pool>/swap\n# swapon /dev/zvol/<pool>/swap\n```\n\nこの設定を永続化させるには、/etc/fstab を編集します。ZVOL は discard をサポートしており、ZFS のブロックアロケータの役に立ったり、スワップが満杯になったときに他のデータセットにおける断片化を減らすことができます。\n\n/etc/fstab に次の行を追加:\n\n```\n/dev/zvol/<pool>/swap none swap discard 0 0\n```\n\nハイバネートフックはファイルシステムよりも前にロードされる必要があり、ZVOL をスワップとして使うとハイバネート機能が使えなくなることを覚えておいて下さい。ハイバネートが必要な場合は、スワップ用のパーティションを取っておいて下さい。\n\n"
    },
    {
      "title": "Linux の ZFS 自動スナップショットサービス",
      "level": 4,
      "content": "AUR の zfs-auto-snapshot-gitAUR パッケージにはスナップショットの管理の自動化を行うシェルスクリプトが入っています。スナップショットには日付とラベルで名前が付けられ (1時間毎、1日毎など)、全ての ZFS のデータセットを素早く簡単にスナップショットできます。15分毎・1時間毎・1日毎・1週間毎・1ヶ月毎にスナップショットを作成する cron タスクもインストールされます。任意で --keep パラメータを調整することでスナップショットを保存する期間を設定できます (1ヶ月毎のスクリプトはデフォルトで1年間までデータを保存します)。\n\nデータセットがスナップショットされないようにするには、データセットに com.sun:auto-snapshot=false を設定してください。同じように、ラベルで細かい設定をすることができます。例えば、1ヶ月毎のスナップショットを作成しないようにするには com.sun:auto-snapshot:monthly=false を設定します。\n\n"
    },
    {
      "title": "ZFS スナップショットマネージャ",
      "level": 4,
      "content": "AUR の zfs-snap-managerAUR パッケージは ZFS データセットのスナップショットを毎日作成して \"Grandfather-father-son\" 方式で削除していく python サービスを提供します。7日毎、5週間毎、3ヶ月毎、2年間毎などのスナップショットを保持するよう設定できます。\n\nこのパッケージは ZFS が動作している他のマシンへのレプリケーションもサポートしており zfs send と zfs receive を使います。同期先のマシンでもこのパッケージを使用している場合、複製したスナップショットを長期間にわたって保持するように設定することが可能です。これによって、同期元のマシンには毎日のスナップショットだけを少しだけローカルに保存して、リモートのストレージサーバーに大量のスナップショットを保存するという構成ができます。\n\n"
    },
    {
      "title": "ZPool の作成が失敗する",
      "level": 3,
      "content": "以下のエラーが発生した場合:\n\n```\n# the kernel failed to rescan the partition table: 16\n# cannot label 'sdc': try using parted(8) and then provide a specific slice: -1\n```\n\nZFS がプールの作成が1秒未満で終わることを要求しているのが原因の一つです。通常状態では問題ありませんが、1秒以上かかってしまう状況というのはたくさんあります。もう一度作成を試行する前に、それぞれのドライブをクリアにする必要があります。\n\n```\n# parted /dev/sda rm 1\n# parted /dev/sda rm 1\n# dd if=/dev/zero of=/dev/sdb bs=512 count=1\n# zpool labelclear /dev/sda\n```\n\n力づくで何度も作成してみることもでき、運が良ければ ZPool の作成が1秒未満で終わるでしょう。作成のスローダウンが一度発生すると、ドライブのバースト読み書きが遅くなることがあります。ZPool の作成と並行して読み込むことで、バースト速度を向上させることが可能です。\n\n```\n# dd if=/dev/sda of=/dev/null\n```\n\n複数のディスクで行うには、上記のコマンドをそれぞれのディスクでファイルに保存して次を実行:\n\n```\n# cat $FILE | parallel\n```\n\nそして同時に ZPool の作成を実行します。\n\n"
    },
    {
      "title": "ZFS の使用 RAM が多すぎる",
      "level": 3,
      "content": "デフォルトでは、ZFS はホストのシステムメモリの3分の2まで使用してファイル操作をキャッシュします (ARC)。ZFS はエンタープライズクラスのストレージシステム用に設計されたことを思い出して下さい。ARC サイズを調整するには、以下をカーネルパラメータのリストに追加します:\n\n```\nzfs.zfs_arc_max=536870912 # (for 512MB)\n```\n\n他の設定オプションや、より詳しい説明は gentoo-wiki:zfs#arc を見て下さい。\n\n"
    },
    {
      "title": "Does not contain an EFI label",
      "level": 3,
      "content": "zfs ファイルシステムを作成しようとするときに以下のエラーが発生することがあります:\n\n```\n/dev/disk/by-id/<id> does not contain an EFI label but it may contain partition\n```\n\nこのエラーを打破するには zfs の create コマンドで -f を使って下さい。\n\n"
    },
    {
      "title": "No hostid found",
      "level": 3,
      "content": "起動時に initscript の出力が表示される前に以下のエラーが発生する場合:\n\n```\nZFS: No hostid found on kernel command line or /etc/hostid.\n```\n\nこの警告は ZFS モジュールが spl hosted にアクセスできないのが原因です。この問題には2つの解決方法があります。一つはブートローダーのカーネルパラメータに spl hostid を記述することです。例えば、spl.spl_hostid=0x00bab10c を追加します。\n\nもう一つの解決方法は /etc/hostid に hostid があることを確認して、initramfs イメージを再生成することです。それによって hostid 情報が initramfs イメージにコピーされます。\n\n```\n# mkinitcpio -p linux\n```\n\n"
    },
    {
      "title": "SAS/SCSI デバイスからの起動中にプールが見つかりません",
      "level": 3,
      "content": "SAS/SCSI ベースでブートしている場合、ブートしようとしているプールが見つからないというブートの問題が発生することがあります。この原因として考えられるのは、デバイスの初期化がプロセスの中で遅すぎることです。これは、zfs がプールを組み立てようとする時点でデバイスを見つけることができないことを意味します。\n\nこの場合、続行する前に、デバイスがオンラインになるまで SCSI ドライバーを強制的に待機させる必要があります。これを /etc/modprobe.d/zfs.conf に記述します。\n\n```\n/etc/modprobe.d/zfs.conf\n```\n\n```\noptions scsi_mod scan=sync\n```\n\nその後、initramfs を再生成\n\nこれが機能するのは、zfs フックが /etc/modprobe.d/zfs.conf にあるファイルを initcpio にコピーし、ビルド時に使用されるためです。\n\n"
    },
    {
      "title": "プールがエクスポートされていない",
      "level": 4,
      "content": "zpool がインポートできないために新しいインストール環境が起動しない場合、インストール環境に chroot して zpool を正しくエクスポートしてください。#archzfs による chroot の応急処置 を参照。\n\nchroot 環境に入ったら、ZFS モジュールをロードして強制的に zpool をインポートします:\n\n```\n# zpool import -a -f\n```\n\nそしてプールをエクスポートします:\n\n```\n# zpool export <pool>\n```\n\n利用可能なプールを確認するには、次を使用:\n\n```\n# zpool status\n```\n\nZFS は hostid を使って zpool が作成されたシステムを追跡するのでプールをエクスポートする必要があります。hostid はネットワークの設定時に生成されます。archiso でのインストール中、ネットワーク設定が違うせいで新しい環境に含まれている hostid と異なる hostid が生成されることがあります。zfs ファイルシステムをエクスポートしたら新しい環境でインポートしなおしてください。それで hostid はリセットされます。参照: Re: Howto zpool import/export automatically? - msg#00227。\n\n起動する度に ZFS が \"pool may be in use\" と表示する場合は、上記のようにプールを適切にエクスポートして、通常通りに ramdisk を再生成してください:\n\n```\n# mkinitcpio -p linux\n```\n\n"
    },
    {
      "title": "hostid が間違っている",
      "level": 4,
      "content": "プールが正しくエクスポートされていることを確認してください。zpool をエクスポートすると所有者を示す hostid がクリアされます。そのため最初の起動時に zpool を正しくマウントする必要があります。マウントされなかった場合、他の問題が起こります。\n\nhostid が正しく設定されておらず zfs が混同されているために zfs プールがマウントできないときは、もう一度再起動してください。手動で zfs に適切な数字を指定します。hostid は再起動しても変わらないため zpool は適切にマウントされます。\n\nzfs_force を使って起動して hostid をメモします。例:\n\n```\n% hostid\n0a0af0f8\n```\n\nこの数字を spl.spl_hostid=0x0a0af0f8 のようにして カーネルパラメータ に追加してください。もう一つの解決方法は hostid を initram イメージの中に書き出すことです。この方法については インストールガイド の説明を見て下さい。\n\nカーネルパラメータ に zfs_force=1 を追加することでチェックを常に無視することができます。ただし恒久的に使用するのは推奨されません。\n\n"
    },
    {
      "title": "デバイスのセクタアライメントが異なっている",
      "level": 3,
      "content": "ドライブが故障したときは、できるだけはやく同じドライブでドライブを交換してください。\n\n```\n# zpool replace bigdata ata-ST3000DM001-9YN166_S1F0KDGY ata-ST3000DM001-1CH166_W1F478BD -f\n```\n\nただし、この場合、以下のエラーが発生します:\n\n```\ncannot replace ata-ST3000DM001-9YN166_S1F0KDGY with ata-ST3000DM001-1CH166_W1F478BD: devices have different sector alignment\n```\n\nZFS は ashift オプションを使って物理ブロックサイズを調整しています。故障したディスクを置き換えるとき、ZFS は ashift=12 を使用しますが、故障したディスクの ashift が異なっている (例えば ashift=9) 場合、結果としてこのエラーが起こってしまいます。\n\nブロックサイズが 4KB の Advanced Format ディスクの場合、一番良いパフォーマンスを出すため ashift は 12 が推奨されます。1.10 What’s going on with performance? や ZFS and Advanced Format disks を見て下さい。\n\nzpool の ashift を確認するには zdb を使います: zdb | grep ashift。\n\n-o 引数を使って交換用ドライブの ashift を設定してください:\n\n```\n# zpool replace bigdata ata-ST3000DM001-9YN166_S1F0KDGY ata-ST3000DM001-1CH166_W1F478BD -o ashift=9 -f\n```\n\nzpool status で確認を行なって下さい:\n\n```\n# zpool status -v\n```\n\n```\npool: bigdata\nstate: DEGRADED\nstatus: One or more devices is currently being resilvered.  The pool will\n        continue to function, possibly in a degraded state.\naction: Wait for the resilver to complete.\nscan: resilver in progress since Mon Jun 16 11:16:28 2014\n    10.3G scanned out of 5.90T at 81.7M/s, 20h59m to go\n    2.57G resilvered, 0.17% done\nconfig:\n\n        NAME                                   STATE     READ WRITE CKSUM\n        bigdata                                DEGRADED     0     0     0\n        raidz1-0                               DEGRADED     0     0     0\n            replacing-0                        OFFLINE      0     0     0\n            ata-ST3000DM001-9YN166_S1F0KDGY    OFFLINE      0     0     0\n            ata-ST3000DM001-1CH166_W1F478BD    ONLINE       0     0     0  (resilvering)\n            ata-ST3000DM001-9YN166_S1F0JKRR    ONLINE       0     0     0\n            ata-ST3000DM001-9YN166_S1F0KBP8    ONLINE       0     0     0\n            ata-ST3000DM001-9YN166_S1F0JTM1    ONLINE       0     0     0\n\nerrors: No known data errors\n```\n\n"
    },
    {
      "title": "プールの再同期が停止/再起動/遅いですか?",
      "level": 3,
      "content": "ZFSonLinux github によると、これは 2012 年以降の ZFS-ZED の既知の問題で、再同期化プロセスが常に再起動し、場合によってはスタックし、一部のハードウェアでは一般的に遅くなります。最も簡単な軽減策は、再同期が完了するまで zfs-zed.service を停止することです。\n\n"
    },
    {
      "title": "initramfs zpool.cache 内の使用できないプールのインポートの失敗によって引き起こされる起動の遅さを修正しました。",
      "level": 3,
      "content": "永続的に接続されていない追加のプールがインポートされているときに intramfs を更新すると (カーネル更新の実行時など)、起動時間が大幅に影響を受ける可能性があります。これは、これらのプールが initramfs zpool.cache に追加され、ZFS がこれらのプールをインポートしようとするためです。エクスポートしたり、通常の zpool.cache から削除したりしたかどうかに関係なく、ブートごとに追加のプールが作成されます。\n\nZFS が起動時に使用できないプールをインポートしようとしていることに気付いた場合は、まず次のコマンドを実行します。\n\n```\n$ zdb -C\n```\n\n起動時にインポートしたくないプールがないか zpool.cache を確認します。このコマンドで、現在使用できない追加のプールが表示される場合は、次のコマンドを実行します。\n\n```\n# zpool set cachefile=/etc/zfs/zpool.cache zroot\n```\n\nzroot という名前のプール以外のプールの zpool.cache をクリアします。場合によっては、zpool.cache を更新する必要がなく、代わりに initramfs を再生成 するだけで済みます。\n\n"
    },
    {
      "title": "ZFS コマンド履歴",
      "level": 3,
      "content": "ZFS は、プールの構造への変更を、実行されたコマンドのログとしてリングバッファーにネイティブに記録します (これをオフにすることはできません) ログは、劣化したプールまたは障害が発生したプールを復元するときに役立つ場合があります。\n\n```\n# zpool history zpool\n```\n\n```\nHistory for 'zpool':\n2023-02-19.16:28:44 zpool create zpool raidz1 /scratch/disk_1.img /scratch/disk_2.img /scratch/disk_3.img\n2023-02-19.16:31:29 zfs set compression=lz4 zpool\n2023-02-19.16:41:45 zpool scrub zpool\n2023-02-19.17:00:57 zpool replace zpool /scratch/disk_1.img /scratch/bigger_disk_1.img\n2023-02-19.17:01:34 zpool scrub zpool\n2023-02-19.17:01:42 zpool replace zpool /scratch/disk_2.img /scratch/bigger_disk_2.img\n2023-02-19.17:01:46 zpool replace zpool /scratch/disk_3.img /scratch/bigger_disk_3.img\n```\n\n"
    },
    {
      "title": "archzfs パッケージを archiso に埋め込む",
      "level": 3,
      "content": "必要なソフトウェアが含まれるインストールメディアを作成するのは良い考えです。ただし、最新の archiso インストールメディアを CD や USB キーに書き込む必要があります。\n\n既存の環境から、archiso に zfs を埋め込むには、archiso パッケージをダウンロード:\n\n```\n# pacman -S archiso\n```\n\nプロセスを開始:\n\n```\n# cp -r /usr/share/archiso/configs/releng /root/media\n```\n\npackages.x86_64 ファイルを編集して以下の行を追加:\n\n```\nspl-utils-linux-git\nspl-linux-git\nzfs-utils-linux-git\nzfs-linux-git\n```\n\npacman.conf ファイルを編集して以下の行を追加:\n\n```\n[archzfs]\nSigLevel = Never\nServer = http://archzfs.com/$repo/x86_64\n```\n\n必要であれば他のパッケージも packages.both, packages.i686, packages.x86_64 に追加してイメージを作成します。\n\n```\n# ./build.sh -v\n```\n\nイメージは /root/media/out ディレクトリにできます。\n\nプロセスの詳細は このガイド や Archiso の記事で読めます。\n\nUEFI 環境にインストールする場合は、Unified Extensible Firmware Interface#ISO から UEFI ブータブル USB を作成する を見て UEFI に対応するインストールメディアを作成してください。\n\n"
    },
    {
      "title": "ZFS on Linux で暗号化",
      "level": 3,
      "content": "ZFS on Linux の安定版は直接暗号化をサポートしていませんが、zpool を dm-crypt ブロックデバイスの中に作成することができます。zpool は平文の抽象レイヤー上に作成されるので、重複排除や圧縮、データの堅牢性といった ZFS の利点を全て活かしながらデータを暗号化することが可能です。\n\ndm-crypt (LUKS) は /dev/mapper にデバイスを作成し、デバイスの名前は固定されます。そのため、zpool create コマンドを変更してその名前を使う必要があります。/dev/mapper ブロックデバイスを作成してそこから zpool をインポートするようにシステムを設定することが狙いです。zpool は複数のデバイスに作成することができるので (raid, mirroring, striping, ...)、全てのデバイスを暗号化するのが重要です。そうしないと保護が部分的に欠ける恐れがあります。\n\n例えば、以下を使うことでプレーンな dm-crypt (LUKS は使わない) で暗号化された zpool を作成することができます:\n\n```\n# cryptsetup --hash=sha512 --cipher=twofish-xts-plain64 --offset=0 --key-file=/dev/sdZ --key-size=512 open --type=plain /dev/sdX enc\n# zpool create zroot /dev/mapper/enc\n```\n\nroot ファイルシステムのプールの場合、mkinicpio.conf の HOOKS 行でパスワードを入力するためのキーボードを有効にしたり、デバイスを作成、プールをロードします。以下のようになります:\n\n```\nHOOKS=\"... keyboard encrypt zfs ...\"\n```\n\n/dev/mapper/enc の名前は固定されているためインポートエラーは発生しません。\n\n暗号化された zpool の作成は問題なく動作します。ただし、ディレクトリの暗号化が必要な場合、例えばユーザーのホームを保護したいとき、ZFS は機能をいくつか失います。\n\nZFS は平文の抽象レイヤーではなく暗号化されたデータを回覧するので、圧縮や重複排除は動作しません。暗号化されているデータは高エントロピーであり圧縮がうまく効かないのと、入力と出力が異なって重複排除ができなくなってしまうからです。不必要なオーバーヘッドを減らすために、暗号化ディレクトリにサブファイルシステムを作成して、その上で eCryptfs を使うことができます。\n\n例えばホームを暗号化するには: (暗号化とログインの2つのパスワードは同じである必要があります)\n\n```\n# zfs create -o compression=off -o dedup=off -o mountpoint=/home/<username> <zpool>/<username>\n# useradd -m <username>\n# passwd <username>\n# ecryptfs-migrate-home -u <username>\n<log in user and complete the procedure with ecryptfs-unwrap-passphrase>\n```\n\n"
    },
    {
      "title": "archzfs による chroot の応急処置",
      "level": 3,
      "content": "以下はメンテナンスのために archiso を使って ZFS ファイルシステムに入る方法です。\n\n最新の archiso を起動してネットワークを立ち上げる:\n\n```\n# wifi-menu\n# ip link set eth0 up\n```\n\nネットワーク接続をテスト:\n\n```\n# ping google.com\n```\n\npacman パッケージデータベースを同期:\n\n```\n# pacman -Syy\n```\n\n(任意) テキストエディタのインストール:\n\n```\n# pacman -S vim\n```\n\narchzfs の archiso リポジトリを pacman.conf に追加:\n\n```\n/etc/pacman.conf\n```\n\n```\n[archzfs]\nServer = http://archzfs.com/$repo/x86_64\n```\n\npacman パッケージデータベースを同期:\n\n```\n# pacman -Syy\n```\n\narchzfs のメンテナの PGP 鍵をローカルの (インストーライメージの) 信頼リストに追加:\n\n```\n# pacman-key --lsign-key 0EE7A126\n```\n\nZFS パッケージグループをインストール:\n\n```\n# pacman -S archzfs-linux\n```\n\nZFS カーネルモジュールをロード:\n\n```\n# modprobe zfs\n```\n\nプールをインポート:\n\n```\n# zpool import -a -R /mnt\n```\n\n(存在する場合) ブートパーティションをマウント:\n\n```\n# mount /dev/sda2 /mnt/boot\n# mount /dev/sda1 /mnt/boot/efi\n```\n\nZFS ファイルシステムに chroot:\n\n```\n# arch-chroot /mnt /bin/bash\n```\n\nカーネルのバージョンを確認:\n\n```\n# pacman -Qi linux\n# uname -r\n```\n\nuname は archiso のカーネルバージョンを表示します。バージョンが異なる場合、chroot 環境の適切なカーネルバージョンで depmod を (chroot の中で) 実行してください (バージョンは pacman -Qi linux で確認できます):\n\n```\n# depmod -a 3.6.9-1-ARCH\n```\n\nこれで chroot 環境にインストールされているカーネルバージョンに合ったカーネルモジュールがロードされます。\n\nramdisk の再生成:\n\n```\n# mkinitcpio -p linux\n```\n\nエラーは起こらないはずです。\n\n"
    },
    {
      "title": "バインドマウント",
      "level": 3,
      "content": "以下では /mnt/zfspool から /srv/nfs4/music へのバインドマウントを作成します。バインドマウントを作成する前に zfs プールの準備ができていることを確認してください。\n\n"
    },
    {
      "title": "fstab",
      "level": 4,
      "content": "systemd-fstab-generator を使って systemd が fstab を mount ユニットファイルにどうやって変換するのかは systemd.mount を見てください。\n\n```\n/etc/fstab\n```\n\n```\n/mnt/zfspool\t\t/srv/nfs4/music\t\tnone\tbind,defaults,nofail,x-systemd.requires=zfs-mount.service\t0 0\n```\n\n"
    },
    {
      "title": "モニタリングやイベント時にメール",
      "level": 3,
      "content": "詳しくは ZED: The ZFS Event Daemon を見てください。\n\nメールを送信するには msmtp などのメールフォワーダが必要です。動作することを確認してください。\n\n設定ファイルで以下の行をアンコメント:\n\n```\n/etc/zfs/zed.d/zed.rc\n```\n\n```\nZED_EMAIL_ADDR=\"root\"\n ZED_EMAIL_PROG=\"mail\"\n ZED_NOTIFY_VERBOSE=0\n ZED_EMAIL_OPTS=\"-s '@SUBJECT@' @ADDRESS@\"\n```\n\nZED_EMAIL_ADDR=\"root\" の 'root' は通知を受信したいメールアドレスに置き換えてください。\n\nプールの状態に関わらずメールを受信したい場合は ZED_NOTIFY_VERBOSE=1 と設定すると良いでしょう。\n\n設定したら zfs-zed.service を起動・有効化してください。\n\nverbose を 1 に設定したら、スクラブを実行することでテストできます。\n\n"
    },
    {
      "title": "参照",
      "level": 2,
      "content": "- Aaron Toponce による ZFS に関する17のブログ記事\n- ZFS on Linux\n- ZFS on Linux FAQ\n- FreeBSD ハンドブック -- The Z File System\n- Oracle Solaris ZFS 管理ガイド\n- Solaris Internals -- ZFS トラブルシューティングガイド\n- ZFS を使って毎日 5TB の MySQL のデータをバックアップする Pingdom の例\n- カスタムカーネルにモジュールを追加するチュートリアル\n\n"
    }
  ]
}